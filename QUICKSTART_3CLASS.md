# ä¸‰åˆ†ç±»æ¨¡å‹å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: æ•°æ®å·²å‡†å¤‡å¥½ âœ…

æ•°æ®å·²ç»æ¸…æ´—å®Œæˆï¼Œä½äºï¼š
- `dataset/processed/train.json` (109,486 æ¡)
- `dataset/processed/dev.json` (13,685 æ¡)
- `dataset/processed/test.json` (13,688 æ¡)

### æ­¥éª¤ 2: ä¿®æ”¹ä»£ç ï¼ˆå¿…é¡»ï¼ï¼‰

#### 2.1 ä¿®æ”¹ main.py

æ‰¾åˆ°ç¬¬ 30 è¡Œå·¦å³ï¼š
```python
parser.add_argument('--num_classes', type=int, default=2, help='åˆ†ç±»ç±»åˆ«æ•°')
```

æ”¹ä¸ºï¼š
```python
parser.add_argument('--num_classes', type=int, default=3, help='åˆ†ç±»ç±»åˆ«æ•°')
```

#### 2.2 æ·»åŠ ç±»åˆ«æƒé‡ï¼ˆæ¨èï¼‰

åœ¨ `main.py` çš„ `main()` å‡½æ•°ä¸­ï¼Œæ‰¾åˆ°å®šä¹‰æŸå¤±å‡½æ•°çš„åœ°æ–¹ï¼ˆçº¦ç¬¬ 230 è¡Œï¼‰ï¼š

```python
# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ - åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥
criterion = nn.CrossEntropyLoss()
```

æ”¹ä¸ºï¼š
```python
# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ - åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥
# æ·»åŠ ç±»åˆ«æƒé‡å¤„ç†åè®½æ•°æ®ä¸å¹³è¡¡ï¼ˆåè®½åªå 3.56%ï¼‰
class_weights = torch.tensor([1.0, 1.0, 13.5]).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
logger.info(f"ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights.tolist()}")
```

### æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹

```bash
python main.py --dataset_dir dataset/processed --num_classes 3 --batch_size 16 --epochs 50
```

### æ­¥éª¤ 4: æŸ¥çœ‹ç»“æœ

è®­ç»ƒå®Œæˆåï¼ŒæŸ¥çœ‹ï¼š
- è®­ç»ƒæ—¥å¿—ï¼š`logs/training_*.log`
- æ¨¡å‹æ–‡ä»¶ï¼š`checkpoints/best_model.pth`
- è®­ç»ƒæ›²çº¿ï¼š`checkpoints/training_history.png`
- æ··æ·†çŸ©é˜µï¼š`checkpoints/confusion_matrix.png`

## ğŸ“Š ä¸‰åˆ†ç±»æ ‡ç­¾è¯´æ˜

| Label | å«ä¹‰ | ç¤ºä¾‹ |
|-------|------|------|
| 0 | æ­£å¸¸-æ­£é¢ | "è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼" |
| 1 | æ­£å¸¸-è´Ÿé¢ | "å¤ªå·®äº†ï¼Œå®Œå…¨ä¸èƒ½ç”¨" |
| 2 | é˜´é˜³æ€ªæ°” | "å‘µå‘µï¼ŒçœŸæ˜¯å¤ªå¥½äº†å‘¢" |

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè¦ç”¨ç±»åˆ«æƒé‡ï¼Ÿ
A: åè®½æ•°æ®åªå  3.56%ï¼Œä¸åŠ æƒé‡æ¨¡å‹ä¼šå€¾å‘äºé¢„æµ‹ Label 0/1ï¼Œå¯¼è‡´åè®½è¯†åˆ«ç‡ä½ã€‚

### Q2: å¦‚ä½•è¯„ä¼°æ¨¡å‹ï¼Ÿ
A: ä¸è¦åªçœ‹ Accuracyï¼Œé‡ç‚¹å…³æ³¨ï¼š
- F1-Score (Macro)
- æ¯ä¸ªç±»åˆ«çš„ Precision/Recall
- æ··æ·†çŸ©é˜µï¼ˆç‰¹åˆ«æ˜¯åè®½çš„è¯†åˆ«ç‡ï¼‰

### Q3: è®­ç»ƒå¤šä¹…ï¼Ÿ
A: å»ºè®®è‡³å°‘ 30 ä¸ª epochï¼Œè§‚å¯ŸéªŒè¯é›† F1-Score æ˜¯å¦æ”¶æ•›ã€‚

### Q4: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ
A: é™ä½ batch_size åˆ° 8 æˆ– 4ã€‚

## ğŸ¯ é¢„æœŸæ•ˆæœ

### å¥½çš„æ¨¡å‹åº”è¯¥è¾¾åˆ°ï¼š
- **Accuracy**: > 85%
- **F1-Score (Macro)**: > 0.75
- **åè®½ F1-Score**: > 0.60

### å¦‚æœæ•ˆæœä¸å¥½ï¼š
1. å¢åŠ åè®½çš„ç±»åˆ«æƒé‡ï¼ˆå¦‚ 20.0ï¼‰
2. ä½¿ç”¨ Focal Loss
3. å¢åŠ è®­ç»ƒè½®æ•°
4. è°ƒæ•´å­¦ä¹ ç‡

## ğŸ“ å®Œæ•´å‘½ä»¤ç¤ºä¾‹

```bash
# è®­ç»ƒ
python main.py \
    --dataset_dir dataset/processed \
    --num_classes 3 \
    --batch_size 16 \
    --epochs 50 \
    --learning_rate 2e-5 \
    --patience 10

# è¯„ä¼°
python main.py \
    --dataset_dir dataset/processed \
    --num_classes 3 \
    --evaluate_only \
    --resume checkpoints/best_model.pth
```

## ğŸ” è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹æ•°æ®åˆ†å¸ƒ
```bash
cd dataset
python verify_data.py
```

### æµ‹è¯•å•ä¸ªæ ·æœ¬
```python
from inference import predict

text = "å‘µå‘µï¼ŒçœŸæ˜¯å¤ªå¥½äº†å‘¢"
result = predict(text, model_path='checkpoints/best_model.pth')
print(f"é¢„æµ‹: {result['label']}, æ¦‚ç‡: {result['probs']}")
```

## ğŸ“š æ›´å¤šæ–‡æ¡£

- è¯¦ç»†è¯´æ˜ï¼š`dataset/README.md`
- è¿ç§»æŒ‡å—ï¼š`dataset/MIGRATION_GUIDE.md`
- å¼€å‘æ—¥å¿—ï¼š`process.txt`

## ğŸ‰ å¼€å§‹è®­ç»ƒå§ï¼

ä¿®æ”¹å®Œä»£ç åï¼Œç›´æ¥è¿è¡Œï¼š
```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€
