"""
æ•°æ®é¢„å¤„ç†æ¨¡å—
è´Ÿè´£ HanLP å¤„ç†ã€ç”Ÿæˆè¶…å›¾ç»“æ„
"""

import hanlp
import torch
import numpy as np
from typing import List, Dict, Tuple, Any
import networkx as nx
from collections import defaultdict
from transformers import BertTokenizer
import os
import pickle
from tqdm import tqdm


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡æœ¬å¤„ç†å’Œè¶…å›¾æ„å»º"""
    
    def __init__(self, bert_model_name: str = 'bert-base-chinese', hanlp_model_path: str = None):
        """
        åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            bert_model_name: BERTæ¨¡å‹åç§°ï¼Œç”¨äºåˆå§‹åŒ–tokenizer
            hanlp_model_path: HanLPæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
        """
        # åˆå§‹åŒ–BERT tokenizer - ç»å¯¹ä¸èƒ½è‡ªå·±æ„å»ºè¯æ±‡è¡¨ï¼
        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)
        
        # åˆå§‹åŒ–HanLPæ¨¡å‹ - åªåœ¨initä¸­åŠ è½½ä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½
        self.hanlp_pipeline = self._load_hanlp_model(hanlp_model_path)
        
    def _load_hanlp_model(self, model_path: str = None):
        """åŠ è½½HanLPæ¨¡å‹ - åªåŠ è½½ä¸€æ¬¡"""
        if model_path:
            return hanlp.load(model_path)
        else:
            # ä½¿ç”¨é»˜è®¤çš„ä¸­æ–‡æ¨¡å‹
            return hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
    
    def process_text_pair(self, text: str, topic: str, max_length: int = 512) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†æ–‡æœ¬å¯¹ï¼ˆè¯„è®º+ä¸»é¢˜ï¼‰ï¼Œç”¨äºè®½åˆºæ£€æµ‹ä»»åŠ¡
        
        Args:
            text: è¯„è®ºæ–‡æœ¬
            topic: ä¸»é¢˜æ–‡æœ¬
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            
        Returns:
            åŒ…å«input_ids, attention_mask, token_type_idsçš„å­—å…¸
        """
        # ä½¿ç”¨BERT tokenizerå¤„ç†æ–‡æœ¬å¯¹
        encoded = self.tokenizer(
            text,
            topic,  # ç¬¬äºŒä¸ªå¥å­
            add_special_tokens=True,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoded['input_ids'].squeeze(0),
            'attention_mask': encoded['attention_mask'].squeeze(0),
            'token_type_ids': encoded.get('token_type_ids', torch.zeros_like(encoded['input_ids'])).squeeze(0)
        }
    
    def process_text_with_hanlp(self, text: str, topic: str = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨HanLPå¤„ç†æ–‡æœ¬ï¼Œè·å–è¯­è¨€å­¦ç‰¹å¾ç”¨äºæ„å»ºè¶…å›¾
        ä¼˜åŒ–ï¼šå¤„ç†æ–‡æœ¬å¯¹ï¼ˆè¯„è®º+ä¸»é¢˜ï¼‰ä»¥è·å¾—æ›´ä¸°å¯Œçš„è¯­è¨€å­¦ç‰¹å¾
        
        Args:
            text: è¯„è®ºæ–‡æœ¬
            topic: ä¸»é¢˜æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ç­‰ç»“æœçš„å­—å…¸
        """
        # åˆå¹¶æ–‡æœ¬å’Œä¸»é¢˜è¿›è¡ŒHanLPå¤„ç†ï¼Œè·å¾—æ›´å…¨é¢çš„è¯­è¨€å­¦ç‰¹å¾
        combined_text = text
        if topic:
            combined_text = f"{topic} {text}"  # ä¸»é¢˜åœ¨å‰ï¼Œè¯„è®ºåœ¨å
            
        result = self.hanlp_pipeline(combined_text)
        
        # åˆ†ç¦»ä¸»é¢˜å’Œè¯„è®ºçš„tokenç´¢å¼•
        topic_tokens = self.hanlp_pipeline(topic)['tok'] if topic else []
        text_tokens = self.hanlp_pipeline(text)['tok']
        
        return {
            'tokens': result.get('tok', []),
            'pos_tags': result.get('pos', []),
            'ner_tags': result.get('ner', []),
            'dependencies': result.get('dep', []),
            'semantic_roles': result.get('srl', []),
            'topic_length': len(topic_tokens),  # ç”¨äºåŒºåˆ†ä¸»é¢˜å’Œè¯„è®ºéƒ¨åˆ†
            'text_length': len(text_tokens)
        }
    
    def _create_single_hypergraph_matrix(self, hanlp_result: Dict[str, Any], max_length: int, max_edges: int) -> np.ndarray:
        """
        ä¸ºå•ä¸ªæ ·æœ¬åˆ›å»ºç‹¬ç«‹çš„è¶…å›¾å…³è”çŸ©é˜µ - ä¿®æ­£ï¼šé¿å…æ ·æœ¬é—´æ•°æ®æ³„éœ²ï¼
        
        Args:
            hanlp_result: å•ä¸ªæ ·æœ¬çš„HanLPå¤„ç†ç»“æœ
            max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆèŠ‚ç‚¹æ•°Nï¼‰
            max_edges: æœ€å¤§è¶…è¾¹æ•°ï¼ˆMï¼Œç”¨äºpaddingï¼‰
            
        Returns:
            è¶…å›¾å…³è”çŸ©é˜µ H âˆˆ R^{NÃ—M}ï¼Œå•ä¸ªæ ·æœ¬ç‹¬ç«‹
        """
        tokens = hanlp_result['tokens']
        dependencies = hanlp_result.get('dependencies', [])
        pos_tags = hanlp_result.get('pos_tags', [])
        ner_tags = hanlp_result.get('ner_tags', [])
        
        # æ”¶é›†å½“å‰æ ·æœ¬çš„è¶…è¾¹
        sample_hyperedges = []
        
        # è¶…è¾¹1: ä¾å­˜å¥æ³•ç°‡
        if dependencies:
            for dep in dependencies:
                if len(dep) >= 3:
                    head_idx, tail_idx = dep[0], dep[1]
                    if head_idx < len(tokens) and tail_idx < len(tokens) and head_idx < max_length and tail_idx < max_length:
                        hyperedge = set([head_idx, tail_idx])
                        sample_hyperedges.append(hyperedge)
        
        # è¶…è¾¹2: è¯æ€§æ ‡æ³¨ç°‡
        pos_groups = defaultdict(set)
        for i, pos in enumerate(pos_tags):
            if i < len(tokens) and i < max_length:
                pos_groups[pos].add(i)
        
        for pos, indices in pos_groups.items():
            if len(indices) > 1:
                sample_hyperedges.append(indices)
        
        # è¶…è¾¹3: å‘½åå®ä½“ç°‡
        if ner_tags:
            current_entity = None
            current_indices = set()
            
            for i, ner in enumerate(ner_tags):
                if i < len(tokens) and i < max_length:
                    if ner.startswith('B-'):
                        if current_entity and len(current_indices) > 1:
                            sample_hyperedges.append(current_indices)
                        current_entity = ner[2:]
                        current_indices = {i}
                    elif ner.startswith('I-') and current_entity:
                        current_indices.add(i)
                    else:
                        if current_entity and len(current_indices) > 1:
                            sample_hyperedges.append(current_indices)
                        current_entity = None
                        current_indices = set()
            
            if current_entity and len(current_indices) > 1:
                sample_hyperedges.append(current_indices)
        
        # è¶…è¾¹4: æ»‘åŠ¨çª—å£
        window_size = 3
        for i in range(min(len(tokens), max_length) - window_size + 1):
            window_indices = set(range(i, i + window_size))
            sample_hyperedges.append(window_indices)
        
        # è¶…è¾¹5: ä¸»é¢˜-è¯„è®ºå…³è”
        topic_length = hanlp_result.get('topic_length', 0)
        text_length = hanlp_result.get('text_length', 0)
        
        if topic_length > 0 and text_length > 0:
            topic_indices = set(range(1, min(topic_length + 1, len(tokens), max_length)))
            text_start = topic_length + 1
            text_indices = set(range(text_start, min(text_start + text_length, len(tokens), max_length)))
            
            if len(topic_indices) > 0 and len(text_indices) > 0:
                sample_hyperedges.append(topic_indices.union(text_indices))
                if len(topic_indices) > 1:
                    sample_hyperedges.append(topic_indices)
                if len(text_indices) > 1:
                    sample_hyperedges.append(text_indices)
        
        # æ„å»ºå…³è”çŸ©é˜µå¹¶è¿›è¡Œpadding
        num_actual_edges = len(sample_hyperedges)
        incidence_matrix = np.zeros((max_length, max_edges))
        
        # å¡«å……å®é™…çš„è¶…è¾¹ï¼ˆé™åˆ¶åœ¨max_edgesèŒƒå›´å†…ï¼‰
        for j, hyperedge in enumerate(sample_hyperedges[:max_edges]):
            for node_idx in hyperedge:
                if node_idx < max_length:
                    incidence_matrix[node_idx, j] = 1.0
        
        return incidence_matrix
    
    def estimate_max_edges(self, hanlp_results: List[Dict[str, Any]]) -> int:
        """
        ä¼°ç®—æ‰¹æ¬¡ä¸­çš„æœ€å¤§è¶…è¾¹æ•°ï¼Œç”¨äºpadding
        
        Args:
            hanlp_results: HanLPå¤„ç†ç»“æœåˆ—è¡¨
            
        Returns:
            æœ€å¤§è¶…è¾¹æ•°
        """
        max_edges = 0
        
        for result in hanlp_results:
            tokens = result['tokens']
            dependencies = result.get('dependencies', [])
            pos_tags = result.get('pos_tags', [])
            ner_tags = result.get('ner_tags', [])
            
            # ç²—ç•¥ä¼°ç®—å½“å‰æ ·æœ¬çš„è¶…è¾¹æ•°
            edge_count = 0
            
            # ä¾å­˜å…³ç³»è¶…è¾¹
            edge_count += len(dependencies) if dependencies else 0
            
            # è¯æ€§è¶…è¾¹ï¼ˆä¼°ç®—ï¼‰
            unique_pos = len(set(pos_tags)) if pos_tags else 0
            edge_count += unique_pos
            
            # å®ä½“è¶…è¾¹ï¼ˆä¼°ç®—ï¼‰
            entity_count = sum(1 for tag in (ner_tags or []) if tag.startswith('B-'))
            edge_count += entity_count
            
            # æ»‘åŠ¨çª—å£è¶…è¾¹
            window_edges = max(0, len(tokens) - 2) if tokens else 0
            edge_count += window_edges
            
            # ä¸»é¢˜-è¯„è®ºè¶…è¾¹
            edge_count += 3  # äº¤äº’+ä¸»é¢˜+è¯„è®º
            
            max_edges = max(max_edges, edge_count)
        
        # æ·»åŠ ä¸€äº›ç¼“å†²
        return min(max_edges + 10, 200)  # é™åˆ¶æœ€å¤§å€¼é¿å…å†…å­˜çˆ†ç‚¸


def load_dataset(file_path: str) -> List[Tuple[str, str, int]]:
    """
    åŠ è½½JSONæ ¼å¼çš„æ•°æ®é›†
    
    Args:
        file_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        (æ–‡æœ¬, ä¸»é¢˜, æ ‡ç­¾) çš„åˆ—è¡¨
    """
    import json
    
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
            
        for item in json_data:
            text = item.get('text', '').strip()
            topic = item.get('topic', '').strip()
            label = int(item.get('label', '0'))  # è½¬æ¢å­—ç¬¦ä¸²æ ‡ç­¾ä¸ºæ•´æ•°
            
            if text:  # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
                data.append((text, topic, label))
                
    except Exception as e:
        print(f"åŠ è½½æ•°æ®é›†æ—¶å‡ºé”™: {file_path}, é”™è¯¯: {e}")
        
    return data


def load_all_datasets(dataset_dir: str = "dataset") -> Tuple[List[Tuple[str, str, int]], 
                                                           List[Tuple[str, str, int]], 
                                                           List[Tuple[str, str, int]]]:
    """
    åŠ è½½æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶
    
    Args:
        dataset_dir: æ•°æ®é›†ç›®å½•è·¯å¾„
        
    Returns:
        (è®­ç»ƒæ•°æ®, éªŒè¯æ•°æ®, æµ‹è¯•æ•°æ®) çš„å…ƒç»„
    """
    import os
    
    train_path = os.path.join(dataset_dir, 'train.json')
    dev_path = os.path.join(dataset_dir, 'dev.json')
    test_path = os.path.join(dataset_dir, 'test.json')
    
    train_data = load_dataset(train_path) if os.path.exists(train_path) else []
    dev_data = load_dataset(dev_path) if os.path.exists(dev_path) else []
    test_data = load_dataset(test_path) if os.path.exists(test_path) else []
    
    print(f"æ•°æ®é›†åŠ è½½å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(dev_data)} æ ·æœ¬") 
    print(f"  æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    return train_data, dev_data, test_data


class SarcasmDataset(torch.utils.data.Dataset):
    """è®½åˆºæ£€æµ‹æ•°æ®é›†ç±» - ç§»åˆ°å…¨å±€ä½œç”¨åŸŸï¼Œæ·»åŠ ç¼“å­˜æœºåˆ¶é¿å…æ€§èƒ½é™·é˜±"""
    
    def __init__(self, data, preprocessor, max_length, cache_file=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data: åŸå§‹æ•°æ® [(text, topic, label), ...]
            preprocessor: æ•°æ®é¢„å¤„ç†å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨ç¼“å­˜æœºåˆ¶
        """
        self.data = data
        self.preprocessor = preprocessor
        self.max_length = max_length
        self.cached_data = []
        
        # ç¼“å­˜é€»è¾‘ï¼šå¦‚æœæœ‰ç¼“å­˜æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½ï¼›å¦åˆ™å¤„ç†å¹¶ä¿å­˜
        if cache_file and os.path.exists(cache_file):
            print(f"ğŸ“¥ æ­£åœ¨åŠ è½½ç¼“å­˜æ•°æ®: {cache_file} ...")
            with open(cache_file, 'rb') as f:
                self.cached_data = pickle.load(f)
            print(f"âœ… ç¼“å­˜åŠ è½½å®Œæˆ: {len(self.cached_data)} æ ·æœ¬")
        else:
            print("ğŸ”„ æ­£åœ¨è¿›è¡Œé¢„å¤„ç†ï¼ˆHanLPè§£æï¼‰ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
            print("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯ä¸€æ¬¡æ€§æ“ä½œï¼Œå¤„ç†åä¼šç¼“å­˜ï¼Œä¸‹æ¬¡å¯åŠ¨ä¼šå¾ˆå¿«")
            
            for text, topic, label in tqdm(self.data, desc="é¢„å¤„ç†æ•°æ®"):
                # 1. BERT Tokenize
                bert_tokens = self.preprocessor.process_text_pair(text, topic, self.max_length)
                
                # 2. HanLP å¤„ç† (æœ€è€—æ—¶çš„ä¸€æ­¥ - ä½†åªåšä¸€æ¬¡ï¼)
                hanlp_result = self.preprocessor.process_text_with_hanlp(text, topic)
                
                self.cached_data.append({
                    'input_ids': bert_tokens['input_ids'],
                    'attention_mask': bert_tokens['attention_mask'],
                    'token_type_ids': bert_tokens['token_type_ids'],
                    'hanlp_result': hanlp_result,
                    'label': torch.tensor(label, dtype=torch.long),
                    'text': text,  # ä¿ç•™åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
                    'topic': topic  # ä¿ç•™åŸå§‹ä¸»é¢˜ç”¨äºè°ƒè¯•
                })
            
            # ä¿å­˜ç¼“å­˜
            if cache_file:
                print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜åˆ°: {cache_file}")
                os.makedirs(os.path.dirname(cache_file), exist_ok=True)
                with open(cache_file, 'wb') as f:
                    pickle.dump(self.cached_data, f)
                print(f"âœ… ç¼“å­˜ä¿å­˜å®Œæˆ")
        
    def __len__(self):
        return len(self.cached_data)
    
    def __getitem__(self, idx):
        """ç›´æ¥è¿”å›å†…å­˜ä¸­çš„æ•°æ®ï¼Œæ²¡æœ‰ä»»ä½•è®¡ç®—é‡ï¼"""
        return self.cached_data[idx]


def create_hypergraph_collate_fn(preprocessor, max_length):
    """åˆ›å»ºè¶…å›¾æ‰¹å¤„ç†å‡½æ•°çš„å·¥å‚å‡½æ•°"""
    def collate_fn(batch):
        """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†è¶…å›¾çŸ©é˜µæ„å»º - ä¿®æ­£ï¼šæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹çš„è¶…å›¾çŸ©é˜µ"""
        # æå–æ‰¹æ¬¡æ•°æ®
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        token_type_ids = torch.stack([item['token_type_ids'] for item in batch])
        labels = torch.stack([item['label'] for item in batch])
        
        # æ„å»ºæ‰¹æ¬¡çš„è¶…å›¾çŸ©é˜µ - å…³é”®ä¿®æ­£ï¼šæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹ï¼
        hanlp_results = [item['hanlp_result'] for item in batch]
        
        # ä¼°ç®—æœ€å¤§è¶…è¾¹æ•°ç”¨äºpadding
        max_edges = preprocessor.estimate_max_edges(hanlp_results)
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºç‹¬ç«‹çš„è¶…å›¾çŸ©é˜µ
        batch_hypergraph_matrices = []
        for hanlp_result in hanlp_results:
            single_matrix = preprocessor._create_single_hypergraph_matrix(hanlp_result, max_length, max_edges)
            batch_hypergraph_matrices.append(single_matrix)
        
        # å †å æˆ [batch_size, max_length, max_edges] çš„3Då¼ é‡
        hypergraph_matrix = torch.tensor(np.stack(batch_hypergraph_matrices), dtype=torch.float32)
        
        # ç¡¬ä»¶æ— å…³æ€§
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        return {
            'input_ids': input_ids.to(device),
            'attention_mask': attention_mask.to(device),
            'token_type_ids': token_type_ids.to(device),
            'hypergraph_matrix': hypergraph_matrix.to(device),  # [batch_size, N, M]
            'labels': labels.to(device)
        }
    
    return collate_fn


def create_data_loaders(train_data: List[Tuple[str, str, int]], 
                       val_data: List[Tuple[str, str, int]], 
                       preprocessor: DataPreprocessor,
                       batch_size: int = 32,
                       max_length: int = 512,
                       cache_dir: str = "cache") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨ - é’ˆå¯¹JSONæ ¼å¼æ•°æ®é›†ä¼˜åŒ–ï¼Œæ”¯æŒç¼“å­˜æœºåˆ¶
    
    Args:
        train_data: è®­ç»ƒæ•°æ® [(text, topic, label), ...]
        val_data: éªŒè¯æ•°æ® [(text, topic, label), ...]
        preprocessor: æ•°æ®é¢„å¤„ç†å™¨
        batch_size: æ‰¹æ¬¡å¤§å°
        max_length: BERTæœ€å¤§åºåˆ—é•¿åº¦
        cache_dir: ç¼“å­˜ç›®å½•
        
    Returns:
        è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    """
    from torch.utils.data import DataLoader
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    os.makedirs(cache_dir, exist_ok=True)
    
    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    train_cache_file = os.path.join(cache_dir, 'train_cache.pkl')
    val_cache_file = os.path.join(cache_dir, 'val_cache.pkl')
    
    # åˆ›å»ºæ‰¹å¤„ç†å‡½æ•°
    collate_fn = create_hypergraph_collate_fn(preprocessor, max_length)
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆå¸¦ç¼“å­˜ï¼‰
    print("ğŸ”§ åˆå§‹åŒ–è®­ç»ƒæ•°æ®é›†...")
    train_dataset = SarcasmDataset(train_data, preprocessor, max_length, cache_file=train_cache_file)
    
    print("ğŸ”§ åˆå§‹åŒ–éªŒè¯æ•°æ®é›†...")
    val_dataset = SarcasmDataset(val_data, preprocessor, max_length, cache_file=val_cache_file)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    return train_loader, val_loader