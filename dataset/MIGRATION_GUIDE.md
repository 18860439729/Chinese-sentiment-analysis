# ä¸‰åˆ†ç±»å‡çº§è¿ç§»æŒ‡å—

## ğŸ“‹ å˜æ›´æ€»ç»“

### ä»äºŒåˆ†ç±»åˆ°ä¸‰åˆ†ç±»

**æ—§æ–¹æ¡ˆï¼ˆäºŒåˆ†ç±»ï¼‰**:
- Label 0: éåè®½
- Label 1: åè®½

**æ–°æ–¹æ¡ˆï¼ˆä¸‰åˆ†ç±»ï¼‰**:
- Label 0: æ­£å¸¸-æ­£é¢
- Label 1: æ­£å¸¸-è´Ÿé¢
- Label 2: é˜´é˜³æ€ªæ°”

## ğŸ”§ å¿…é¡»ä¿®æ”¹çš„ä»£ç 

### 1. main.py

**ä¿®æ”¹å‘½ä»¤è¡Œå‚æ•°é»˜è®¤å€¼**:
```python
# æ‰¾åˆ°è¿™ä¸€è¡Œï¼ˆçº¦ç¬¬ 30 è¡Œï¼‰
parser.add_argument('--num_classes', type=int, default=2, help='åˆ†ç±»ç±»åˆ«æ•°')

# æ”¹ä¸º
parser.add_argument('--num_classes', type=int, default=3, help='åˆ†ç±»ç±»åˆ«æ•°')
```

### 2. model.py

**æ£€æŸ¥è¾“å‡ºå±‚å®šä¹‰**:
```python
# ç¡®ä¿ num_classes å‚æ•°æ­£ç¡®ä¼ é€’
def __init__(self, ..., num_classes=3, ...):
    ...
    self.classifier = nn.Linear(hidden_size, num_classes)
```

### 3. è®­ç»ƒå‘½ä»¤

**ä½¿ç”¨æ–°æ•°æ®è®­ç»ƒ**:
```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

## ğŸ“Š æ•°æ®å˜æ›´

### æ•°æ®è§„æ¨¡
- **æ€»æ ·æœ¬**: 136,859 æ¡
- **è®­ç»ƒé›†**: 109,486 æ¡
- **éªŒè¯é›†**: 13,685 æ¡
- **æµ‹è¯•é›†**: 13,688 æ¡

### æ ‡ç­¾åˆ†å¸ƒ
- **Label 0 (æ­£é¢)**: 48.22%
- **Label 1 (è´Ÿé¢)**: 48.22%
- **Label 2 (åè®½)**: 3.56%

## âš ï¸ ç±»åˆ«ä¸å¹³è¡¡å¤„ç†

### æ–¹æ³• 1: ç±»åˆ«æƒé‡

åœ¨ `main.py` ä¸­æ·»åŠ ï¼š

```python
# å®šä¹‰æŸå¤±å‡½æ•°æ—¶æ·»åŠ æƒé‡
class_weights = torch.tensor([1.0, 1.0, 13.5]).to(device)  # åè®½æƒé‡æé«˜
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

### æ–¹æ³• 2: Focal Loss

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()

# ä½¿ç”¨
criterion = FocalLoss(alpha=0.25, gamma=2.0)
```

### æ–¹æ³• 3: è¯„ä¼°æŒ‡æ ‡è°ƒæ•´

ä¸ä»…çœ‹ Accuracyï¼Œé‡ç‚¹å…³æ³¨ï¼š
- **F1-Score (Macro)**: å¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»åˆ«
- **F1-Score (Weighted)**: æŒ‰æ ·æœ¬æ•°åŠ æƒ
- **æ¯ä¸ªç±»åˆ«çš„ Precision/Recall**
- **æ··æ·†çŸ©é˜µ**

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡æ›´æ–°

### utils.py ä¿®æ”¹å»ºè®®

åœ¨ `MetricsCalculator.calculate_metrics()` ä¸­ï¼š

```python
# æ·»åŠ æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
for i, label_name in enumerate(['æ­£é¢', 'è´Ÿé¢', 'åè®½']):
    metrics[f'{label_name}_precision'] = precision_per_class[i]
    metrics[f'{label_name}_recall'] = recall_per_class[i]
    metrics[f'{label_name}_f1'] = f1_per_class[i]
```

### æ‰“å°åˆ†ç±»æŠ¥å‘Šæ—¶

```python
labels = ['æ­£é¢', 'è´Ÿé¢', 'åè®½']
print(classification_report(y_true, y_pred, target_names=labels))
```

## ğŸ¯ "å¼ºæ’‘"è¯†åˆ«å®ç°

### æ¨ç†æ—¶çš„åå¤„ç†

```python
def detect_complex_emotion(probs):
    """
    æ£€æµ‹å¤æ‚æƒ…æ„Ÿï¼ˆå¦‚"å¼ºæ’‘"ï¼‰
    
    Args:
        probs: [P(æ­£é¢), P(è´Ÿé¢), P(åè®½)]
    
    Returns:
        emotion_type: 'positive', 'negative', 'sarcastic', 'struggling'
    """
    p_pos, p_neg, p_sar = probs
    
    # å¼ºæ’‘ï¼šæ­£é¢æ¦‚ç‡é«˜ + åè®½æ¦‚ç‡ä¹Ÿä¸ä½
    if p_pos > 0.4 and p_sar > 0.2:
        return 'struggling'  # å¼ºæ’‘/è‹¦ç¬‘
    
    # æ­£å¸¸åˆ¤æ–­
    max_idx = probs.argmax()
    if max_idx == 0:
        return 'positive'
    elif max_idx == 1:
        return 'negative'
    else:
        return 'sarcastic'
```

## ğŸ§ª æµ‹è¯•å»ºè®®

### 1. å•å…ƒæµ‹è¯•

æµ‹è¯•æ¯ä¸ªç±»åˆ«çš„è¯†åˆ«ï¼š

```python
test_cases = [
    ("è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼", 0),  # æ­£é¢
    ("å¤ªå·®äº†ï¼Œå®Œå…¨ä¸èƒ½ç”¨", 1),    # è´Ÿé¢
    ("å‘µå‘µï¼ŒçœŸæ˜¯å¤ªå¥½äº†å‘¢", 2),    # åè®½
]

for text, expected_label in test_cases:
    pred = model.predict(text)
    assert pred == expected_label
```

### 2. è¾¹ç•Œæ¡ˆä¾‹æµ‹è¯•

```python
edge_cases = [
    "è¿˜è¡Œå§",           # ä¸­æ€§ï¼Œå¯èƒ½æ˜¯æ­£é¢æˆ–è´Ÿé¢
    "æˆ‘å¤ªå¼€å¿ƒäº†å‘¢[å¾®ç¬‘]", # å¯èƒ½æ˜¯å¼ºæ’‘
    "çœŸæ˜¯å‰å®³å•Š",       # å¯èƒ½æ˜¯åè®½
]
```

### 3. æ··æ·†çŸ©é˜µåˆ†æ

é‡ç‚¹å…³æ³¨ï¼š
- æ­£é¢ vs åè®½ çš„æ··æ·†
- è´Ÿé¢ vs åè®½ çš„æ··æ·†

## ğŸ“ å®Œæ•´è®­ç»ƒæµç¨‹

```bash
# 1. æ•°æ®æ¸…æ´—ï¼ˆå¦‚æœè¿˜æ²¡åšï¼‰
cd dataset
python build_dataset_3class.py

# 2. éªŒè¯æ•°æ®
python verify_data.py

# 3. ä¿®æ”¹ä»£ç 
# - main.py: num_classes=3
# - model.py: ç¡®è®¤è¾“å‡ºå±‚
# - æ·»åŠ ç±»åˆ«æƒé‡ï¼ˆå¯é€‰ï¼‰

# 4. è®­ç»ƒæ¨¡å‹
cd ..
python main.py \
    --dataset_dir dataset/processed \
    --num_classes 3 \
    --batch_size 16 \
    --epochs 50 \
    --learning_rate 2e-5

# 5. è¯„ä¼°
python main.py \
    --dataset_dir dataset/processed \
    --num_classes 3 \
    --evaluate_only \
    --resume checkpoints/best_model.pth
```

## ğŸ“ è®ºæ–‡å†™ä½œå»ºè®®

### åˆ›æ–°ç‚¹
1. å¸¦æƒ…æ„Ÿææ€§çš„åè®½æ£€æµ‹ï¼ˆä¸‰åˆ†ç±»ï¼‰
2. å¤æ‚æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¼ºæ’‘/è‹¦ç¬‘ï¼‰
3. å¤šæ•°æ®æºèåˆç­–ç•¥

### æ¶ˆèå®éªŒ
1. äºŒåˆ†ç±» vs ä¸‰åˆ†ç±»
2. ä¸åŒæ•°æ®æºçš„è´¡çŒ®
3. ç±»åˆ«æƒé‡çš„å½±å“

### è¯„ä¼°æŒ‡æ ‡
- Accuracy
- F1-Score (Macro/Weighted)
- æ¯ä¸ªç±»åˆ«çš„ P/R/F1
- æ··æ·†çŸ©é˜µ
- AUC (One-vs-Rest)

## âœ… æ£€æŸ¥æ¸…å•

- [ ] ä¿®æ”¹ main.py çš„ num_classes é»˜è®¤å€¼
- [ ] ä¿®æ”¹ model.py çš„è¾“å‡ºå±‚
- [ ] æ·»åŠ ç±»åˆ«æƒé‡æˆ– Focal Loss
- [ ] æ›´æ–°è¯„ä¼°æŒ‡æ ‡ï¼ˆF1-Scoreï¼‰
- [ ] æµ‹è¯•ä¸‰ä¸ªç±»åˆ«çš„è¯†åˆ«
- [ ] å®ç°"å¼ºæ’‘"æ£€æµ‹é€»è¾‘
- [ ] å‡†å¤‡æ¶ˆèå®éªŒ
- [ ] æ’°å†™è®ºæ–‡ç›¸å…³ç« èŠ‚

## ğŸ‰ é¢„æœŸæ•ˆæœ

ä½¿ç”¨ä¸‰åˆ†ç±»åï¼š
- âœ… æ›´å‡†ç¡®çš„æƒ…æ„Ÿåˆ†æ
- âœ… å¯ä»¥è¯†åˆ«åè®½
- âœ… å¯ä»¥æ£€æµ‹å¤æ‚æƒ…æ„Ÿ
- âœ… æ›´é«˜çš„å­¦æœ¯ä»·å€¼
- âœ… æ›´å¼ºçš„å®ç”¨æ€§

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€
