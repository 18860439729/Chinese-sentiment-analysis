"""
æ•°æ®é›†å¤„ç†è„šæœ¬
å°† raw_source çš„åŸå§‹æ•°æ®è½¬æ¢ä¸º processed çš„è®­ç»ƒæ•°æ®
æ”¯æŒ ChnSentiCorp å’Œ Weibo æ•°æ®é›†
"""

import json
import os
import csv
from pathlib import Path
from typing import List, Dict, Tuple
import random

def load_jsonl(file_path: str) -> List[Dict]:
    """
    åŠ è½½ JSONL æ–‡ä»¶
    
    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®åˆ—è¡¨
    """
    data = []
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return data
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSON è§£æé”™è¯¯: {e}")
                    continue
    
    return data


def load_csv(file_path: str) -> List[Dict]:
    """
    åŠ è½½ CSV æ–‡ä»¶
    
    Args:
        file_path: CSV æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®åˆ—è¡¨
    """
    data = []
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return data
    
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)
    
    return data


def process_chnsenticorp(raw_dir: str = "raw_source/ChnSentiCorp") -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    å¤„ç† ChnSentiCorp æƒ…æ„Ÿåˆ†ææ•°æ®
    
    Args:
        raw_dir: åŸå§‹æ•°æ®ç›®å½•
        
    Returns:
        (train_data, dev_data, test_data)
    """
    print("=" * 60)
    print("ğŸ“¥ å¤„ç† ChnSentiCorp æ•°æ®é›†...")
    print("=" * 60)
    
    train_data = []
    dev_data = []
    test_data = []
    
    # åŠ è½½è®­ç»ƒé›†
    train_file = os.path.join(raw_dir, "train.jsonl")
    if os.path.exists(train_file):
        raw_train = load_jsonl(train_file)
        print(f"âœ… åŠ è½½è®­ç»ƒé›†: {len(raw_train)} æ ·æœ¬")
        
        for item in raw_train:
            # æå–æ–‡æœ¬å’Œæ ‡ç­¾
            text = item.get('text', '').strip()
            label = item.get('label', 0)
            
            if text:  # è¿‡æ»¤ç©ºæ–‡æœ¬
                train_data.append({
                    'text': text,
                    'topic': '',  # ChnSentiCorp æ²¡æœ‰ topic
                    'label': str(label)
                })
    
    # åŠ è½½éªŒè¯é›†
    val_file = os.path.join(raw_dir, "validation.jsonl")
    if os.path.exists(val_file):
        raw_val = load_jsonl(val_file)
        print(f"âœ… åŠ è½½éªŒè¯é›†: {len(raw_val)} æ ·æœ¬")
        
        for item in raw_val:
            text = item.get('text', '').strip()
            label = item.get('label', 0)
            
            if text:
                dev_data.append({
                    'text': text,
                    'topic': '',
                    'label': str(label)
                })
    
    # åŠ è½½æµ‹è¯•é›†
    test_file = os.path.join(raw_dir, "test.jsonl")
    if os.path.exists(test_file):
        raw_test = load_jsonl(test_file)
        print(f"âœ… åŠ è½½æµ‹è¯•é›†: {len(raw_test)} æ ·æœ¬")
        
        for item in raw_test:
            text = item.get('text', '').strip()
            label = item.get('label', 0)
            
            if text:
                test_data.append({
                    'text': text,
                    'topic': '',
                    'label': str(label)
                })
    
    print(f"âœ… ChnSentiCorp å¤„ç†å®Œæˆ:")
    print(f"   - è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(dev_data)} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    return train_data, dev_data, test_data


def process_weibo(raw_file: str = "raw_source/Weibo/weibo_senti_100k.csv", 
                 train_ratio: float = 0.8, 
                 val_ratio: float = 0.1) -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    å¤„ç† Weibo æƒ…æ„Ÿåˆ†ææ•°æ®
    
    Args:
        raw_file: åŸå§‹ CSV æ–‡ä»¶è·¯å¾„
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        
    Returns:
        (train_data, dev_data, test_data)
    """
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¤„ç† Weibo æ•°æ®é›†...")
    print("=" * 60)
    
    if not os.path.exists(raw_file):
        print(f"âš ï¸ Weibo æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {raw_file}")
        return [], [], []
    
    # åŠ è½½ CSV æ•°æ®
    raw_data = load_csv(raw_file)
    print(f"âœ… åŠ è½½ Weibo æ•°æ®: {len(raw_data)} æ ·æœ¬")
    
    # è½¬æ¢æ ¼å¼å¹¶è¿‡æ»¤
    processed_data = []
    for item in raw_data:
        text = item.get('review', '').strip()
        label = item.get('label', '0')
        
        if text:  # è¿‡æ»¤ç©ºæ–‡æœ¬
            processed_data.append({
                'text': text,
                'topic': '',
                'label': str(label)
            })
    
    print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(processed_data)} æ¡")
    
    # æ‰“ä¹±æ•°æ®
    random.shuffle(processed_data)
    
    # åˆ’åˆ†æ•°æ®é›†
    total = len(processed_data)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    train_data = processed_data[:train_end]
    dev_data = processed_data[train_end:val_end]
    test_data = processed_data[val_end:]
    
    print(f"âœ… Weibo å¤„ç†å®Œæˆ:")
    print(f"   - è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(dev_data)} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    return train_data, dev_data, test_data


def save_json(data: List[Dict], output_file: str):
    """
    ä¿å­˜ä¸º JSON æ ¼å¼
    
    Args:
        data: æ•°æ®åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ”§" * 30)
    print("æ•°æ®é›†å¤„ç†å·¥å…·")
    print("ğŸ”§" * 30 + "\n")
    
    # åˆ‡æ¢åˆ° dataset ç›®å½•
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    print(f"ğŸ“‚ å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # å¤„ç† ChnSentiCorp
    train_data, dev_data, test_data = process_chnsenticorp()
    
    # å¤„ç† Weiboï¼ˆå¯é€‰ï¼Œä½œä¸ºè¡¥å……æ•°æ®ï¼‰
    try:
        weibo_train, weibo_dev, weibo_test = process_weibo()
        
        if weibo_train:
            print(f"\nğŸ“Š æ··åˆ Weibo æ•°æ®...")
            train_data.extend(weibo_train)
            dev_data.extend(weibo_dev)
            test_data.extend(weibo_test)
            
            # æ‰“ä¹±æ··åˆåçš„æ•°æ®
            random.shuffle(train_data)
            random.shuffle(dev_data)
            random.shuffle(test_data)
            
            print(f"âœ… æ··åˆåæ€»æ ·æœ¬æ•°:")
            print(f"   - è®­ç»ƒé›†: {len(train_data)}")
            print(f"   - éªŒè¯é›†: {len(dev_data)}")
            print(f"   - æµ‹è¯•é›†: {len(test_data)}")
    except Exception as e:
        print(f"âš ï¸ Weibo å¤„ç†å¤±è´¥ï¼Œè·³è¿‡: {e}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "processed"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
    print("=" * 60)
    
    save_json(train_data, os.path.join(output_dir, "train.json"))
    save_json(dev_data, os.path.join(output_dir, "dev.json"))
    save_json(test_data, os.path.join(output_dir, "test.json"))
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print("=" * 60)
    print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(dev_data)} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    # æ ‡ç­¾åˆ†å¸ƒ
    if train_data:
        label_counts = {}
        for item in train_data:
            label = item['label']
            label_counts[label] = label_counts.get(label, 0) + 1
        
        print(f"\nè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in sorted(label_counts.items()):
            print(f"  Label {label}: {count} ({count/len(train_data)*100:.2f}%)")
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   cd ..")
    print("   python main.py --dataset_dir dataset/processed")


if __name__ == "__main__":
    main()
