"""
æ•°æ®æ¸…æ´—è„šæœ¬ - ä¸‰åˆ†ç±»ç‰ˆæœ¬
å®ç°å¸¦æƒ…æ„Ÿææ€§çš„åè®½æ£€æµ‹

æ ‡ç­¾ä½“ç³»ï¼š
- Label 0: æ­£å¸¸-æ­£é¢ (Normal-Positive) - çœŸæ­£çš„å¤¸å¥–
- Label 1: æ­£å¸¸-è´Ÿé¢ (Normal-Negative) - çœŸæ­£çš„æ‰¹è¯„
- Label 2: é˜´é˜³æ€ªæ°” (Sarcastic) - åè®½/è®½åˆº
"""

import json
import os
import csv
from pathlib import Path
from typing import List, Dict, Tuple
import random
from collections import Counter


def load_jsonl(file_path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶"""
    data = []
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return data
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ ç¬¬ {line_num} è¡Œ JSON è§£æé”™è¯¯: {e}")
                    continue
    
    return data


def load_json(file_path: str) -> List[Dict]:
    """åŠ è½½ JSON æ–‡ä»¶"""
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
            return data if isinstance(data, list) else []
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON è§£æé”™è¯¯: {e}")
            return []


def load_csv(file_path: str) -> List[Dict]:
    """åŠ è½½ CSV æ–‡ä»¶"""
    data = []
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return data
    
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)
    
    return data


def process_chnsenticorp(raw_dir: str = "raw_source/ChnSentiCorp") -> List[Dict]:
    """
    å¤„ç† ChnSentiCorp æ•°æ®
    åŸæ ‡ç­¾ 0 -> Label 1 (è´Ÿé¢)
    åŸæ ‡ç­¾ 1 -> Label 0 (æ­£é¢)
    
    Returns:
        å¤„ç†åçš„æ•°æ®åˆ—è¡¨
    """
    print("=" * 60)
    print("ğŸ“¥ å¤„ç† ChnSentiCorp æ•°æ®é›†...")
    print("=" * 60)
    
    all_data = []
    
    # å¤„ç†æ‰€æœ‰åˆ†å‰²
    for split in ['train', 'validation', 'test']:
        file_path = os.path.join(raw_dir, f"{split}.jsonl")
        if not os.path.exists(file_path):
            continue
        
        raw_data = load_jsonl(file_path)
        print(f"âœ… åŠ è½½ {split}: {len(raw_data)} æ ·æœ¬")
        
        for item in raw_data:
            text = item.get('text', '').strip()
            original_label = item.get('label', 0)
            
            if text:
                # é‡æ–°æ ‡æ³¨ï¼šåŸæ ‡ç­¾ 1(å¥½è¯„) -> Label 0(æ­£é¢)
                #          åŸæ ‡ç­¾ 0(å·®è¯„) -> Label 1(è´Ÿé¢)
                new_label = 0 if original_label == 1 else 1
                
                all_data.append({
                    'text': text,
                    'topic': '',
                    'label': new_label,
                    'source': 'ChnSentiCorp'
                })
    
    print(f"âœ… ChnSentiCorp å¤„ç†å®Œæˆ: {len(all_data)} æ ·æœ¬")
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    label_counts = Counter(item['label'] for item in all_data)
    print(f"   - Label 0 (æ­£é¢): {label_counts[0]} æ¡")
    print(f"   - Label 1 (è´Ÿé¢): {label_counts[1]} æ¡")
    
    return all_data


def process_weibo(raw_file: str = "raw_source/Weibo/weibo_senti_100k.csv") -> List[Dict]:
    """
    å¤„ç† Weibo æ•°æ®
    åŸæ ‡ç­¾ 1 -> Label 0 (æ­£é¢)
    åŸæ ‡ç­¾ 0 -> Label 1 (è´Ÿé¢)
    
    Returns:
        å¤„ç†åçš„æ•°æ®åˆ—è¡¨
    """
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¤„ç† Weibo æ•°æ®é›†...")
    print("=" * 60)
    
    if not os.path.exists(raw_file):
        print(f"âš ï¸ Weibo æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {raw_file}")
        return []
    
    raw_data = load_csv(raw_file)
    print(f"âœ… åŠ è½½ Weibo æ•°æ®: {len(raw_data)} æ ·æœ¬")
    
    processed_data = []
    for item in raw_data:
        text = item.get('review', '').strip()
        original_label = int(item.get('label', 0))
        
        if text:
            # é‡æ–°æ ‡æ³¨ï¼šåŸæ ‡ç­¾ 1(æ­£å‘) -> Label 0(æ­£é¢)
            #          åŸæ ‡ç­¾ 0(è´Ÿå‘) -> Label 1(è´Ÿé¢)
            new_label = 0 if original_label == 1 else 1
            
            processed_data.append({
                'text': text,
                'topic': '',
                'label': new_label,
                'source': 'Weibo'
            })
    
    print(f"âœ… Weibo å¤„ç†å®Œæˆ: {len(processed_data)} æ ·æœ¬")
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    label_counts = Counter(item['label'] for item in processed_data)
    print(f"   - Label 0 (æ­£é¢): {label_counts[0]} æ¡")
    print(f"   - Label 1 (è´Ÿé¢): {label_counts[1]} æ¡")
    
    return processed_data


def process_tosarcasm(raw_dir: str = "raw_source/ToSarcasm") -> List[Dict]:
    """
    å¤„ç† ToSarcasm æ•°æ®
    æ‰€æœ‰æ•°æ® -> Label 2 (åè®½)
    
    Returns:
        å¤„ç†åçš„æ•°æ®åˆ—è¡¨
    """
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¤„ç† ToSarcasm æ•°æ®é›†...")
    print("=" * 60)
    
    all_data = []
    
    # å¤„ç†æ‰€æœ‰åˆ†å‰²
    for split in ['train', 'dev', 'test']:
        file_path = os.path.join(raw_dir, f"{split}.json")
        if not os.path.exists(file_path):
            continue
        
        raw_data = load_json(file_path)
        print(f"âœ… åŠ è½½ {split}: {len(raw_data)} æ ·æœ¬")
        
        for item in raw_data:
            text = item.get('text', '').strip()
            topic = item.get('topic', '').strip()
            
            if text:
                # æ‰€æœ‰ ToSarcasm æ•°æ®æ ‡æ³¨ä¸º Label 2 (åè®½)
                all_data.append({
                    'text': text,
                    'topic': topic,
                    'label': 2,
                    'source': 'ToSarcasm'
                })
    
    print(f"âœ… ToSarcasm å¤„ç†å®Œæˆ: {len(all_data)} æ ·æœ¬")
    print(f"   - Label 2 (åè®½): {len(all_data)} æ¡")
    
    return all_data


def balance_and_split_data(data: List[Dict], 
                          train_ratio: float = 0.8,
                          val_ratio: float = 0.1) -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    å¹³è¡¡æ•°æ®å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    
    Args:
        data: æ‰€æœ‰æ•°æ®
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        
    Returns:
        (train_data, val_data, test_data)
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®å¹³è¡¡å’Œåˆ’åˆ†...")
    print("=" * 60)
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„
    label_groups = {0: [], 1: [], 2: []}
    for item in data:
        label_groups[item['label']].append(item)
    
    print(f"åŸå§‹æ•°æ®åˆ†å¸ƒ:")
    for label, items in label_groups.items():
        label_name = ['æ­£é¢', 'è´Ÿé¢', 'åè®½'][label]
        print(f"   Label {label} ({label_name}): {len(items)} æ¡")
    
    # æ‰“ä¹±æ¯ä¸ªæ ‡ç­¾çš„æ•°æ®
    for label in label_groups:
        random.shuffle(label_groups[label])
    
    # åˆ’åˆ†æ¯ä¸ªæ ‡ç­¾çš„æ•°æ®
    train_data = []
    val_data = []
    test_data = []
    
    for label, items in label_groups.items():
        total = len(items)
        train_end = int(total * train_ratio)
        val_end = train_end + int(total * val_ratio)
        
        train_data.extend(items[:train_end])
        val_data.extend(items[train_end:val_end])
        test_data.extend(items[val_end:])
    
    # æ‰“ä¹±æ··åˆåçš„æ•°æ®
    random.shuffle(train_data)
    random.shuffle(val_data)
    random.shuffle(test_data)
    
    print(f"\nåˆ’åˆ†åæ•°æ®é›†:")
    print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡")
    print(f"   éªŒè¯é›†: {len(val_data)} æ¡")
    print(f"   æµ‹è¯•é›†: {len(test_data)} æ¡")
    
    # ç»Ÿè®¡æ¯ä¸ªæ•°æ®é›†çš„æ ‡ç­¾åˆ†å¸ƒ
    for dataset_name, dataset in [('è®­ç»ƒé›†', train_data), ('éªŒè¯é›†', val_data), ('æµ‹è¯•é›†', test_data)]:
        label_counts = Counter(item['label'] for item in dataset)
        print(f"\n{dataset_name}æ ‡ç­¾åˆ†å¸ƒ:")
        for label in [0, 1, 2]:
            label_name = ['æ­£é¢', 'è´Ÿé¢', 'åè®½'][label]
            count = label_counts[label]
            percentage = count / len(dataset) * 100 if dataset else 0
            print(f"   Label {label} ({label_name}): {count} ({percentage:.2f}%)")
    
    return train_data, val_data, test_data


def save_json(data: List[Dict], output_file: str):
    """ä¿å­˜ä¸º JSON æ ¼å¼"""
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # ç§»é™¤ source å­—æ®µï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰
    clean_data = []
    for item in data:
        clean_data.append({
            'text': item['text'],
            'topic': item['topic'],
            'label': item['label']
        })
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(clean_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ”§" * 30)
    print("æ•°æ®æ¸…æ´—è„šæœ¬ - ä¸‰åˆ†ç±»ç‰ˆæœ¬")
    print("ğŸ”§" * 30 + "\n")
    
    print("ğŸ“‹ æ ‡ç­¾ä½“ç³»:")
    print("   Label 0: æ­£å¸¸-æ­£é¢ (Normal-Positive)")
    print("   Label 1: æ­£å¸¸-è´Ÿé¢ (Normal-Negative)")
    print("   Label 2: é˜´é˜³æ€ªæ°” (Sarcastic)")
    print()
    
    # åˆ‡æ¢åˆ° dataset ç›®å½•
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    print(f"ğŸ“‚ å·¥ä½œç›®å½•: {os.getcwd()}\n")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # å¤„ç†å„ä¸ªæ•°æ®é›†
    chnsenticorp_data = process_chnsenticorp()
    weibo_data = process_weibo()
    tosarcasm_data = process_tosarcasm()
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data = chnsenticorp_data + weibo_data + tosarcasm_data
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(all_data)} æ¡")
    print("=" * 60)
    
    # å¹³è¡¡å’Œåˆ’åˆ†æ•°æ®
    train_data, val_data, test_data = balance_and_split_data(all_data)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "processed"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
    print("=" * 60)
    
    save_json(train_data, os.path.join(output_dir, "train.json"))
    save_json(val_data, os.path.join(output_dir, "dev.json"))
    save_json(test_data, os.path.join(output_dir, "test.json"))
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
    print("\nâš ï¸ é‡è¦æç¤º:")
    print("   æ¨¡å‹éœ€è¦ä¿®æ”¹ä¸º 3 åˆ†ç±»:")
    print("   - åœ¨ main.py ä¸­è®¾ç½® --num_classes 3")
    print("   - æˆ–ä¿®æ”¹ model.py ä¸­çš„ num_classes é»˜è®¤å€¼ä¸º 3")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   cd ..")
    print("   python main.py --dataset_dir dataset/processed --num_classes 3")


if __name__ == "__main__":
    main()
