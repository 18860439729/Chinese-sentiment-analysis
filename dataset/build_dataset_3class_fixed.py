"""
æ•°æ®æ¸…æ´—è„šæœ¬ - ä¸‰åˆ†ç±»ç‰ˆæœ¬ (ä¿®å¤ç‰ˆ)
ä¿®å¤å†…å®¹ï¼š
1. Topic å¡«å……ï¼šé˜²æ­¢æ¨¡å‹é€šè¿‡"æœ‰æ—  Topic"ä½œå¼Š
2. é™é‡‡æ ·å¹³è¡¡ï¼šè§£å†³ 3.56% çš„æç«¯ä¸å¹³è¡¡
3. æ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼šå»é™¤è¿‡çŸ­å’Œè¿‡é•¿çš„æ–‡æœ¬
"""

import json
import os
import csv
from pathlib import Path
from typing import List, Dict, Tuple
import random
from collections import Counter


# ================= é…ç½®åŒº =================
# é™é‡‡æ ·ç›®æ ‡æ•°é‡ï¼šè®© Label 0/1 çš„æ•°é‡æ¥è¿‘ Label 2
# Label 2 å¤§çº¦æœ‰ 4800 æ¡ï¼Œæˆ‘ä»¬æŠŠ 0 å’Œ 1 ä¹Ÿé™åˆ¶åœ¨ 6000 æ¡å·¦å³
TARGET_SAMPLE_NUM = 6000

# æ–‡æœ¬é•¿åº¦è¿‡æ»¤
MIN_TEXT_LENGTH = 5
MAX_TEXT_LENGTH = 200
# ==========================================


def load_jsonl(file_path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶"""
    data = []
    if not os.path.exists(file_path):
        return data
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    data.append(json.loads(line.strip()))
                except:
                    pass
    return data


def load_json(file_path: str) -> List[Dict]:
    """åŠ è½½ JSON æ–‡ä»¶"""
    if not os.path.exists(file_path):
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        try:
            return json.load(f)
        except:
            return []


def load_csv(file_path: str) -> List[Dict]:
    """åŠ è½½ CSV æ–‡ä»¶"""
    data = []
    if not os.path.exists(file_path):
        return data
    
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)
    return data


def get_random_topic(source_type: str) -> str:
    """
    ã€å…³é”®ä¿®å¤ã€‘ç”Ÿæˆéšæœºé€šç”¨ Topicï¼Œé˜²æ­¢æ¨¡å‹æ ¹æ® Topic æ˜¯å¦ä¸ºç©ºæ¥ä½œå¼Š
    
    Args:
        source_type: 'chn' æˆ– 'weibo'
        
    Returns:
        éšæœºé€‰æ‹©çš„é€šç”¨ Topic
    """
    if source_type == 'chn':
        topics = [
            "ç”¨æˆ·è¯„ä»·", "è´­ç‰©å¿ƒå¾—", "é…’åº—å…¥ä½ä½“éªŒ", 
            "äº§å“åé¦ˆ", "æœåŠ¡ç‚¹è¯„", "ä¹°å®¶ç§€",
            "å•†å“è¯„è®º", "æ¶ˆè´¹ä½“éªŒ", "ä½¿ç”¨æ„Ÿå—"
        ]
    else:  # weibo
        topics = [
            "å¾®åšçƒ­æœ", "å¿ƒæƒ…è®°å½•", "æ¯æ—¥åæ§½", 
            "ç½‘å‹çƒ­è®®", "ç”Ÿæ´»ç‚¹æ»´", "åƒç“œç°åœº",
            "ä»Šæ—¥è¯é¢˜", "éšæ‰‹ä¸€æ‹", "æ—¥å¸¸åˆ†äº«"
        ]
    return random.choice(topics)


def process_chnsenticorp(raw_dir: str = "raw_source/ChnSentiCorp") -> List[Dict]:
    """
    å¤„ç† ChnSentiCorp æ•°æ®
    - é‡æ–°æ ‡æ³¨ï¼šåŸæ ‡ç­¾ 1(å¥½è¯„) -> Label 0(æ­£é¢), åŸæ ‡ç­¾ 0(å·®è¯„) -> Label 1(è´Ÿé¢)
    - å¡«å…… Topicï¼šé˜²æ­¢æ•°æ®æ³„éœ²
    - é™é‡‡æ ·ï¼šæ§åˆ¶æ•°é‡åœ¨ TARGET_SAMPLE_NUM
    """
    print("=" * 60)
    print(f"ğŸ“¥ å¤„ç† ChnSentiCorp (ç›®æ ‡é‡‡æ ·: {TARGET_SAMPLE_NUM} æ¡)...")
    print("=" * 60)
    
    all_data = []
    
    # è¯»å–æ‰€æœ‰æ•°æ®
    temp_data = []
    for split in ['train', 'validation', 'test']:
        file_path = os.path.join(raw_dir, f"{split}.jsonl")
        temp_data.extend(load_jsonl(file_path))
    
    print(f"åŸå§‹æ•°æ®: {len(temp_data)} æ¡")
    
    # éšæœºæ‰“ä¹±
    random.shuffle(temp_data)
    
    count_0, count_1 = 0, 0
    
    for item in temp_data:
        text = item.get('text', '').strip()
        original_label = item.get('label', 0)
        
        # æ–‡æœ¬é•¿åº¦è¿‡æ»¤
        if len(text) < MIN_TEXT_LENGTH or len(text) > MAX_TEXT_LENGTH:
            continue
        
        # é‡æ–°æ ‡æ³¨ï¼š0: æ­£é¢ (åŸlabel 1), 1: è´Ÿé¢ (åŸlabel 0)
        new_label = 0 if original_label == 1 else 1
        
        # é™é‡‡æ ·æ§åˆ¶
        if new_label == 0 and count_0 >= TARGET_SAMPLE_NUM:
            continue
        if new_label == 1 and count_1 >= TARGET_SAMPLE_NUM:
            continue
        
        all_data.append({
            'text': text,
            'topic': get_random_topic('chn'),  # ã€ä¿®å¤ã€‘å¡«å…… Topic
            'label': new_label,
            'source': 'ChnSentiCorp'
        })
        
        if new_label == 0:
            count_0 += 1
        else:
            count_1 += 1
        
        # ä¸¤ä¸ªç±»åˆ«éƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œåœæ­¢
        if count_0 >= TARGET_SAMPLE_NUM and count_1 >= TARGET_SAMPLE_NUM:
            break
    
    print(f"âœ… ChnSentiCorp å¤„ç†å®Œæˆ:")
    print(f"   - Label 0 (æ­£é¢): {count_0} æ¡")
    print(f"   - Label 1 (è´Ÿé¢): {count_1} æ¡")
    
    return all_data


def process_weibo(raw_file: str = "raw_source/Weibo/weibo_senti_100k.csv") -> List[Dict]:
    """
    å¤„ç† Weibo æ•°æ®
    - é‡æ–°æ ‡æ³¨ï¼šåŸæ ‡ç­¾ 1(æ­£å‘) -> Label 0(æ­£é¢), åŸæ ‡ç­¾ 0(è´Ÿå‘) -> Label 1(è´Ÿé¢)
    - å¡«å…… Topicï¼šé˜²æ­¢æ•°æ®æ³„éœ²
    - é™é‡‡æ ·ï¼šæ§åˆ¶æ•°é‡åœ¨ TARGET_SAMPLE_NUM
    """
    print("\n" + "=" * 60)
    print(f"ğŸ“¥ å¤„ç† Weibo (ç›®æ ‡é‡‡æ ·: {TARGET_SAMPLE_NUM} æ¡)...")
    print("=" * 60)
    
    raw_data = load_csv(raw_file)
    print(f"åŸå§‹æ•°æ®: {len(raw_data)} æ¡")
    
    # éšæœºæ‰“ä¹±
    random.shuffle(raw_data)
    
    processed_data = []
    count_0, count_1 = 0, 0
    
    for item in raw_data:
        text = item.get('review', '').strip()
        
        try:
            original_label = int(item.get('label', 0))
        except:
            continue
        
        # æ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼ˆå¾®åšé™åˆ¶ 140 å­—ï¼‰
        if len(text) < MIN_TEXT_LENGTH or len(text) > 140:
            continue
        
        # é‡æ–°æ ‡æ³¨ï¼š0: æ­£é¢ (åŸlabel 1), 1: è´Ÿé¢ (åŸlabel 0)
        new_label = 0 if original_label == 1 else 1
        
        # é™é‡‡æ ·æ§åˆ¶
        if new_label == 0 and count_0 >= TARGET_SAMPLE_NUM:
            continue
        if new_label == 1 and count_1 >= TARGET_SAMPLE_NUM:
            continue
        
        processed_data.append({
            'text': text,
            'topic': get_random_topic('weibo'),  # ã€ä¿®å¤ã€‘å¡«å…… Topic
            'label': new_label,
            'source': 'Weibo'
        })
        
        if new_label == 0:
            count_0 += 1
        else:
            count_1 += 1
        
        # ä¸¤ä¸ªç±»åˆ«éƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œåœæ­¢
        if count_0 >= TARGET_SAMPLE_NUM and count_1 >= TARGET_SAMPLE_NUM:
            break
    
    print(f"âœ… Weibo å¤„ç†å®Œæˆ:")
    print(f"   - Label 0 (æ­£é¢): {count_0} æ¡")
    print(f"   - Label 1 (è´Ÿé¢): {count_1} æ¡")
    
    return processed_data


def process_tosarcasm(raw_dir: str = "raw_source/ToSarcasm") -> List[Dict]:
    """
    å¤„ç† ToSarcasm æ•°æ®
    - å…¨éƒ¨æ ‡æ³¨ä¸º Label 2 (åè®½)
    - ä¿ç•™çœŸå®çš„ Topic (æ–°é—»æ ‡é¢˜)
    """
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¤„ç† ToSarcasm (å…¨éƒ¨ä¿ç•™)...")
    print("=" * 60)
    
    all_data = []
    
    for split in ['train', 'dev', 'test']:
        file_path = os.path.join(raw_dir, f"{split}.json")
        for item in load_json(file_path):
            text = item.get('text', '').strip()
            topic = item.get('topic', '').strip()
            
            if text:
                all_data.append({
                    'text': text,
                    'topic': topic,  # ä¿ç•™çœŸå®æ–°é—»æ ‡é¢˜
                    'label': 2,  # é˜´é˜³æ€ªæ°”
                    'source': 'ToSarcasm'
                })
    
    print(f"âœ… ToSarcasm å¤„ç†å®Œæˆ: {len(all_data)} æ¡")
    print(f"   - Label 2 (åè®½): {len(all_data)} æ¡")
    
    return all_data


def save_and_split(data: List[Dict], output_dir: str, 
                   train_ratio: float = 0.8, val_ratio: float = 0.1):
    """
    ä¿å­˜å¹¶åˆ’åˆ†æ•°æ®é›†
    
    Args:
        data: æ‰€æœ‰æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š åˆ’åˆ†æ•°æ®é›†...")
    print("=" * 60)
    
    # æ‰“ä¹±æ•°æ®
    random.shuffle(data)
    
    total = len(data)
    train_size = int(total * train_ratio)
    val_size = int(total * val_ratio)
    
    train_data = data[:train_size]
    val_data = data[train_size:train_size + val_size]
    test_data = data[train_size + val_size:]
    
    os.makedirs(output_dir, exist_ok=True)
    
    def _save(d, name):
        """ä¿å­˜æ•°æ®å¹¶æ‰“å°åˆ†å¸ƒ"""
        # ç§»é™¤ source å­—æ®µ
        clean_data = []
        for item in d:
            clean_data.append({
                'text': item['text'],
                'topic': item['topic'],
                'label': item['label']
            })
        
        path = os.path.join(output_dir, name)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(clean_data, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°åˆ†å¸ƒ
        cnt = Counter([x['label'] for x in d])
        total_count = len(d)
        print(f"\nğŸ’¾ {name}:")
        print(f"   æ€»æ•°: {total_count}")
        print(f"   Label 0 (æ­£é¢): {cnt[0]:5d} ({cnt[0]/total_count*100:5.2f}%)")
        print(f"   Label 1 (è´Ÿé¢): {cnt[1]:5d} ({cnt[1]/total_count*100:5.2f}%)")
        print(f"   Label 2 (åè®½): {cnt[2]:5d} ({cnt[2]/total_count*100:5.2f}%)")
    
    _save(train_data, "train.json")
    _save(val_data, "dev.json")
    _save(test_data, "test.json")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ”§" * 30)
    print("æ•°æ®æ¸…æ´—è„šæœ¬ - ä¸‰åˆ†ç±»ç‰ˆæœ¬ (ä¿®å¤ç‰ˆ)")
    print("ğŸ”§" * 30 + "\n")
    
    print("ğŸ”´ ä¿®å¤å†…å®¹:")
    print("   1. Topic å¡«å……ï¼šé˜²æ­¢æ¨¡å‹é€šè¿‡'æœ‰æ—  Topic'ä½œå¼Š")
    print("   2. é™é‡‡æ ·å¹³è¡¡ï¼šè§£å†³ 3.56% çš„æç«¯ä¸å¹³è¡¡")
    print("   3. æ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼šå»é™¤è¿‡çŸ­å’Œè¿‡é•¿çš„æ–‡æœ¬")
    print()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # åˆ‡æ¢åˆ° dataset ç›®å½•
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    print(f"ğŸ“‚ å·¥ä½œç›®å½•: {os.getcwd()}\n")
    
    # 1. è·å–æ•°æ® (å¸¦é™é‡‡æ ·å’Œ Topic å¡«å……)
    d1 = process_chnsenticorp()
    d2 = process_weibo()
    d3 = process_tosarcasm()
    
    # 2. åˆå¹¶
    final_data = d1 + d2 + d3
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆæ•°æ®åˆ†å¸ƒ")
    print("=" * 60)
    
    cnt = Counter([x['label'] for x in final_data])
    total = len(final_data)
    
    print(f"æ€»æ•°æ®é‡: {total} æ¡\n")
    print(f"Label 0 (æ­£é¢): {cnt[0]:5d} ({cnt[0]/total*100:5.2f}%)")
    print(f"Label 1 (è´Ÿé¢): {cnt[1]:5d} ({cnt[1]/total*100:5.2f}%)")
    print(f"Label 2 (åè®½): {cnt[2]:5d} ({cnt[2]/total*100:5.2f}%)")
    print("=" * 60)
    
    # æ£€æŸ¥å¹³è¡¡æ€§
    min_count = min(cnt.values())
    max_count = max(cnt.values())
    ratio = max_count / min_count if min_count > 0 else float('inf')
    
    print(f"\nğŸ“ˆ ç±»åˆ«å¹³è¡¡æ€§:")
    print(f"   æœ€å°ç±»åˆ«: {min_count} æ¡")
    print(f"   æœ€å¤§ç±»åˆ«: {max_count} æ¡")
    print(f"   ä¸å¹³è¡¡æ¯”ä¾‹: {ratio:.2f}:1")
    
    if ratio < 2.0:
        print("   âœ… ç±»åˆ«ç›¸å¯¹å¹³è¡¡ï¼ˆ< 2:1ï¼‰")
    elif ratio < 5.0:
        print("   âš ï¸ ç±»åˆ«ç•¥æœ‰ä¸å¹³è¡¡ï¼ˆ2:1 ~ 5:1ï¼‰")
    else:
        print("   âŒ ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼ˆ> 5:1ï¼‰")
    
    # 3. ä¿å­˜
    save_and_split(final_data, "processed")
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath('processed')}")
    print("\nâš ï¸ é‡è¦æç¤º:")
    print("   1. æ‰€æœ‰æ•°æ®éƒ½æœ‰ Topicï¼ˆé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰")
    print("   2. ç±»åˆ«å·²å¹³è¡¡ï¼ˆé™é‡‡æ ·ï¼‰")
    print("   3. æ–‡æœ¬é•¿åº¦å·²è¿‡æ»¤")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   cd ..")
    print("   python main.py --dataset_dir dataset/processed --num_classes 3")


if __name__ == "__main__":
    main()
