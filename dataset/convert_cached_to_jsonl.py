"""
ä»ç¼“å­˜çš„æ•°æ®é›†æ–‡ä»¶è½¬æ¢ä¸º JSONL
"""

import os
import json
from pathlib import Path
from datasets import Dataset


def convert_dataset_cache(cache_dir: str, output_dir: str):
    """
    ä»ç¼“å­˜ç›®å½•åŠ è½½æ•°æ®é›†å¹¶è½¬æ¢ä¸º JSONL
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"ğŸ“ å¤„ç†ç›®å½•: {cache_dir}")
    
    cache_path = Path(cache_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾ arrow æ–‡ä»¶
    arrow_files = list(cache_path.glob("*.arrow"))
    
    if not arrow_files:
        print(f"   âš ï¸ æ²¡æœ‰æ‰¾åˆ° Arrow æ–‡ä»¶")
        return False
    
    print(f"   æ‰¾åˆ° {len(arrow_files)} ä¸ªæ–‡ä»¶")
    
    success_count = 0
    
    for arrow_file in arrow_files:
        try:
            # ç¡®å®šåˆ†å‰²åç§°
            # chn_senti_corp-train.arrow -> train
            base_name = arrow_file.stem.replace("chn_senti_corp-", "")
            
            print(f"\nğŸ“„ è½¬æ¢: {arrow_file.name} -> {base_name}.jsonl")
            
            # ä½¿ç”¨ datasets åº“åŠ è½½
            dataset = Dataset.from_file(str(arrow_file))
            
            print(f"   - æ ·æœ¬æ•°: {len(dataset)}")
            print(f"   - ç‰¹å¾: {list(dataset.features.keys())}")
            
            # ä¿å­˜ä¸º JSONL
            output_file = output_path / f"{base_name}.jsonl"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for item in dataset:
                    json.dump(item, f, ensure_ascii=False)
                    f.write('\n')
            
            print(f"   âœ… å·²ä¿å­˜åˆ°: {output_file}")
            success_count += 1
            
        except Exception as e:
            print(f"   âŒ è½¬æ¢å¤±è´¥: {e}")
    
    return success_count > 0


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ”„" * 30)
    print("ç¼“å­˜æ•°æ®é›†è½¬æ¢å·¥å…·")
    print("ğŸ”„" * 30 + "\n")
    
    # ChnSentiCorp
    chn_cache = "raw_source/ChnSentiCorp"
    chn_output = "raw_source/ChnSentiCorp"
    
    print("=" * 60)
    print("ğŸ“¥ è½¬æ¢ ChnSentiCorp æ•°æ®é›†")
    print("=" * 60)
    
    if convert_dataset_cache(chn_cache, chn_output):
        print("\nâœ… ChnSentiCorp è½¬æ¢å®Œæˆï¼")
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        info_file = Path(chn_output) / "dataset_info.txt"
        jsonl_files = list(Path(chn_output).glob("*.jsonl"))
        
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write("ChnSentiCorp æ•°æ®é›†ä¿¡æ¯\n")
            f.write("=" * 50 + "\n")
            f.write(f"æ¥æº: HuggingFace - seamew/ChnSentiCorp\n")
            f.write(f"ä»»åŠ¡: æƒ…æ„Ÿåˆ†æï¼ˆæ­£é¢/è´Ÿé¢ï¼‰\n")
            f.write(f"æ–‡ä»¶: {[f.name for f in jsonl_files]}\n")
        
        print(f"ğŸ“„ æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")
    else:
        print("\nâŒ ChnSentiCorp è½¬æ¢å¤±è´¥")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   python build_dataset.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
