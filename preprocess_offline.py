"""
ç¦»çº¿é¢„å¤„ç†è„šæœ¬ - å®Œå…¨é‡å†™ç‰ˆæœ¬
å½»åº•è§£è€¦ï¼Œä¸ä¾èµ– data_preprocess.pyï¼Œé¿å…ç¯å¢ƒå†²çª
ç‹¬ç«‹åŠ è½½çº¯å‡€çš„ BertTokenizer å’Œ HanLP
"""

import json
import pickle
import os
import argparse
from tqdm import tqdm
from typing import List, Dict, Any, Tuple
import torch
from collections import defaultdict

# ç‹¬ç«‹å¯¼å…¥ï¼Œé¿å…å‘½åç©ºé—´å†²çª
print("ğŸ”§ å¯¼å…¥ä¾èµ–åº“...")
try:
    from transformers import BertTokenizer
    print("âœ… transformers.BertTokenizer å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ transformers å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·å®‰è£…: pip install transformers")
    exit(1)

try:
    import hanlp
    print("âœ… hanlp å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ hanlp å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·å®‰è£…: pip install hanlp")
    exit(1)


class IndependentPreprocessor:
    """
    å®Œå…¨ç‹¬ç«‹çš„é¢„å¤„ç†å™¨
    ä¸ä¾èµ–ä»»ä½•å…¶ä»–æ¨¡å—ï¼Œé¿å…ç¯å¢ƒå†²çª
    """
    
    def __init__(self, bert_model_name: str = 'bert-base-chinese'):
        """åˆå§‹åŒ–ç‹¬ç«‹çš„é¢„å¤„ç†å™¨"""
        print(f"ğŸš€ åˆå§‹åŒ–ç‹¬ç«‹é¢„å¤„ç†å™¨...")
        
        # 1. åˆå§‹åŒ–çº¯å‡€çš„ BERT tokenizer
        print(f"ğŸ“¥ åŠ è½½ BERT tokenizer: {bert_model_name}")
        try:
            self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)
            print("âœ… BERT tokenizer åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ BERT tokenizer åŠ è½½å¤±è´¥: {e}")
            raise e
        
        # 2. åˆå§‹åŒ– HanLP æ¨¡å‹
        print("ğŸ“¥ åŠ è½½ HanLP æ¨¡å‹...")
        try:
            self.hanlp_model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
            print("âœ… HanLP æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ HanLP æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
        
        print("ğŸ‰ ç‹¬ç«‹é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def bert_encode(self, text: str, topic: str, max_length: int = 512) -> Dict[str, torch.Tensor]:
        """
        çº¯å‡€çš„ BERT ç¼–ç ï¼Œä¸ä¸å…¶ä»–åº“å†²çª
        
        Args:
            text: è¯„è®ºæ–‡æœ¬
            topic: ä¸»é¢˜æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            BERT ç¼–ç ç»“æœ
        """
        try:
            # ä½¿ç”¨çº¯å‡€çš„ transformers tokenizer
            encoded = self.bert_tokenizer(
                text,                    # ç¬¬ä¸€ä¸ªå¥å­
                topic,                   # ç¬¬äºŒä¸ªå¥å­
                add_special_tokens=True, # æ·»åŠ  [CLS], [SEP]
                max_length=max_length,   # æœ€å¤§é•¿åº¦
                padding='max_length',    # å¡«å……åˆ°æœ€å¤§é•¿åº¦
                truncation=True,         # æˆªæ–­è¶…é•¿æ–‡æœ¬
                return_tensors='pt'      # è¿”å› PyTorch å¼ é‡
            )
            
            return {
                'input_ids': encoded['input_ids'].squeeze(0),
                'attention_mask': encoded['attention_mask'].squeeze(0),
                'token_type_ids': encoded.get('token_type_ids', torch.zeros_like(encoded['input_ids'])).squeeze(0)
            }
            
        except Exception as e:
            print(f"âŒ BERT ç¼–ç å¤±è´¥: {e}")
            print(f"   æ–‡æœ¬: {text[:100]}...")
            print(f"   ä¸»é¢˜: {topic[:100]}...")
            raise e
    
    def hanlp_analyze(self, text: str, topic: str = None) -> Dict[str, Any]:
        """
        çº¯å‡€çš„ HanLP åˆ†æï¼Œä¸ä¸å…¶ä»–åº“å†²çª
        
        Args:
            text: è¯„è®ºæ–‡æœ¬
            topic: ä¸»é¢˜æ–‡æœ¬
            
        Returns:
            HanLP åˆ†æç»“æœ
        """
        try:
            # åˆå¹¶æ–‡æœ¬è¿›è¡Œåˆ†æ
            combined_text = text
            if topic:
                combined_text = f"{topic} {text}"
            
            # HanLP å¤šä»»åŠ¡åˆ†æ
            result = self.hanlp_model(combined_text)
            
            # åˆ†åˆ«åˆ†æä¸»é¢˜å’Œæ–‡æœ¬é•¿åº¦
            topic_length = 0
            text_length = 0
            
            if topic:
                try:
                    topic_result = self.hanlp_model(topic)
                    topic_length = len(topic_result.get('tok', []))
                except:
                    topic_length = 0
            
            try:
                text_result = self.hanlp_model(text)
                text_length = len(text_result.get('tok', []))
            except:
                text_length = 0
            
            return {
                'tokens': result.get('tok', []),
                'pos_tags': result.get('pos', []),
                'ner_tags': result.get('ner', []),
                'dependencies': result.get('dep', []),
                'semantic_roles': result.get('srl', []),
                'topic_length': topic_length,
                'text_length': text_length
            }
            
        except Exception as e:
            print(f"âŒ HanLP åˆ†æå¤±è´¥: {e}")
            print(f"   æ–‡æœ¬: {text[:100]}...")
            if topic:
                print(f"   ä¸»é¢˜: {topic[:100]}...")
            
            # è¿”å›ç©ºç»“æœï¼Œé¿å…ç¨‹åºå´©æºƒ
            return {
                'tokens': [],
                'pos_tags': [],
                'ner_tags': [],
                'dependencies': [],
                'semantic_roles': [],
                'topic_length': 0,
                'text_length': 0
            }
    
    def process_single_sample(self, text: str, topic: str, label: int, max_length: int = 512) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬
        
        Args:
            text: è¯„è®ºæ–‡æœ¬
            topic: ä¸»é¢˜æ–‡æœ¬
            label: æ ‡ç­¾
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            å¤„ç†åçš„æ ·æœ¬æ•°æ®
        """
        # BERT ç¼–ç 
        bert_result = self.bert_encode(text, topic, max_length)
        
        # HanLP åˆ†æ
        hanlp_result = self.hanlp_analyze(text, topic)
        
        # éªŒè¯ç»“æœ
        if bert_result['input_ids'].numel() == 0:
            raise ValueError("BERT ç¼–ç ç»“æœä¸ºç©º")
        
        if len(hanlp_result['tokens']) == 0:
            raise ValueError("HanLP åˆ†æç»“æœä¸ºç©º")
        
        return {
            'text': text,
            'topic': topic,
            'label': label,
            'input_ids': bert_result['input_ids'],
            'attention_mask': bert_result['attention_mask'],
            'token_type_ids': bert_result['token_type_ids'],
            'hanlp_result': hanlp_result
        }


def load_json_dataset(file_path: str) -> List[Tuple[str, str, int]]:
    """
    åŠ è½½ JSON æ•°æ®é›†
    
    Args:
        file_path: JSON æ–‡ä»¶è·¯å¾„
        
    Returns:
        (text, topic, label) åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        dataset = []
        for item in raw_data:
            text = item.get('text', '').strip()
            topic = item.get('topic', '').strip()
            label = int(item.get('label', '0'))
            
            if text:  # åªä¿ç•™éç©ºæ–‡æœ¬
                dataset.append((text, topic, label))
        
        return dataset
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return []


def preprocess_dataset(dataset_dir: str = "dataset", 
                      output_dir: str = "preprocessed_data",
                      bert_model_name: str = "bert-base-chinese",
                      max_length: int = 512):
    """
    ç¦»çº¿é¢„å¤„ç†æ•°æ®é›† - å®Œå…¨ç‹¬ç«‹ç‰ˆæœ¬
    
    Args:
        dataset_dir: åŸå§‹æ•°æ®é›†ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        bert_model_name: BERT æ¨¡å‹åç§°
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
    """
    print("ğŸš€ å¼€å§‹ç¦»çº¿é¢„å¤„ç†ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–ç‹¬ç«‹é¢„å¤„ç†å™¨
    try:
        preprocessor = IndependentPreprocessor(bert_model_name)
    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    total_success = 0
    total_samples = 0
    
    for split in ['train', 'dev', 'test']:
        print(f"\n{'='*50}")
        print(f"ğŸ”„ å¤„ç† {split.upper()} æ•°æ®é›†")
        print(f"{'='*50}")
        
        # æ–‡ä»¶è·¯å¾„
        json_file = os.path.join(dataset_dir, f'{split}.json')
        output_file = os.path.join(output_dir, f'{split}_preprocessed.pkl')
        
        if not os.path.exists(json_file):
            print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {json_file}")
            continue
        
        # åŠ è½½åŸå§‹æ•°æ®
        print(f"ğŸ“¥ åŠ è½½æ•°æ®: {json_file}")
        raw_dataset = load_json_dataset(json_file)
        
        if not raw_dataset:
            print(f"âŒ æ•°æ®é›†ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")
            continue
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(raw_dataset)} æ ·æœ¬")
        
        # é€æ ·æœ¬å¤„ç†
        processed_data = []
        error_count = 0
        
        for i, (text, topic, label) in enumerate(tqdm(raw_dataset, desc=f"Processing {split}")):
            try:
                # å¤„ç†å•ä¸ªæ ·æœ¬
                processed_sample = preprocessor.process_single_sample(text, topic, label, max_length)
                processed_data.append(processed_sample)
                
            except Exception as e:
                error_count += 1
                if error_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"\nâŒ æ ·æœ¬ {i+1} å¤„ç†å¤±è´¥: {str(e)}")
                    print(f"   æ–‡æœ¬: {text[:50]}...")
                elif error_count == 4:
                    print(f"\nâš ï¸  æ›´å¤šé”™è¯¯å°†ä¸å†æ˜¾ç¤º...")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        if processed_data:
            try:
                with open(output_file, 'wb') as f:
                    pickle.dump(processed_data, f)
                
                success_rate = len(processed_data) / len(raw_dataset) * 100
                print(f"\nâœ… {split.upper()} æ•°æ®é›†å¤„ç†å®Œæˆ:")
                print(f"   ğŸ“Š æˆåŠŸ: {len(processed_data)} æ ·æœ¬")
                print(f"   âŒ å¤±è´¥: {error_count} æ ·æœ¬")
                print(f"   ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"   ğŸ’¾ ä¿å­˜è‡³: {output_file}")
                
                total_success += len(processed_data)
                total_samples += len(raw_dataset)
                
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        else:
            print(f"âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬")
    
    # æ€»ç»“
    print(f"\n{'='*50}")
    print(f"ğŸ‰ é¢„å¤„ç†å®Œæˆ")
    print(f"{'='*50}")
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   æˆåŠŸæ ·æœ¬: {total_success}")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    if total_samples > 0:
        overall_success_rate = total_success / total_samples * 100
        print(f"   æ€»æˆåŠŸç‡: {overall_success_rate:.1f}%")
    print(f"ğŸ“ é¢„å¤„ç†æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ’¡ è®­ç»ƒæ—¶ä½¿ç”¨: python main.py --use_preprocessed")


# ç‹¬ç«‹çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œç”¨äºè®­ç»ƒæ—¶åŠ è½½é¢„å¤„ç†æ•°æ®
def load_preprocessed_data(preprocessed_dir: str = "preprocessed_data") -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    åŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®
    
    Args:
        preprocessed_dir: é¢„å¤„ç†æ•°æ®ç›®å½•
        
    Returns:
        (train_data, dev_data, test_data) å…ƒç»„
    """
    datasets = {}
    
    for split in ['train', 'dev', 'test']:
        pkl_file = os.path.join(preprocessed_dir, f'{split}_preprocessed.pkl')
        
        if os.path.exists(pkl_file):
            try:
                with open(pkl_file, 'rb') as f:
                    datasets[split] = pickle.load(f)
                print(f"ğŸ“¥ åŠ è½½ {split} æ•°æ®: {len(datasets[split])} æ ·æœ¬")
            except Exception as e:
                print(f"âŒ åŠ è½½ {split} æ•°æ®å¤±è´¥: {e}")
                datasets[split] = []
        else:
            datasets[split] = []
            print(f"âš ï¸  æœªæ‰¾åˆ° {split} é¢„å¤„ç†æ–‡ä»¶: {pkl_file}")
    
    return datasets.get('train', []), datasets.get('dev', []), datasets.get('test', [])


class PreprocessedDataset:
    """é¢„å¤„ç†æ•°æ®é›†ç±» - ç›´æ¥åŠ è½½é¢„å¤„ç†ç»“æœï¼Œæ— éœ€é‡å¤è®¡ç®—"""
    
    def __init__(self, preprocessed_data: List[Dict[str, Any]]):
        self.data = preprocessed_data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            'input_ids': item['input_ids'],
            'attention_mask': item['attention_mask'],
            'token_type_ids': item['token_type_ids'],
            'hanlp_result': item['hanlp_result'],
            'label': item['label'],
            'text': item['text'],
            'topic': item['topic']
        }


def create_fast_data_loaders(train_data, val_data, batch_size):
    """
    ç»ˆæä¿®æ­£ç‰ˆåŠ è½½å™¨ï¼š
    1. åŒ…å« FastDataset é˜²æ­¢è¯»å– text æŠ¥é”™
    2. åŒ…å« build_hypergraph_matrix è¡¥å…¨ç¼ºå¤±çš„ hypergraph_matrix é”®
    """
    from torch.utils.data import Dataset, DataLoader
    import torch
    import numpy as np
    from collections import defaultdict

    # --- 1. å®šä¹‰æ•°æ®é›†ç±» ---
    class FastDataset(Dataset):
        def __init__(self, data):
            self.data = data

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            return self.data[idx]

    # --- 2. å†…ç½®è¶…å›¾æ„å»ºé€»è¾‘ (ä¸ºäº†è®© main.py èƒ½æ‹¿åˆ° hypergraph_matrix) ---
    def build_single_matrix(hanlp_result, max_len=512, max_edges=100):
        """ç®€åŒ–çš„è¶…å›¾æ„å»ºé€»è¾‘ï¼Œç¡®ä¿æ¨¡å‹æœ‰ä¸œè¥¿å¯ç®—"""
        try:
            tokens = hanlp_result.get('tok', [])
            if not tokens: tokens = hanlp_result.get('tokens', [])  # å…¼å®¹ä¸åŒ key

            # åˆå§‹åŒ–çŸ©é˜µ [N, M]
            matrix = np.zeros((max_len, max_edges), dtype=np.float32)
            edge_idx = 0

            # ç­–ç•¥A: ç®€å•çš„æ»‘åŠ¨çª—å£æ„å»ºè¶…è¾¹ (æœ€ç¨³å¥ï¼Œä¸ä¾èµ–å¤æ‚å¥æ³•)
            window_size = 3
            seq_len = min(len(tokens), max_len)

            for i in range(max(0, seq_len - window_size + 1)):
                if edge_idx >= max_edges: break
                # å°†çª—å£å†…çš„è¯è¿æ¥åˆ°åŒä¸€æ¡è¶…è¾¹
                for w in range(window_size):
                    if i + w < max_len:
                        matrix[i + w, edge_idx] = 1.0
                edge_idx += 1

            # ç­–ç•¥B: ä¾å­˜å…³ç³»æ„å»º (å¦‚æœå­˜åœ¨)
            deps = hanlp_result.get('dep', [])
            if not deps: deps = hanlp_result.get('dependencies', [])

            if deps:
                for dep in deps:
                    if edge_idx >= max_edges: break
                    if len(dep) >= 2:
                        head, tail = dep[0], dep[1] - 1  # HanLPç´¢å¼•é€šå¸¸ä»1å¼€å§‹
                        if head < max_len and tail < max_len and tail >= 0:
                            matrix[head, edge_idx] = 1.0
                            matrix[tail, edge_idx] = 1.0
                            edge_idx += 1

            return matrix
        except Exception:
            # ä¸‡ä¸€å‡ºé”™ï¼Œè¿”å›å…¨é›¶çŸ©é˜µé˜²æ­¢ç¨‹åºå´©æºƒ
            return np.zeros((max_len, max_edges), dtype=np.float32)

    # --- 3. å®šä¹‰æ‰“åŒ…å‡½æ•° ---
    def fast_collate(batch):
        # æå–åŸºç¡€å¼ é‡
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        token_type_ids = torch.stack([item['token_type_ids'] for item in batch])
        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)

        # --- å…³é”®ä¿®å¤ï¼šç°åœºæ„å»ºè¶…å›¾çŸ©é˜µ ---
        # ä½ çš„ main.py éœ€è¦ batch['hypergraph_matrix']ï¼Œæˆ‘ä»¬è¿™é‡Œé€ ç»™å®ƒ
        matrices = []
        for item in batch:
            # ä» hanlp_result æ„å»ºçŸ©é˜µ
            mat = build_single_matrix(item['hanlp_result'])
            matrices.append(mat)

        hypergraph_matrix = torch.tensor(np.stack(matrices), dtype=torch.float32)

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids,
            'labels': labels,
            'hypergraph_matrix': hypergraph_matrix,  # <--- è¡¥ä¸Šäº†è¿™ä¸ªå…³é”®çš„ Key
            'hanlp_results': [item['hanlp_result'] for item in batch]
        }

    # --- 4. åˆ›å»º DataLoader ---
    train_loader = DataLoader(FastDataset(train_data), batch_size=batch_size, shuffle=True, collate_fn=fast_collate)
    val_loader = DataLoader(FastDataset(val_data), batch_size=batch_size, shuffle=False, collate_fn=fast_collate)

    return train_loader, val_loader

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ç¦»çº¿é¢„å¤„ç†æ•°æ®é›† - ç‹¬ç«‹ç‰ˆæœ¬')
    parser.add_argument('--dataset_dir', type=str, default='dataset', help='åŸå§‹æ•°æ®é›†ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='preprocessed_data', help='é¢„å¤„ç†ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--bert_model', type=str, default='bert-base-chinese', help='BERTæ¨¡å‹åç§°')
    parser.add_argument('--max_length', type=int, default=512, help='æœ€å¤§åºåˆ—é•¿åº¦')
    
    args = parser.parse_args()
    
    preprocess_dataset(
        dataset_dir=args.dataset_dir,
        output_dir=args.output_dir,
        bert_model_name=args.bert_model,
        max_length=args.max_length
    )