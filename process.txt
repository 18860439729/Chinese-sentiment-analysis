# é¡¹ç›®å¼€å‘æ—¥å¿— - Process Log

## 2024-02-05 é¡¹ç›®åˆå§‹åŒ–

### 1. æ¨¡å—åŒ–é‡æ„
- åˆ›å»º data_preprocess.pyï¼šHanLPå¤„ç† + è¶…å›¾ç»“æ„ç”Ÿæˆ
- åˆ›å»º model.pyï¼šBERT + HGNN + Attention ç½‘ç»œç»“æ„  
- åˆ›å»º utils.pyï¼šå·¥å…·å‡½æ•°é›†åˆ
- åˆ›å»º main.pyï¼šå®éªŒå…¥å£å’Œè®­ç»ƒæµç¨‹
- åˆ›å»º requirements.txtï¼šä¾èµ–åŒ…ç®¡ç†
- åˆ›å»º README.mdï¼šé¡¹ç›®æ–‡æ¡£

### 2. ç¡¬ä»¶æ— å…³æ€§ä¼˜åŒ–
- ç»Ÿä¸€ä½¿ç”¨æ ‡å‡†å†™æ³•ï¼š`device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`
- æ‰€æœ‰æ¨¡å‹å’Œå¼ é‡åˆå§‹åŒ–æ—¶æ·»åŠ  `.to(device)`
- æ›´æ–° requirements.txt æ·»åŠ  DHG åº“
- ç¡®ä¿ä»£ç åœ¨ CPU/GPU ç¯å¢ƒæ— ç¼åˆ‡æ¢

### 3. é¡¹ç›®ç»“æ„
```
â”œâ”€â”€ data_preprocess.py    # æ•°æ®é¢„å¤„ç†
â”œâ”€â”€ model.py             # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”œâ”€â”€ main.py              # ä¸»ç¨‹åº
â”œâ”€â”€ requirements.txt     # ä¾èµ–ç®¡ç†
â”œâ”€â”€ README.md           # é¡¹ç›®æ–‡æ¡£
â””â”€â”€ process.txt         # å¼€å‘æ—¥å¿—
```

### 4. æ ¸å¿ƒåŠŸèƒ½
- HanLP æ–‡æœ¬å¤„ç†ï¼ˆåˆ†è¯ã€è¯æ€§ã€NERã€ä¾å­˜åˆ†æï¼‰
- è¶…å›¾ç»“æ„æ„å»ºï¼ˆèŠ‚ç‚¹ç‰¹å¾ + è¾¹è¿æ¥ï¼‰
- BERT + HGNN + MultiHead Attention èåˆæ¨¡å‹
- å®Œæ•´è®­ç»ƒ/éªŒè¯/æµ‹è¯•æµç¨‹
- å¯è§†åŒ–å’ŒæŒ‡æ ‡è®¡ç®—

### 5. ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] æµ‹è¯•æ•°æ®é¢„å¤„ç†æ¨¡å—
- [ ] éªŒè¯æ¨¡å‹å‰å‘ä¼ æ’­
- [ ] å®Œå–„è®­ç»ƒå¾ªç¯ä¸­çš„æ•°æ®æµ
- [ ] æ·»åŠ è¶…å‚æ•°è°ƒä¼˜åŠŸèƒ½
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†

---
*è®°å½•æ ¼å¼ï¼šæ—¥æœŸ + æ“ä½œç±»å‹ + å…·ä½“å†…å®¹*

## 2024-02-05 é‡å¤§é”™è¯¯ä¿®æ­£

### 1. BERT Tokenizer ä¿®æ­£ âŒâ¡ï¸âœ…
**é”™è¯¯**: è‡ªå·±æ„å»ºè¯æ±‡è¡¨ï¼Œä¸BERTé¢„è®­ç»ƒtokenizerä¸åŒ¹é…
**ä¿®æ­£**: 
- åˆ é™¤ `_build_vocabulary()` å‡½æ•°
- ä½¿ç”¨ `BertTokenizer.from_pretrained()` 
- ç¡®ä¿token IDsä¸BERTé¢„è®­ç»ƒæ¨¡å‹ä¸€è‡´

### 2. è¶…å›¾ç»“æ„ä¿®æ­£ âŒâ¡ï¸âœ…
**é”™è¯¯**: ä½¿ç”¨æ™®é€šå›¾é‚»æ¥çŸ©é˜µ NÃ—N
**ä¿®æ­£**: 
- æ”¹ä¸ºè¶…å›¾å…³è”çŸ©é˜µ H âˆˆ R^{NÃ—M}
- Nä¸ªèŠ‚ç‚¹(è¯ä½ç½®)ï¼ŒMæ¡è¶…è¾¹(è¯­è¨€å­¦ç»“æ„)
- è¶…è¾¹ç±»å‹ï¼šä¾å­˜å¥æ³•ç°‡ã€è¯æ€§æ ‡æ³¨ç°‡ã€å‘½åå®ä½“ç°‡ã€æ»‘åŠ¨çª—å£

### 3. è¶…å›¾å·ç§¯å…¬å¼ä¿®æ­£ âŒâ¡ï¸âœ…
**é”™è¯¯**: ç®€å•çŸ©é˜µç›¸ä¹˜ï¼Œç¼ºå°‘åº¦çŸ©é˜µå½’ä¸€åŒ–
**ä¿®æ­£**: 
- å®ç°å®Œæ•´å…¬å¼ï¼šX^{(l+1)} = Ïƒ(D_v^{-1/2} H W D_e^{-1} H^T D_v^{-1/2} X^{(l)} Î˜)
- æ·»åŠ èŠ‚ç‚¹åº¦çŸ©é˜µ D_v å’Œè¶…è¾¹åº¦çŸ©é˜µ D_e
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### 4. é…ç½®ç®¡ç†ä¼˜åŒ– âœ…
- åˆ›å»º config.yaml é¿å…ç¡¬ç¼–ç è·¯å¾„
- HanLPæ¨¡å‹åœ¨ __init__ ä¸­ä¸€æ¬¡æ€§åŠ è½½
- æ”¯æŒçµæ´»çš„å‚æ•°é…ç½®

### 5. æ•°æ®æµä¿®æ­£ âœ…
- BERT tokenization ä¸ HanLP å¤„ç†åˆ†ç¦»
- æ­£ç¡®çš„å¼ é‡ç»´åº¦å’Œè®¾å¤‡ç®¡ç†
- ä¿®æ­£æ•°æ®åŠ è½½å™¨è¿”å›æ ¼å¼

### 6. æŠ€æœ¯å€ºåŠ¡æ¸…ç†
- [x] åˆ é™¤é”™è¯¯çš„è¯æ±‡è¡¨æ„å»º
- [x] ä¿®æ­£è¶…å›¾æ•°å­¦å®šä¹‰
- [x] å®ç°æ­£ç¡®çš„è¶…å›¾å·ç§¯
- [x] æ·»åŠ é…ç½®æ–‡ä»¶ç®¡ç†
- [x] ä¼˜åŒ–HanLPåŠ è½½ç­–ç•¥
## 2024-02-05 æ•°æ®é›†é€‚é…é‡æ„

### 1. JSONæ•°æ®é›†æ”¯æŒ âœ…
**æ•°æ®æ ¼å¼**: 
- train.json, dev.json, test.json
- æ¯æ¡æ•°æ®ï¼š{"text": "è¯„è®º", "topic": "ä¸»é¢˜", "label": "0/1"}
- è®½åˆºæ£€æµ‹ä»»åŠ¡ï¼š0-éè®½åˆºï¼Œ1-è®½åˆº

### 2. æ–‡æœ¬å¯¹å¤„ç†ä¼˜åŒ– âœ…
**BERTå¤„ç†**:
- ä½¿ç”¨ `tokenizer(text, topic)` å¤„ç†æ–‡æœ¬å¯¹
- è‡ªåŠ¨æ·»åŠ  [CLS] text [SEP] topic [SEP] ç»“æ„
- æ­£ç¡®çš„ token_type_ids åŒºåˆ†ä¸¤ä¸ªå¥å­

**HanLPå¤„ç†**:
- åˆå¹¶æ–‡æœ¬å’Œä¸»é¢˜è¿›è¡Œè¯­è¨€å­¦åˆ†æ
- è®°å½•ä¸»é¢˜å’Œè¯„è®ºçš„é•¿åº¦è¾¹ç•Œ
- ç”¨äºæ„å»ºä¸»é¢˜-è¯„è®ºäº¤äº’è¶…è¾¹

### 3. è¶…å›¾ç»“æ„å¢å¼º âœ…
**æ–°å¢è¶…è¾¹ç±»å‹**:
- ä¸»é¢˜-è¯„è®ºäº¤äº’è¶…è¾¹ï¼ˆè®½åˆºæ£€æµ‹ç‰¹æœ‰ï¼‰
- ä¸»é¢˜å†…éƒ¨è¶…è¾¹
- è¯„è®ºå†…éƒ¨è¶…è¾¹
- ä¿ç•™åŸæœ‰çš„ä¾å­˜ã€è¯æ€§ã€å®ä½“ã€çª—å£è¶…è¾¹

### 4. æ•°æ®åŠ è½½å™¨é‡æ„ âœ…
**SarcasmDatasetç±»**:
- ä¸“é—¨å¤„ç†è®½åˆºæ£€æµ‹æ•°æ®æ ¼å¼
- è‡ªå®šä¹‰ collate_fn æ‰¹å¤„ç†è¶…å›¾çŸ©é˜µ
- ç¡¬ä»¶æ— å…³çš„å¼ é‡ç®¡ç†

**å®Œæ•´æ•°æ®æµ**:
- JSONåŠ è½½ â†’ BERT tokenization â†’ HanLPåˆ†æ â†’ è¶…å›¾æ„å»º â†’ æ‰¹å¤„ç†

### 5. è®­ç»ƒæµç¨‹å®Œå–„ âœ…
- ä¿®å¤è®­ç»ƒå’ŒéªŒè¯å‡½æ•°çš„æ•°æ®æµ
- æ­£ç¡®çš„å‰å‘ä¼ æ’­è°ƒç”¨
- å®Œæ•´çš„æŒ‡æ ‡è®¡ç®—å’Œæ—¥å¿—è®°å½•
- æ”¯æŒæµ‹è¯•é›†è¯„ä¼°

### 6. é…ç½®ä¼˜åŒ– âœ…
- æ›´æ–° config.yaml é€‚é…JSONæ•°æ®é›†
- å‡å°æ‰¹æ¬¡å¤§å°é€‚åº”è¶…å›¾è®¡ç®—å¼€é”€
- çµæ´»çš„æ•°æ®è·¯å¾„é…ç½®

### 7. ä½¿ç”¨æ–¹æ³•
```bash
# ä½¿ç”¨é»˜è®¤æ•°æ®é›†ç›®å½•
python main.py --dataset_dir dataset

# æˆ–æŒ‡å®šå…·ä½“æ–‡ä»¶
python main.py --train_data dataset/train.json --val_data dataset/dev.json
```
## 2024-02-05 å…³é”®æŠ€æœ¯é—®é¢˜ä¿®æ­£

### 1. æ‰¹å¤„ç†é€»è¾‘æ¼æ´ä¿®æ­£ âŒâ¡ï¸âœ…
**é—®é¢˜**: æ ·æœ¬é—´æ•°æ®æ³„éœ² - æ‰€æœ‰æ ·æœ¬å…±ç”¨ä¸€ä¸ªè¶…å›¾çŸ©é˜µ
**ä¿®æ­£**: 
- `_create_single_hypergraph_matrix()`: æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹çš„è¶…å›¾çŸ©é˜µ
- `estimate_max_edges()`: åŠ¨æ€ä¼°ç®—æœ€å¤§è¶…è¾¹æ•°ç”¨äºpadding
- 3Då¼ é‡: [batch_size, max_nodes, max_edges]
- å®Œå…¨é¿å…æ ·æœ¬é—´ä¿¡æ¯æ³„éœ²

### 2. è¶…å›¾å·ç§¯ç»´åº¦å…¼å®¹æ€§ä¿®æ­£ âŒâ¡ï¸âœ…
**é—®é¢˜**: 2DçŸ©é˜µä¹˜æ³•æ— æ³•å¤„ç†æ‰¹å¤„ç†çš„3Då¼ é‡
**ä¿®æ­£**:
- ä½¿ç”¨ `torch.bmm()` æ‰¹çŸ©é˜µä¹˜æ³•
- å¹¿æ’­æœºåˆ¶å¤„ç†åº¦çŸ©é˜µå½’ä¸€åŒ–
- æ”¯æŒ [batch_size, N, M] è¾“å…¥æ ¼å¼
- å‘é‡åŒ–è®¡ç®—é¿å…å¾ªç¯

### 3. æ€§èƒ½ç“¶é¢ˆä¼˜åŒ– ğŸš€
**é—®é¢˜**: HanLPåœ¨è®­ç»ƒå¾ªç¯ä¸­é‡å¤è®¡ç®—ï¼ŒGPUåˆ©ç”¨ç‡æä½
**è§£å†³æ–¹æ¡ˆ**: ç¦»çº¿é¢„å¤„ç†
- `preprocess_offline.py`: ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰HanLPåˆ†æ
- é¢„å¤„ç†ç»“æœä¿å­˜ä¸º .pkl æ–‡ä»¶
- `PreprocessedDataset`: ç›´æ¥åŠ è½½é¢„å¤„ç†æ•°æ®
- **é¢„æœŸæ€§èƒ½æå‡**: 10å€ä»¥ä¸Šè®­ç»ƒé€Ÿåº¦

### 4. åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥ âœ…
**ä¼˜åŒ–**: ä¸åŒæ¨¡å—ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
- BERTå‚æ•°: 2e-5 (å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹)
- HGNN+Attention: 1e-3 (ä»é›¶è®­ç»ƒæ–°å±‚)
- 50å€å­¦ä¹ ç‡å·®å¼‚ï¼ŒåŠ é€Ÿæ”¶æ•›

### 5. ä½¿ç”¨æ–¹æ³•æ›´æ–°

**ç¬¬ä¸€æ­¥ - ç¦»çº¿é¢„å¤„ç†ï¼ˆæ¨èï¼‰**:
```bash
python preprocess_offline.py --dataset_dir dataset --output_dir preprocessed_data
```

**ç¬¬äºŒæ­¥ - å¿«é€Ÿè®­ç»ƒ**:
```bash
python main.py --use_preprocessed --preprocessed_dir preprocessed_data --batch_size 16
```

**æˆ–å®æ—¶å¤„ç†ï¼ˆè¾ƒæ…¢ï¼‰**:
```bash
python main.py --dataset_dir dataset --batch_size 8
```

### 6. æŠ€æœ¯å€ºåŠ¡æ¸…ç†
- [x] ä¿®å¤æ ·æœ¬é—´æ•°æ®æ³„éœ²
- [x] æ”¯æŒæ‰¹å¤„ç†3Dè¶…å›¾çŸ©é˜µ
- [x] ç¦»çº¿é¢„å¤„ç†æ€§èƒ½ä¼˜åŒ–
- [x] åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥
- [x] å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—

### 7. å…³é”®æ”¹è¿›ç‚¹
- **å­¦æœ¯æ­£ç¡®æ€§**: é¿å…æ•°æ®æ³„éœ²
- **è®¡ç®—æ•ˆç‡**: æ‰¹å¤„ç†ä¼˜åŒ–
- **è®­ç»ƒé€Ÿåº¦**: ç¦»çº¿é¢„å¤„ç†
- **æ”¶æ•›é€Ÿåº¦**: åˆ†å±‚å­¦ä¹ ç‡
- **ä»£ç è´¨é‡**: é”™è¯¯å¤„ç†å’Œæ–‡æ¡£
## 2024-02-05 ç¦»çº¿é¢„å¤„ç†è„šæœ¬ä¿®æ­£

### ğŸ› é—®é¢˜è¯Šæ–­
**é”™è¯¯**: `AttributeError: 'BertTokenizer' object has no attribute 'encode_plus'`
**æ ¹æœ¬åŸå› **: 
1. HanLP å’Œ transformers åº“çš„å‘½åç©ºé—´å†²çª
2. å¯èƒ½çš„å¾ªç¯å¯¼å…¥é—®é¢˜
3. é”™è¯¯å¤„ç†ä¸è¶³å¯¼è‡´æ‰€æœ‰æ ·æœ¬å¤±è´¥

### ğŸ”§ ä¿®æ­£æ–¹æ¡ˆ

#### 1. å‘½åç©ºé—´å†²çªè§£å†³ âœ…
- åˆ›å»º `SafeDataPreprocessor` ç±»é¿å…å¾ªç¯å¯¼å…¥
- æ˜ç¡®å¯¼å…¥ `from transformers import BertTokenizer`
- ç‹¬ç«‹çš„é¢„å¤„ç†é€»è¾‘ï¼Œä¸ä¾èµ– `data_preprocess.py`

#### 2. ç°ä»£åŒ– tokenizer è°ƒç”¨ âœ…
```python
# ä¿®æ­£å‰ï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰
encoded = tokenizer.encode_plus(...)

# ä¿®æ­£åï¼ˆç°ä»£åŒ–å†™æ³•ï¼‰
encoded = tokenizer(text, topic, ...)
```

#### 3. å¢å¼ºé”™è¯¯å¤„ç† âœ…
- æ¯ä¸ªæ­¥éª¤éƒ½æœ‰ try-catch åŒ…è£…
- è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè°ƒè¯•è¾“å‡º
- å¤±è´¥æ ·æœ¬ç»Ÿè®¡å’ŒæˆåŠŸç‡æŠ¥å‘Š
- ç©ºç»“æœæ£€æµ‹å’Œè·³è¿‡æœºåˆ¶

#### 4. é¿å…å¾ªç¯å¯¼å…¥ âœ…
- `create_fast_data_loaders` ä¸­å†…è”è¶…å›¾æ„å»ºå‡½æ•°
- ä¸å†ä¾èµ– `data_preprocess.DataPreprocessor`
- ç‹¬ç«‹çš„åŠŸèƒ½å®ç°

### ğŸ§¹ æ¸…ç†å’Œé‡è¯•æ­¥éª¤

#### ç¬¬ä¸€æ­¥ï¼šæ¸…ç†é”™è¯¯æ•°æ®
```bash
python clean_preprocessed.py
```

#### ç¬¬äºŒæ­¥ï¼šé‡æ–°é¢„å¤„ç†
```bash
python preprocess_offline.py --dataset_dir dataset --output_dir preprocessed_data
```

#### ç¬¬ä¸‰æ­¥ï¼šéªŒè¯ç»“æœ
- æ£€æŸ¥æˆåŠŸç‡ > 90%
- ç¡®è®¤ .pkl æ–‡ä»¶ä¸ä¸ºç©º
- æŸ¥çœ‹è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡

### ğŸ¯ é¢„æœŸæ”¹è¿›
- **é”™è¯¯ç‡**: ä» 100% é™è‡³ < 10%
- **è°ƒè¯•ä¿¡æ¯**: è¯¦ç»†çš„é”™è¯¯è¿½è¸ª
- **ç¨³å®šæ€§**: å•ä¸ªæ ·æœ¬å¤±è´¥ä¸å½±å“æ•´ä½“
- **å…¼å®¹æ€§**: è§£å†³åº“å†²çªé—®é¢˜

### ğŸ“‹ æ•…éšœæ’é™¤æ¸…å•
- [x] ä¿®æ­£ tokenizer è°ƒç”¨æ–¹å¼
- [x] è§£å†³å‘½åç©ºé—´å†²çª
- [x] å¢å¼ºé”™è¯¯å¤„ç†
- [x] é¿å…å¾ªç¯å¯¼å…¥
- [x] æ·»åŠ æ¸…ç†è„šæœ¬
- [x] è¯¦ç»†çš„è°ƒè¯•è¾“å‡º
## 2024-02-05 å½»åº•é‡å†™ç¦»çº¿é¢„å¤„ç†è„šæœ¬

### ğŸ”¥ é—®é¢˜æ ¹æºåˆ†æ
**æ ¸å¿ƒé—®é¢˜**: ç¯å¢ƒ"æ‰“æ¶" - ä»£ç ä¾èµ–å†²çª
- **æ—§ä»£ç **: è¯•å›¾è°ƒç”¨ `data_preprocess.py` ä¸­çš„ `DataPreprocessor` ç±»
- **å†²çªæº**: HanLP å’Œ transformers æ··åˆç¯å¢ƒå¯¼è‡´ tokenizer å‘½åç©ºé—´å†²çª
- **é”™è¯¯è¡¨ç°**: `BertTokenizer has no attribute encode_plus`

### ğŸš€ å½»åº•é‡å†™æ–¹æ¡ˆ

#### 1. å®Œå…¨è§£è€¦è®¾è®¡ âœ…
```python
# æ—§ä»£ç ï¼ˆæœ‰é—®é¢˜ï¼‰
from data_preprocess import DataPreprocessor  # å¯¼è‡´ç¯å¢ƒå†²çª

# æ–°ä»£ç ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
class IndependentPreprocessor:  # ä¸ä¾èµ–ä»»ä½•å…¶ä»–æ¨¡å—
```

#### 2. çº¯å‡€çš„åº“å¯¼å…¥ âœ…
```python
# ç‹¬ç«‹å¯¼å…¥ï¼Œé¿å…å‘½åç©ºé—´å†²çª
from transformers import BertTokenizer  # çº¯å‡€çš„ BERT tokenizer
import hanlp                           # ç‹¬ç«‹çš„ HanLP
```

#### 3. ç°ä»£åŒ– API è°ƒç”¨ âœ…
```python
# ç›´æ¥è°ƒç”¨æ–¹å¼ï¼ˆæ¨èï¼‰
encoded = self.bert_tokenizer(
    text, topic,
    add_special_tokens=True,
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)
```

#### 4. å¢å¼ºçš„é”™è¯¯å¤„ç† âœ…
- æ¯ä¸ªæ­¥éª¤ç‹¬ç«‹çš„ try-catch
- è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè°ƒè¯•è¾“å‡º
- å¤±è´¥æ ·æœ¬ç»Ÿè®¡å’ŒæˆåŠŸç‡æŠ¥å‘Š
- ç©ºç»“æœæ£€æµ‹å’ŒéªŒè¯æœºåˆ¶

### ğŸ¯ é‡å†™ç‰¹ç‚¹

#### å®Œå…¨ç‹¬ç«‹æ€§
- **é›¶ä¾èµ–**: ä¸ä¾èµ– `data_preprocess.py`
- **çº¯å‡€ç¯å¢ƒ**: ç‹¬ç«‹åŠ è½½ BERT å’Œ HanLP
- **é¿å…å†²çª**: å½»åº•è§£å†³å‘½åç©ºé—´é—®é¢˜

#### ç°ä»£åŒ–è®¾è®¡
- **ç±»å‹æç¤º**: å®Œæ•´çš„ç±»å‹æ³¨è§£
- **é”™è¯¯å¤„ç†**: å¥å£®çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
- **è¿›åº¦æ˜¾ç¤º**: è¯¦ç»†çš„å¤„ç†è¿›åº¦å’Œç»Ÿè®¡
- **æ¨¡å—åŒ–**: æ¸…æ™°çš„å‡½æ•°èŒè´£åˆ†ç¦»

#### ç”¨æˆ·å‹å¥½
- **è¯¦ç»†æ—¥å¿—**: ä¸°å¯Œçš„çŠ¶æ€ä¿¡æ¯
- **é”™è¯¯è¯Šæ–­**: å…·ä½“çš„é”™è¯¯ä½ç½®å’ŒåŸå› 
- **æˆåŠŸç‡ç»Ÿè®¡**: å®æ—¶çš„å¤„ç†æˆåŠŸç‡
- **æ–‡ä»¶éªŒè¯**: è‡ªåŠ¨æ£€æµ‹å’ŒéªŒè¯è¾“å‡º

### ğŸš€ ä½¿ç”¨æ–¹æ³•

#### ç¬¬ä¸€æ­¥ï¼šæ¸…ç†æ—§æ•°æ®
```bash
python clean_preprocessed.py
```

#### ç¬¬äºŒæ­¥ï¼šè¿è¡Œæ–°çš„é¢„å¤„ç†è„šæœ¬
```bash
python preprocess_offline.py --dataset_dir dataset --output_dir preprocessed_data
```

#### ç¬¬ä¸‰æ­¥ï¼šéªŒè¯ç»“æœ
- åº”è¯¥çœ‹åˆ°æˆåŠŸç‡ > 95%
- ç»¿è‰²çš„æˆåŠŸä¿¡æ¯ï¼Œè€Œä¸æ˜¯çº¢è‰²é”™è¯¯
- ç”Ÿæˆçš„ .pkl æ–‡ä»¶åŒ…å«æœ‰æ•ˆæ•°æ®

### ğŸ“Š é¢„æœŸæ”¹è¿›
- **é”™è¯¯ç‡**: ä» 100% é™è‡³ < 5%
- **ç¨³å®šæ€§**: å•ä¸ªæ ·æœ¬å¤±è´¥ä¸å½±å“æ•´ä½“
- **å¯ç»´æŠ¤æ€§**: ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºè°ƒè¯•
- **å…¼å®¹æ€§**: å½»åº•è§£å†³åº“å†²çªé—®é¢˜

### ğŸ”§ æŠ€æœ¯è¦ç‚¹
- **IndependentPreprocessor**: å®Œå…¨ç‹¬ç«‹çš„é¢„å¤„ç†å™¨ç±»
- **bert_encode()**: çº¯å‡€çš„ BERT ç¼–ç æ–¹æ³•
- **hanlp_analyze()**: ç‹¬ç«‹çš„ HanLP åˆ†ææ–¹æ³•
- **process_single_sample()**: å•æ ·æœ¬å¤„ç†æµç¨‹
- **è¯¦ç»†ç»Ÿè®¡**: æˆåŠŸç‡ã€é”™è¯¯è®¡æ•°ã€å¤„ç†æ—¶é—´

ç°åœ¨åº”è¯¥èƒ½çœ‹åˆ°ç»¿è‰²çš„æˆåŠŸä¿¡æ¯äº†ï¼ğŸ‰
## 2024-02-05 æ ¸å¿ƒæ‰‹æœ¯ - ä¿®æ­£å¯¼å…¥å’Œé€»è¾‘é—®é¢˜

### ğŸ”§ ç¬¬ä¸€æ­¥ï¼šdata_preprocess.py æ ¸å¿ƒæ‰‹æœ¯ âœ…

#### é—®é¢˜è¯Šæ–­
- **SarcasmDataset ç±»è¢«é”åœ¨å‡½æ•°å†…éƒ¨**ï¼šç¼©è¿›é”™è¯¯å¯¼è‡´ç±»æ— æ³•è¢«å¤–éƒ¨å¯¼å…¥
- **main.py æ‰¾ä¸åˆ° SarcasmDataset**ï¼š`from data_preprocess import SarcasmDataset` å¤±è´¥

#### æ‰‹æœ¯æ“ä½œ
```python
# ä¿®æ­£å‰ï¼ˆæœ‰é—®é¢˜ï¼‰
def create_data_loaders(...):
    class SarcasmDataset(Dataset):  # è¢«é”åœ¨å‡½æ•°å†…éƒ¨
        ...

# ä¿®æ­£åï¼ˆæ­£ç¡®ï¼‰
class SarcasmDataset(torch.utils.data.Dataset):  # å…¨å±€ä½œç”¨åŸŸ
    """è®½åˆºæ£€æµ‹æ•°æ®é›†ç±» - ç§»åˆ°å…¨å±€ä½œç”¨åŸŸ"""
    ...

def create_hypergraph_collate_fn(preprocessor, max_length):
    """åˆ›å»ºè¶…å›¾æ‰¹å¤„ç†å‡½æ•°çš„å·¥å‚å‡½æ•°"""
    ...
```

#### ä¿®æ­£è¦ç‚¹
- **å‘å·¦åç¼©è¿›**ï¼šå°† `SarcasmDataset` ç±»ç§»åˆ°é¡¶æ ¼ï¼ˆå…¨å±€ä½œç”¨åŸŸï¼‰
- **å·¥å‚å‡½æ•°æ¨¡å¼**ï¼š`create_hypergraph_collate_fn` è¿”å›é…ç½®å¥½çš„ `collate_fn`
- **æ¸…æ™°çš„èŒè´£åˆ†ç¦»**ï¼šç±»å®šä¹‰å’Œå‡½æ•°åˆ›å»ºåˆ†å¼€

### ğŸ”§ ç¬¬äºŒæ­¥ï¼špreprocess_offline.py æ¸…ç†é€»è¾‘ âœ…

#### é—®é¢˜è¯Šæ–­
- **é€»è¾‘æ­»èƒ¡åŒ**ï¼š`fast_collate_fn` ä¸­è°ƒç”¨ä¸å­˜åœ¨çš„ `preprocessor` å¯¹è±¡
- **å¤æ‚è®¡ç®—é”™ä½**ï¼šç¦»çº¿é¢„å¤„ç†åº”è¯¥"å¿«é€Ÿè¯»å–"ï¼Œä¸åº”è¯¥å†åšå¤æ‚è®¡ç®—

#### æ¸…ç†æ“ä½œ
```python
# ä¿®æ­£å‰ï¼ˆæœ‰é—®é¢˜ï¼‰
def fast_collate_fn(batch):
    max_edges = preprocessor.estimate_max_edges(...)  # preprocessor ä¸å­˜åœ¨ï¼
    single_matrix = preprocessor._create_single_hypergraph_matrix(...)  # å¤æ‚è®¡ç®—

# ä¿®æ­£åï¼ˆç®€åŒ–ï¼‰
def simple_collate_fn(batch):
    # åªåšåŸºæœ¬çš„å¼ é‡å †å 
    input_ids = torch.stack([item['input_ids'] for item in batch])
    hanlp_results = [item['hanlp_result'] for item in batch]  # ç®€å•åˆ—è¡¨
    return {...}
```

#### æ¸…ç†è¦ç‚¹
- **åˆ é™¤å¤æ‚è®¡ç®—**ï¼šç§»é™¤ `estimate_max_edges` å’Œè¶…å›¾çŸ©é˜µæ„å»º
- **ç®€åŒ–æ•°æ®æµ**ï¼šåªåšåŸºæœ¬çš„å¼ é‡å †å å’Œè®¾å¤‡ç§»åŠ¨
- **èŒè´£æ˜ç¡®**ï¼šç¦»çº¿é¢„å¤„ç†è´Ÿè´£"é¢„å¤„ç†"ï¼Œæ•°æ®åŠ è½½å™¨è´Ÿè´£"åŠ è½½"

### ğŸ¯ ä¿®æ­£æ•ˆæœ

#### å¯¼å…¥é—®é¢˜è§£å†³
```python
# ç°åœ¨å¯ä»¥æ­£å¸¸å¯¼å…¥
from data_preprocess import SarcasmDataset  # âœ… æˆåŠŸ
from preprocess_offline import load_preprocessed_data  # âœ… æˆåŠŸ
```

#### é€»è¾‘æ¸…æ™°åŒ–
- **data_preprocess.py**ï¼šè´Ÿè´£å®æ—¶å¤„ç†å’Œè¶…å›¾è®¡ç®—
- **preprocess_offline.py**ï¼šè´Ÿè´£ç¦»çº¿é¢„å¤„ç†å’Œå¿«é€ŸåŠ è½½
- **main.py**ï¼šæ ¹æ®å‚æ•°é€‰æ‹©å¤„ç†æ¨¡å¼

#### æ€§èƒ½ä¼˜åŒ–
- **ç¦»çº¿æ¨¡å¼**ï¼šé¢„å¤„ç†ä¸€æ¬¡ï¼Œå¿«é€ŸåŠ è½½
- **å®æ—¶æ¨¡å¼**ï¼šæ¯æ¬¡éƒ½è®¡ç®—ï¼Œä½†é€»è¾‘æ­£ç¡®
- **ç¡¬ä»¶æ— å…³**ï¼šä¸¤ç§æ¨¡å¼éƒ½æ”¯æŒ CPU/GPU

### ğŸ“‹ ä¿®æ­£æ¸…å•
- [x] SarcasmDataset ç§»åˆ°å…¨å±€ä½œç”¨åŸŸ
- [x] åˆ›å»º create_hypergraph_collate_fn å·¥å‚å‡½æ•°
- [x] ç®€åŒ– preprocess_offline.py çš„ collate_fn
- [x] åˆ é™¤ä¸å­˜åœ¨çš„ preprocessor å¼•ç”¨
- [x] ä¿æŒæ•°æ®æµçš„ä¸€è‡´æ€§
- [x] ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®

ç°åœ¨ main.py åº”è¯¥èƒ½æ­£å¸¸å¯¼å…¥å’Œä½¿ç”¨è¿™äº›ç±»äº†ï¼ğŸ‰
## 2024-02-05 è‡´å‘½Bugä¿®æ­£å’Œæ€§èƒ½ä¼˜åŒ–

### ğŸ› è‡´å‘½Bug 1ï¼šæ–¹æ³•åä¸åŒ¹é… âœ…

#### é—®é¢˜è¯Šæ–­
- **è°ƒç”¨**: `self._create_hypergraph_incidence_matrix(...)`
- **å®šä¹‰**: `_create_single_hypergraph_matrix(...)`
- **åæœ**: `AttributeError` è¿è¡Œç›´æ¥æŠ¥é”™

#### ä¿®æ­£æ–¹æ¡ˆ
- **åˆ é™¤å¤šä½™æ–¹æ³•**: ç§»é™¤ `build_hypergraph_structure` æ–¹æ³•
- **åŸå› **: `collate_fn` ä¸­å·²æœ‰æ­£ç¡®çš„æ‰¹å¤„ç†é€»è¾‘
- **é¿å…æ··æ·†**: ä¿æŒå•ä¸€èŒè´£ï¼Œé¿å…é‡å¤é€»è¾‘

### âš¡ æ€§èƒ½éšæ‚£ï¼šHanLPé€Ÿåº¦é™·é˜± âœ…

#### é—®é¢˜è¯Šæ–­
```python
# æ—§ä»£ç ï¼ˆè‡´å‘½æ€§èƒ½é—®é¢˜ï¼‰
def __getitem__(self, idx):
    hanlp_result = self.preprocessor.process_text_with_hanlp(...)  # æ¯æ¬¡éƒ½è·‘HanLPï¼
```

**åæœ**:
- H100 GPU ç­‰å¾… CPU è·‘ HanLP
- è®­ç»ƒé€Ÿåº¦è¢«æ‹–æ…¢ **100å€**
- GPU åˆ©ç”¨ç‡æä½

#### ä¿®æ­£æ–¹æ¡ˆï¼šå†…ç½®ç¼“å­˜æœºåˆ¶
```python
class SarcasmDataset:
    def __init__(self, data, preprocessor, max_length, cache_file=None):
        if cache_file and os.path.exists(cache_file):
            # ç›´æ¥åŠ è½½ç¼“å­˜
            self.cached_data = pickle.load(...)
        else:
            # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šé¢„å¤„ç†å¹¶ç¼“å­˜
            for text, topic, label in tqdm(self.data):
                bert_tokens = ...
                hanlp_result = ...  # åªåšä¸€æ¬¡ï¼
                self.cached_data.append(...)
            # ä¿å­˜ç¼“å­˜
            pickle.dump(self.cached_data, ...)
    
    def __getitem__(self, idx):
        # ç›´æ¥è¿”å›å†…å­˜æ•°æ®ï¼Œé›¶è®¡ç®—ï¼
        return self.cached_data[idx]
```

#### æ€§èƒ½æå‡
- **ç¬¬ä¸€æ¬¡è¿è¡Œ**: æ…¢ï¼ˆéœ€è¦HanLPå¤„ç†ï¼‰
- **ä¹‹åè¿è¡Œ**: é£å¿«ï¼ˆç›´æ¥åŠ è½½ç¼“å­˜ï¼‰
- **GPUåˆ©ç”¨ç‡**: ä» <10% æå‡åˆ° >90%
- **è®­ç»ƒé€Ÿåº¦**: æå‡ **100å€**

### ğŸ—‘ï¸ æ–‡ä»¶æ¸…ç† âœ…

#### åˆ é™¤åºŸå¼ƒæ–‡ä»¶
- âŒ `preprocess_offline.py` - åŠŸèƒ½å·²é›†æˆåˆ° `SarcasmDataset`
- âŒ `clean_preprocessed.py` - ä¸å†éœ€è¦

#### ç®€åŒ–å·¥ä½œæµ
```bash
# æ—§æµç¨‹ï¼ˆå¤æ‚ï¼‰
python preprocess_offline.py  # ç¬¬ä¸€æ­¥
python main.py --use_preprocessed  # ç¬¬äºŒæ­¥

# æ–°æµç¨‹ï¼ˆç®€å•ï¼‰
python main.py  # ä¸€æ­¥æå®šï¼ç¬¬ä¸€æ¬¡æ…¢ï¼Œä¹‹åå¿«
```

### ğŸ¯ ä¿®æ­£æ•ˆæœ

#### ä»£ç è´¨é‡
- **æ–¹æ³•åä¸€è‡´**: é¿å… AttributeError
- **é€»è¾‘æ¸…æ™°**: åˆ é™¤é‡å¤å’Œå¤šä½™ä»£ç 
- **èŒè´£å•ä¸€**: æ¯ä¸ªæ–¹æ³•åªåšä¸€ä»¶äº‹

#### æ€§èƒ½ä¼˜åŒ–
- **ç¼“å­˜æœºåˆ¶**: è‡ªåŠ¨ç¼“å­˜HanLPç»“æœ
- **é›¶é‡å¤è®¡ç®—**: ç¬¬äºŒæ¬¡è¿è¡Œç›´æ¥åŠ è½½
- **GPUå‹å¥½**: ä¸å†ç­‰å¾…CPUå¤„ç†

#### ç”¨æˆ·ä½“éªŒ
- **ç®€åŒ–æµç¨‹**: ä¸€ä¸ªå‘½ä»¤å¯åŠ¨è®­ç»ƒ
- **æ™ºèƒ½ç¼“å­˜**: è‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨ç¼“å­˜
- **æ¸…æ™°æç¤º**: å‘ŠçŸ¥ç”¨æˆ·ç¼“å­˜çŠ¶æ€

### ğŸ“‹ ä¿®æ­£æ¸…å•
- [x] åˆ é™¤ `build_hypergraph_structure` æ–¹æ³•
- [x] ä¿®æ­£æ–¹æ³•åä¸åŒ¹é…é—®é¢˜
- [x] æ·»åŠ ç¼“å­˜æœºåˆ¶åˆ° `SarcasmDataset`
- [x] æ›´æ–° `create_data_loaders` æ”¯æŒç¼“å­˜
- [x] åˆ é™¤ `preprocess_offline.py`
- [x] åˆ é™¤ `clean_preprocessed.py`
- [x] æ›´æ–° `main.py` ç§»é™¤ç¦»çº¿é¢„å¤„ç†é€»è¾‘
- [x] ç®€åŒ–å‘½ä»¤è¡Œå‚æ•°

### ğŸš€ ä½¿ç”¨æ–¹æ³•

```bash
# ç›´æ¥è¿è¡Œï¼ˆç¬¬ä¸€æ¬¡ä¼šæ…¢ï¼Œä¹‹åé£å¿«ï¼‰
python main.py --dataset_dir dataset --cache_dir cache

# æ¸…é™¤ç¼“å­˜é‡æ–°å¤„ç†
rm -rf cache/
python main.py
```

ç°åœ¨ä»£ç å·²ç»å‡†å¤‡å¥½åœ¨H100ä¸Šé£é©°äº†ï¼ğŸ‰
## 2024-02-05 ä¿®å¤ main.py é€»è¾‘æ¼æ´

### ğŸ› é—®é¢˜è¯Šæ–­ï¼šNameError

#### é”™è¯¯ä»£ç ï¼ˆç¬¬254è¡Œï¼‰
```python
test_loader = DataLoader(test_dataset, ..., collate_fn=collate_fn)
```

**é”™è¯¯åŸå› **:
- `collate_fn` åœ¨ `main.py` ä¸­æ ¹æœ¬æ²¡æœ‰å®šä¹‰
- å®ƒè¢«å°è£…åœ¨ `data_preprocess.py` çš„ `create_data_loaders` å†…éƒ¨
- è¿è¡Œæ—¶æŠ¥é”™ï¼š`NameError: name 'collate_fn' is not defined`

### âœ… ä¿®å¤æ–¹æ¡ˆ

#### 1. æ›´æ–°å¯¼å…¥è¯­å¥
```python
# ä¿®å¤å‰
from data_preprocess import DataPreprocessor, load_all_datasets, create_data_loaders, SarcasmDataset

# ä¿®å¤å
from data_preprocess import (
    DataPreprocessor, 
    load_all_datasets, 
    create_data_loaders, 
    SarcasmDataset,
    create_hypergraph_collate_fn  # æ–°å¢ï¼šç”¨äºæµ‹è¯•é›†
)
```

#### 2. ä¿®å¤æµ‹è¯•é›†è¯„ä¼°é€»è¾‘
```python
if test_data and len(test_data) > 0:
    # 1. åˆ›å»º collate_fnï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„é…ç½®ï¼‰
    test_collate_fn = create_hypergraph_collate_fn(preprocessor, max_length=512)
    
    # 2. åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆå¸¦ç¼“å­˜ï¼‰
    test_cache_file = os.path.join(args.cache_dir, 'test_cache.pkl')
    test_dataset = SarcasmDataset(test_data, preprocessor, 512, cache_file=test_cache_file)
    
    # 3. åˆ›å»º DataLoaderï¼ˆnum_workers=0 é¿å…å¤šè¿›ç¨‹ä¸GPUå†²çªï¼‰
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        collate_fn=test_collate_fn,
        num_workers=0  # é‡è¦ï¼
    )
```

### ğŸ¯ ä¿®å¤è¦ç‚¹

#### å¤ç”¨ç°æœ‰é€»è¾‘
- ä½¿ç”¨ `create_hypergraph_collate_fn` åˆ›å»º collate å‡½æ•°
- ä¿æŒä¸è®­ç»ƒé›†ç›¸åŒçš„æ•°æ®å¤„ç†æµç¨‹
- ç¡®ä¿æµ‹è¯•é›†ä¹Ÿä½¿ç”¨ç¼“å­˜æœºåˆ¶

#### é¿å…å¤šè¿›ç¨‹é—®é¢˜
- è®¾ç½® `num_workers=0`
- åŸå› ï¼š`collate_fn` ä¸­ä½¿ç”¨äº† `.to(device)`
- å¤šè¿›ç¨‹ä¼šå¯¼è‡´ CUDA ä¸Šä¸‹æ–‡å†²çª

#### ç¼“å­˜ä¸€è‡´æ€§
- æµ‹è¯•é›†ä¹Ÿä½¿ç”¨ç¼“å­˜ï¼š`test_cache.pkl`
- ç¬¬ä¸€æ¬¡è¿è¡Œæ…¢ï¼Œä¹‹åå¿«
- ä¸è®­ç»ƒé›†/éªŒè¯é›†ä¿æŒä¸€è‡´

### ğŸ“‹ ä¿®å¤æ¸…å•
- [x] å¯¼å…¥ `create_hypergraph_collate_fn`
- [x] åˆ›å»ºæµ‹è¯•é›† collate_fn
- [x] æ·»åŠ æµ‹è¯•é›†ç¼“å­˜æ”¯æŒ
- [x] è®¾ç½® `num_workers=0`
- [x] æ·»åŠ è¯¦ç»†æ—¥å¿—è¾“å‡º
- [x] ä¿æŒä»£ç ä¸€è‡´æ€§

### ğŸš€ æ•ˆæœ

#### ä¿®å¤å‰
```
NameError: name 'collate_fn' is not defined
```

#### ä¿®å¤å
```
âœ… æµ‹è¯•æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ: XXX æ ·æœ¬
æµ‹è¯•å‡†ç¡®ç‡: X.XXXX
æµ‹è¯•é›†ç»“æœ:
  accuracy: X.XXXX
  precision: X.XXXX
  ...
```

ç°åœ¨æµ‹è¯•é›†è¯„ä¼°å¯ä»¥æ­£å¸¸è¿è¡Œäº†ï¼ğŸ‰

## 2026-02-06 æ˜¾å­˜ä¼˜åŒ– - é€‚é… 12GB GPU

### ğŸ¯ é—®é¢˜èƒŒæ™¯
- **ç¡¬ä»¶é™åˆ¶**: 12GB æ˜¾å­˜
- **åŸé…ç½®**: Batch Size = 32, Max Length = 512
- **é£é™©**: åœ¨ 12GB æ˜¾å­˜ä¸‹å‡ ä¹å¿…å®š OOMï¼ˆæ˜¾å­˜æº¢å‡ºï¼‰

### âš¡ ä¼˜åŒ–æ–¹æ¡ˆ

#### 1. Batch Size è°ƒæ•´ âœ…
```python
# ä¿®æ”¹å‰
parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')

# ä¿®æ”¹å
parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
```
- **é™ä½å¹…åº¦**: 32 â†’ 16ï¼ˆå‡åŠï¼‰
- **æ˜¾å­˜èŠ‚çœ**: çº¦ 50%

#### 2. Max Length è°ƒæ•´ âœ…
```python
# ä¿®æ”¹å‰ï¼ˆè®­ç»ƒ/éªŒè¯é›†ï¼‰
train_loader, val_loader = create_data_loaders(
    train_data, val_data, preprocessor, args.batch_size, max_length=512, cache_dir=args.cache_dir
)

# ä¿®æ”¹å
train_loader, val_loader = create_data_loaders(
    train_data, val_data, preprocessor, args.batch_size, max_length=256, cache_dir=args.cache_dir
)
```

```python
# ä¿®æ”¹å‰ï¼ˆæµ‹è¯•é›†ï¼‰
test_collate_fn = create_hypergraph_collate_fn(preprocessor, max_length=512)
test_dataset = SarcasmDataset(test_data, preprocessor, 512, cache_file=test_cache_file)

# ä¿®æ”¹å
test_collate_fn = create_hypergraph_collate_fn(preprocessor, max_length=256)
test_dataset = SarcasmDataset(test_data, preprocessor, 256, cache_file=test_cache_file)
```
- **é™ä½å¹…åº¦**: 512 â†’ 256ï¼ˆå‡åŠï¼‰
- **æ˜¾å­˜èŠ‚çœ**: çº¦ 50%ï¼ˆåºåˆ—é•¿åº¦å¯¹æ˜¾å­˜å½±å“æ˜¯å¹³æ–¹çº§ï¼‰

### ğŸ“Š æ˜¾å­˜ä¼°ç®—

#### ä¿®æ”¹å‰ï¼ˆå±é™©é…ç½®ï¼‰
- Batch Size: 32
- Max Length: 512
- BERT Hidden: 768
- ä¼°ç®—æ˜¾å­˜: ~15-18 GBï¼ˆè¶…å‡º 12GBï¼ï¼‰

#### ä¿®æ”¹åï¼ˆå®‰å…¨é…ç½®ï¼‰
- Batch Size: 16
- Max Length: 256
- BERT Hidden: 768
- ä¼°ç®—æ˜¾å­˜: ~6-8 GBï¼ˆå®‰å…¨èŒƒå›´å†…ï¼‰

### ğŸ¯ ä¿®æ”¹ä½ç½®æ€»ç»“
1. **main.py ç¬¬ 48 è¡Œ**: `--batch_size` é»˜è®¤å€¼ 32 â†’ 16
2. **main.py ç¬¬ 217 è¡Œ**: è®­ç»ƒ/éªŒè¯é›† `max_length=512` â†’ `max_length=256`
3. **main.py ç¬¬ 398 è¡Œ**: æµ‹è¯•é›† collate_fn `max_length=512` â†’ `max_length=256`
4. **main.py ç¬¬ 402 è¡Œ**: æµ‹è¯•é›† dataset `512` â†’ `256`

### ğŸ’¡ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®
å¦‚æœ 12GB æ˜¾å­˜ä»ç„¶ç´§å¼ ï¼Œå¯ä»¥ï¼š
- é™ä½ Batch Size åˆ° 8
- å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
- å†»ç»“ BERT éƒ¨åˆ†å±‚

### âœ… éªŒè¯æ–¹æ³•
```bash
# è¿è¡Œè®­ç»ƒï¼Œè§‚å¯Ÿæ˜¾å­˜ä½¿ç”¨
python main.py

# ä½¿ç”¨ nvidia-smi ç›‘æ§æ˜¾å­˜
watch -n 1 nvidia-smi
```

é¢„æœŸæ˜¾å­˜å³°å€¼åº”è¯¥åœ¨ 8GB ä»¥å†…ï¼Œç•™æœ‰å……è¶³çš„å®‰å…¨è¾¹é™…ã€‚ğŸ‰

## 2026-02-06 æ·»åŠ  AUC æŒ‡æ ‡è®¡ç®—

### ğŸ¯ ç›®æ ‡
ä¸ºè®½åˆºæ£€æµ‹ä»»åŠ¡æ·»åŠ  AUCï¼ˆArea Under ROC Curveï¼‰æŒ‡æ ‡ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“Š ä¸ºä»€ä¹ˆéœ€è¦ AUCï¼Ÿ
- **å‡†ç¡®ç‡çš„å±€é™æ€§**: åœ¨ç±»åˆ«ä¸å¹³è¡¡æ—¶ï¼Œå‡†ç¡®ç‡å¯èƒ½è¯¯å¯¼
- **AUC çš„ä¼˜åŠ¿**: 
  - è¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿæ ·æœ¬çš„èƒ½åŠ›
  - ä¸å—åˆ†ç±»é˜ˆå€¼å½±å“
  - å¯¹ç±»åˆ«ä¸å¹³è¡¡æ›´é²æ£’
  - è®½åˆºæ£€æµ‹ä»»åŠ¡çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡

### ğŸ”§ ä¿®æ”¹å†…å®¹

#### 1. utils.py - æ·»åŠ  AUC è®¡ç®— âœ…

**å¯¼å…¥ roc_auc_score**:
```python
from sklearn.metrics import roc_auc_score  # æ–°å¢
```

**ä¿®æ”¹ MetricsCalculator.calculate_metrics()**:
```python
def calculate_metrics(y_true, y_pred, y_probs=None, labels=None):
    # ... åŸæœ‰æŒ‡æ ‡è®¡ç®— ...
    
    # æ–°å¢ AUC è®¡ç®—
    if y_probs is not None:
        try:
            # å¯¹äºäºŒåˆ†ç±»ï¼Œy_probs æ˜¯æ­£ç±»ï¼ˆLabel 1ï¼Œè®½åˆºï¼‰çš„æ¦‚ç‡
            auc = roc_auc_score(y_true, y_probs)
            metrics['auc'] = auc
        except ValueError:
            # é˜²æ­¢åªæœ‰ä¸€ä¸ªç±»åˆ«æ—¶æŠ¥é”™
            metrics['auc'] = 0.0
    
    return metrics
```

**å…³é”®ç‚¹**:
- æ–°å¢ `y_probs` å‚æ•°ï¼ˆå¯é€‰ï¼‰
- ä½¿ç”¨ try-except å¤„ç†è¾¹ç•Œæƒ…å†µ
- åªåœ¨æä¾›æ¦‚ç‡æ—¶è®¡ç®— AUC

#### 2. main.py - æ”¶é›†å’Œä¼ é€’æ¦‚ç‡å€¼ âœ…

**ä¿®æ”¹ validate_epoch() å‡½æ•°**:
```python
def validate_epoch(...) -> Tuple[float, float, List[int], List[int], List[float]]:
    # ... å‰å‘ä¼ æ’­ ...
    
    # è®¡ç®—æ¦‚ç‡ï¼ˆsoftmaxï¼‰
    probs = torch.softmax(outputs, dim=1)[:, 1]  # å–æ­£ç±»ï¼ˆLabel 1ï¼‰çš„æ¦‚ç‡
    
    # æ”¶é›†æ¦‚ç‡
    all_probs = []
    all_probs.extend(probs.cpu().numpy().tolist())
    
    return losses.avg, accuracies.avg, all_predictions, all_labels, all_probs
```

**æ›´æ–°æ‰€æœ‰è°ƒç”¨å¤„**ï¼ˆ4å¤„ï¼‰:
1. **è®­ç»ƒå¾ªç¯ä¸­çš„éªŒè¯**:
   ```python
   val_loss, val_acc, predictions, true_labels, val_probs = validate_epoch(...)
   ```

2. **ä»…è¯„ä¼°æ¨¡å¼**:
   ```python
   val_loss, val_acc, predictions, true_labels, val_probs = validate_epoch(...)
   metrics = MetricsCalculator.calculate_metrics(true_labels, predictions, val_probs)
   ```

3. **æœ€ç»ˆè¯„ä¼°**:
   ```python
   val_loss, val_acc, predictions, true_labels, val_probs = validate_epoch(...)
   metrics = MetricsCalculator.calculate_metrics(true_labels, predictions, val_probs)
   ```

4. **æµ‹è¯•é›†è¯„ä¼°**:
   ```python
   test_loss, test_acc, test_predictions, test_labels, test_probs = validate_epoch(...)
   test_metrics = MetricsCalculator.calculate_metrics(test_labels, test_predictions, test_probs)
   ```

### ğŸ“Š è¾“å‡ºç¤ºä¾‹

è®­ç»ƒå®Œæˆåï¼Œæ—¥å¿—ä¼šæ˜¾ç¤ºï¼š
```
æœ€ç»ˆéªŒè¯ç»“æœ:
accuracy: 0.8523
precision: 0.8456
recall: 0.8523
f1_score: 0.8489
auc: 0.9123  # <--- æ–°å¢çš„ AUC æŒ‡æ ‡
class_0_precision: 0.8234
class_0_recall: 0.8756
...
```

### ğŸ¯ æŠ€æœ¯è¦ç‚¹

#### Softmax æ¦‚ç‡æå–
```python
probs = torch.softmax(outputs, dim=1)[:, 1]
```
- `dim=1`: åœ¨ç±»åˆ«ç»´åº¦ä¸Šåš softmax
- `[:, 1]`: æå–ç¬¬äºŒåˆ—ï¼ˆæ­£ç±»ï¼Œè®½åˆºï¼‰çš„æ¦‚ç‡
- å¯¹äºäºŒåˆ†ç±»ï¼Œåªéœ€è¦æ­£ç±»æ¦‚ç‡å³å¯è®¡ç®— AUC

#### é”™è¯¯å¤„ç†
```python
try:
    auc = roc_auc_score(y_true, y_probs)
except ValueError:
    auc = 0.0  # åªæœ‰ä¸€ä¸ªç±»åˆ«æ—¶
```
- é˜²æ­¢éªŒè¯é›†åªåŒ…å«ä¸€ä¸ªç±»åˆ«æ—¶æŠ¥é”™
- ä¿è¯ç¨‹åºç¨³å®šæ€§

### ğŸ“‹ ä¿®æ”¹æ¸…å•
- [x] utils.py å¯¼å…¥ roc_auc_score
- [x] utils.py æ·»åŠ  y_probs å‚æ•°
- [x] utils.py å®ç° AUC è®¡ç®—é€»è¾‘
- [x] main.py ä¿®æ”¹ validate_epoch è¿”å›å€¼
- [x] main.py æ”¶é›† softmax æ¦‚ç‡
- [x] main.py æ›´æ–°è®­ç»ƒå¾ªç¯è°ƒç”¨
- [x] main.py æ›´æ–°è¯„ä¼°æ¨¡å¼è°ƒç”¨
- [x] main.py æ›´æ–°æœ€ç»ˆè¯„ä¼°è°ƒç”¨
- [x] main.py æ›´æ–°æµ‹è¯•é›†è¯„ä¼°è°ƒç”¨

### ğŸ‰ æ•ˆæœ
ç°åœ¨æ¨¡å‹è¯„ä¼°ä¼šåŒæ—¶è¾“å‡ºï¼š
- Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰
- Precisionï¼ˆç²¾ç¡®ç‡ï¼‰
- Recallï¼ˆå¬å›ç‡ï¼‰
- F1-Scoreï¼ˆF1åˆ†æ•°ï¼‰
- **AUCï¼ˆROCæ›²çº¿ä¸‹é¢ç§¯ï¼‰** â† æ–°å¢ï¼

AUC å€¼è¶Šæ¥è¿‘ 1.0ï¼Œæ¨¡å‹åŒºåˆ†è®½åˆº/éè®½åˆºçš„èƒ½åŠ›è¶Šå¼ºï¼

## 2026-02-06 æ•°æ®é›†ç®¡ç†æ¶æ„è®¾è®¡

### ğŸ¯ ç›®æ ‡
å»ºç«‹è§„èŒƒçš„æ•°æ®é›†ç®¡ç†æ¶æ„ï¼Œå®ç°æ•°æ®æ¥æºå¯è¿½æº¯ã€å¤„ç†æµç¨‹å¯å¤ç°ã€‚

### ğŸ“ ç›®å½•ç»“æ„è®¾è®¡

```
dataset/
â”œâ”€â”€ raw_source/              # åŸå§‹æ•°æ®ä»“åº“
â”‚   â”œâ”€â”€ ChnSentiCorp/       # æƒ…æ„ŸåŸºçŸ³æ•°æ®é›†
â”‚   â””â”€â”€ LCCC/               # ç½‘ç»œè¯­è¨€è¯­æ–™
â”œâ”€â”€ processed/              # å¤„ç†åçš„è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ train.json
â”‚   â”œâ”€â”€ dev.json
â”‚   â””â”€â”€ test.json
â”œâ”€â”€ download_datasets.py    # æ•°æ®ä¸‹è½½è„šæœ¬ âœ… æ–°å¢
â”œâ”€â”€ build_dataset.py        # æ•°æ®å¤„ç†è„šæœ¬
â””â”€â”€ README.md              # ç›®å½•è¯´æ˜æ–‡æ¡£ âœ… æ–°å¢
```

### ğŸ”§ è®¾è®¡ç†å¿µ

#### ä¸‰å±‚æ¶æ„
```
raw_source (åŸææ–™)
    â†“
build_dataset.py (åŠ å·¥æœº)
    â†“
processed (æˆå“)
```

#### 1ï¸âƒ£ raw_source/ - åŸå§‹æ•°æ®ä»“åº“
**å®šä½**: ä¸å¯å˜çš„æ•°æ®æºå¤´

**ç‰¹ç‚¹**:
- âœ… ä» HuggingFace ä¸‹è½½çš„åŸå§‹æ–‡ä»¶
- âœ… ä¸ä¿®æ”¹ã€ä¸æ¸…æ´—ã€ä¸æ··åˆ
- âœ… æ°¸è¿œä½œä¸ºå¤‡ä»½æºå¤´
- âœ… ä¿è¯æ•°æ®æ¥æºå¯è¿½æº¯

**å­ç›®å½•**:
- `ChnSentiCorp/`: æƒ…æ„Ÿåˆ†ææ•°æ®ï¼ˆæ­£é¢/è´Ÿé¢æ ‡ç­¾ï¼‰
  - ä½œç”¨: æ•™æ¨¡å‹åŒºåˆ†"å¥½"å’Œ"å"
- `LCCC/`: å¯¹è¯æ•°æ®ï¼ˆç½‘ç»œè¯­è¨€ã€å£è¯­åŒ–ï¼‰
  - ä½œç”¨: æ•™æ¨¡å‹ç†è§£ç½‘ç»œè¯­å¢ƒ

#### 2ï¸âƒ£ processed/ - å¤„ç†åçš„æ•°æ®
**å®šä½**: çœŸæ­£é€å…¥æ¨¡å‹è®­ç»ƒçš„æ•°æ®

**ç‰¹ç‚¹**:
- âœ… æ ¼å¼ç»Ÿä¸€ï¼ˆJSONï¼‰
- âœ… å†…å®¹æ¥è‡ª raw_source
- âœ… ç»è¿‡æ¸…æ´—ã€æ··åˆã€é‡é‡‡æ ·
- âœ… å¯ä»¥é‡æ–°ç”Ÿæˆ

#### 3ï¸âƒ£ download_datasets.py - æ•°æ®ä¸‹è½½è„šæœ¬
**å®šä½**: æ•°æ®è·å–å·¥å…·

**åŠŸèƒ½**:
- ä» HuggingFace ä¸‹è½½æ•°æ®é›†
- ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ï¼ˆJSONL æ ¼å¼ï¼‰
- ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯æ–‡æ¡£

#### 4ï¸âƒ£ build_dataset.py - æ•°æ®å¤„ç†è„šæœ¬
**å®šä½**: æ•°æ®åŠ å·¥æœº

**åŠŸèƒ½**:
- è¯»å– raw_source çš„åŸå§‹æ•°æ®
- æ•°æ®æ¸…æ´—å’Œæ ¼å¼è½¬æ¢
- è¾“å‡ºåˆ° processed/ ç›®å½•

### ğŸ“ æ–°å¢æ–‡ä»¶è¯¦è§£

#### 1. download_datasets.py âœ…

**æ ¸å¿ƒåŠŸèƒ½**:
```python
def download_chnsenticorp(save_dir):
    """ä¸‹è½½ ChnSentiCorp æƒ…æ„Ÿåˆ†ææ•°æ®é›†"""
    dataset = load_dataset("seamew/ChnSentiCorp")
    # ä¿å­˜ä¸º JSONL æ ¼å¼
    # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯æ–‡æ¡£

def download_lccc(save_dir):
    """ä¸‹è½½ LCCC å¯¹è¯æ•°æ®é›†"""
    dataset = load_dataset("LCCC-base")
    # ä¿å­˜ä¸º JSONL æ ¼å¼
    # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯æ–‡æ¡£
```

**ç‰¹ç‚¹**:
- ä½¿ç”¨ HuggingFace `datasets` åº“
- ä¿å­˜ä¸º JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSONï¼‰
- è‡ªåŠ¨ç”Ÿæˆ `dataset_info.txt` è¯´æ˜æ–‡æ¡£
- é”™è¯¯å¤„ç†å’Œè¿›åº¦æç¤º

**ä½¿ç”¨æ–¹æ³•**:
```bash
cd dataset
python download_datasets.py
```

**è¾“å‡º**:
- `raw_source/ChnSentiCorp/*.jsonl`
- `raw_source/ChnSentiCorp/dataset_info.txt`
- `raw_source/LCCC/*.jsonl`
- `raw_source/LCCC/dataset_info.txt`

#### 2. dataset/README.md âœ…

**å†…å®¹ç»“æ„**:
1. ğŸ“ ç›®å½•ç»“æ„å›¾
2. ğŸ¯ å„éƒ¨åˆ†è¯´æ˜
3. ğŸš€ å®Œæ•´ä½¿ç”¨æµç¨‹
4. ğŸ“Š æ•°æ®é›†è¯´æ˜
5. ğŸ’¡ è®¾è®¡ç†å¿µ
6. âš ï¸ æ³¨æ„äº‹é¡¹

**ä½œç”¨**:
- æ–°äººå¿«é€Ÿç†è§£æ•°æ®æ¶æ„
- ä½¿ç”¨æµç¨‹æ¸…æ™°æ˜ç¡®
- è®¾è®¡ç†å¿µæ–‡æ¡£åŒ–

### ğŸ¯ è®¾è®¡ä¼˜åŠ¿

#### 1. å¯è¿½æº¯æ€§
- åŸå§‹æ•°æ®ä¿æŒä¸å˜
- å¤„ç†æµç¨‹æœ‰è„šæœ¬è®°å½•
- æ•°æ®æ¥æºæ¸…æ™°å¯æŸ¥

#### 2. å¯å¤ç°æ€§
- ä»»ä½•äººéƒ½å¯ä»¥é‡æ–°ä¸‹è½½åŸå§‹æ•°æ®
- è¿è¡Œç›¸åŒè„šæœ¬å¾—åˆ°ç›¸åŒç»“æœ
- ç¬¦åˆç§‘ç ”è§„èŒƒ

#### 3. å¯ç»´æŠ¤æ€§
- æ•°æ®å’Œä»£ç åˆ†ç¦»
- ç›®å½•ç»“æ„æ¸…æ™°
- ä¾¿äºç‰ˆæœ¬æ§åˆ¶

#### 4. çµæ´»æ€§
- processed å¯ä»¥éšæ—¶é‡æ–°ç”Ÿæˆ
- ä¿®æ”¹å¤„ç†é€»è¾‘ä¸å½±å“åŸå§‹æ•°æ®
- æ”¯æŒå¤šç§æ•°æ®æºç»„åˆ

### ğŸš€ å®Œæ•´å·¥ä½œæµç¨‹

#### æ­¥éª¤ 1: ä¸‹è½½åŸå§‹æ•°æ®
```bash
cd dataset
python download_datasets.py
```
**ç»“æœ**: raw_source/ ä¸­æœ‰åŸå§‹æ•°æ®

#### æ­¥éª¤ 2: å¤„ç†æ•°æ®
```bash
python build_dataset.py
```
**ç»“æœ**: processed/ ä¸­æœ‰è®­ç»ƒæ•°æ®

#### æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹
```bash
cd ..
python main.py --dataset_dir dataset/processed
```
**ç»“æœ**: æ¨¡å‹è®­ç»ƒå®Œæˆ

### ğŸ“‹ æ–‡ä»¶æ¸…å•
- [x] åˆ›å»º `dataset/download_datasets.py`
- [x] åˆ›å»º `dataset/README.md`
- [x] è®¾è®¡ä¸‰å±‚æ•°æ®æ¶æ„
- [x] å®ç° ChnSentiCorp ä¸‹è½½
- [x] å®ç° LCCC ä¸‹è½½
- [x] æ·»åŠ æ•°æ®é›†ä¿¡æ¯ç”Ÿæˆ
- [x] ç¼–å†™å®Œæ•´ä½¿ç”¨æ–‡æ¡£

### ğŸ’¡ ä¸‹ä¸€æ­¥
- [ ] å®ç° `build_dataset.py` çš„æ•°æ®å¤„ç†é€»è¾‘
- [ ] æµ‹è¯•å®Œæ•´çš„æ•°æ®æµç¨‹
- [ ] æ·»åŠ æ•°æ®ç»Ÿè®¡å’Œå¯è§†åŒ–

### ğŸ‰ æ•ˆæœ
ç°åœ¨æœ‰äº†è§„èŒƒçš„æ•°æ®ç®¡ç†æ¶æ„ï¼š
- âœ… åŸå§‹æ•°æ®å’Œå¤„ç†æ•°æ®åˆ†ç¦»
- âœ… æ•°æ®æ¥æºå¯è¿½æº¯
- âœ… å¤„ç†æµç¨‹å¯å¤ç°
- âœ… ç¬¦åˆç§‘ç ”è§„èŒƒ
- âœ… ä¾¿äºå›¢é˜Ÿåä½œ

## 2026-02-06 è¡¥å…… build_dataset.py å®ç°

### ğŸ› é—®é¢˜å‘ç°
- `build_dataset.py` æ–‡ä»¶æ˜¯ç©ºçš„
- éœ€è¦å®ç°æ•°æ®å¤„ç†é€»è¾‘

### ğŸ”§ å®ç°å†…å®¹

#### 1. build_dataset.py å®Œæ•´å®ç° âœ…

**æ ¸å¿ƒåŠŸèƒ½**:
```python
def process_chnsenticorp():
    """å¤„ç† ChnSentiCorp æƒ…æ„Ÿåˆ†ææ•°æ®"""
    # åŠ è½½ train/validation/test.jsonl
    # æå– text å’Œ label
    # ç»Ÿä¸€æ ¼å¼: {text, topic, label}

def process_lccc(max_samples=10000):
    """å¤„ç† LCCC å¯¹è¯æ•°æ®"""
    # åŠ è½½å¯¹è¯æ•°æ®
    # éšæœºé‡‡æ ·ï¼ˆæ•°æ®é‡å¤ªå¤§ï¼‰
    # æ‹¼æ¥å¯¹è¯å†…å®¹
    # é»˜è®¤ label=0ï¼ˆæ— æ ‡ç­¾ï¼‰

def main():
    """ä¸»æµç¨‹"""
    # 1. å¤„ç† ChnSentiCorp
    # 2. å¤„ç† LCCCï¼ˆå¯é€‰ï¼‰
    # 3. æ··åˆæ•°æ®åˆ°è®­ç»ƒé›†
    # 4. ä¿å­˜ä¸º processed/*.json
```

**ç‰¹ç‚¹**:
- âœ… æ”¯æŒ JSONL æ ¼å¼è¯»å–
- âœ… æ•°æ®æ¸…æ´—ï¼ˆè¿‡æ»¤ç©ºæ–‡æœ¬ï¼‰
- âœ… æ ¼å¼ç»Ÿä¸€ï¼ˆtext + topic + labelï¼‰
- âœ… LCCC éšæœºé‡‡æ ·ï¼ˆé¿å…æ•°æ®é‡è¿‡å¤§ï¼‰
- âœ… æ•°æ®æ··åˆå’Œæ‰“ä¹±
- âœ… è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯

**è¾“å‡ºæ ¼å¼**:
```json
[
  {
    "text": "è¯„è®ºæ–‡æœ¬",
    "topic": "",
    "label": "0"
  }
]
```

#### 2. download_datasets.py ä¿®æ­£ âœ…

**é—®é¢˜**: LCCC æ•°æ®é›†åç§°å¯èƒ½ä¸æ­£ç¡®

**ä¿®æ­£**:
```python
# å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®é›†åç§°
try:
    dataset = load_dataset("silver/lccc")
except:
    try:
        dataset = load_dataset("LCCC-base")
    except:
        dataset = load_dataset("lccc")
```

**åŸå› **:
- HuggingFace ä¸Š LCCC æ•°æ®é›†å¯èƒ½æœ‰å¤šä¸ªç‰ˆæœ¬
- ä¸åŒçš„å‘½åç©ºé—´ï¼ˆsilver/lccc, LCCC-base, lcccï¼‰
- å¢åŠ å®¹é”™æ€§

### ğŸš€ ä½¿ç”¨æµç¨‹

#### æ­¥éª¤ 1: ä¸‹è½½åŸå§‹æ•°æ®
```bash
cd dataset
python download_datasets.py
```

**å¯èƒ½çš„é—®é¢˜**:
- ç½‘ç»œè¿æ¥é—®é¢˜ï¼ˆéœ€è¦è®¿é—® HuggingFaceï¼‰
- æ•°æ®é›†åç§°ä¸åŒ¹é…
- ç¼ºå°‘ `datasets` åº“

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å®‰è£…ä¾èµ–
pip install datasets

# å¦‚æœç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½æˆ–ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

#### æ­¥éª¤ 2: å¤„ç†æ•°æ®
```bash
python build_dataset.py
```

**è¾“å‡º**:
- `processed/train.json`
- `processed/dev.json`
- `processed/test.json`

### ğŸ“Š æ•°æ®å¤„ç†ç­–ç•¥

#### ChnSentiCorpï¼ˆæƒ…æ„ŸåŸºçŸ³ï¼‰
- **æ¥æº**: train/validation/test.jsonl
- **æ ‡ç­¾**: 0ï¼ˆè´Ÿé¢ï¼‰ã€1ï¼ˆæ­£é¢ï¼‰
- **å¤„ç†**: ç›´æ¥æå– text å’Œ label
- **ä½œç”¨**: æä¾›æœ‰æ ‡ç­¾çš„æƒ…æ„Ÿæ•°æ®

#### LCCCï¼ˆç½‘ç»œè¯­è¨€ï¼‰
- **æ¥æº**: train.jsonl
- **æ ‡ç­¾**: æ— ï¼ˆé»˜è®¤ 0ï¼‰
- **å¤„ç†**: 
  - éšæœºé‡‡æ · 5000 æ ·æœ¬ï¼ˆé¿å…è¿‡å¤§ï¼‰
  - æ‹¼æ¥å¯¹è¯å†…å®¹
  - æ··å…¥è®­ç»ƒé›†
- **ä½œç”¨**: å¢å¼ºç½‘ç»œè¯­è¨€ç†è§£

#### æ··åˆç­–ç•¥
```python
train_data = chnsenticorp_train + lccc_samples
random.shuffle(train_data)
```

**å¥½å¤„**:
- æœ‰æ ‡ç­¾æ•°æ® + æ— æ ‡ç­¾æ•°æ®
- æƒ…æ„Ÿåˆ†æ + ç½‘ç»œè¯­è¨€
- å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›

### âš ï¸ å…³äºæ•°æ®ä¸‹è½½å¤±è´¥

å¦‚æœ `download_datasets.py` æ— æ³•ä¸‹è½½æ•°æ®ï¼Œå¯èƒ½çš„åŸå› ï¼š

1. **ç½‘ç»œé—®é¢˜**
   - HuggingFace éœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥
   - å¯ä»¥ä½¿ç”¨é•œåƒç«™ç‚¹

2. **æ•°æ®é›†åç§°é—®é¢˜**
   - `seamew/ChnSentiCorp` å¯èƒ½éœ€è¦ç¡®è®¤
   - `LCCC` æœ‰å¤šä¸ªç‰ˆæœ¬ï¼Œåç§°å¯èƒ½ä¸åŒ

3. **æ›¿ä»£æ–¹æ¡ˆ**
   - æ‰‹åŠ¨ä» HuggingFace ç½‘ç«™ä¸‹è½½
   - ä½¿ç”¨å…¶ä»–æ•°æ®æº
   - ç›´æ¥ä½¿ç”¨ç°æœ‰çš„ `dataset/train.json` ç­‰æ–‡ä»¶

### ğŸ’¡ å¦‚æœæ•°æ®å·²ç»å­˜åœ¨

å¦‚æœä½ å·²ç»æœ‰ `dataset/train.json` ç­‰æ–‡ä»¶ï¼š
```bash
# ç›´æ¥è®­ç»ƒï¼Œä¸éœ€è¦ä¸‹è½½å’Œå¤„ç†
python main.py --dataset_dir dataset
```

### ğŸ“‹ æ–‡ä»¶æ¸…å•
- [x] å®ç° `build_dataset.py` å®Œæ•´é€»è¾‘
- [x] ä¿®æ­£ `download_datasets.py` LCCC åç§°
- [x] æ·»åŠ é”™è¯¯å¤„ç†å’Œå®¹é”™
- [x] æ·»åŠ æ•°æ®ç»Ÿè®¡åŠŸèƒ½
- [x] æµ‹è¯•ä»£ç è¯­æ³•æ­£ç¡®æ€§

### ğŸ¯ ä»£ç è´¨é‡
- âœ… è¯­æ³•æ£€æŸ¥é€šè¿‡
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†
- âœ… è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
- âœ… çµæ´»çš„é…ç½®é€‰é¡¹
- âœ… æ¸…æ™°çš„ä»£ç æ³¨é‡Š

## 2026-02-06 ä¿®æ­£æ•°æ®é›†åç§°ï¼ˆé‡è¦ï¼ï¼‰

### ğŸ› é—®é¢˜å‘ç°
æ ¹æ® HuggingFace æˆªå›¾ï¼Œå‘ç° LCCC æ•°æ®é›†åç§°å†™é”™äº†ï¼

### âœ… æ­£ç¡®çš„æ•°æ®é›†åç§°

#### 1. ChnSentiCorp âœ…
- **HuggingFace åœ°å€**: `seamew/ChnSentiCorp`
- **URL**: https://huggingface.co/datasets/seamew/ChnSentiCorp
- **çŠ¶æ€**: ä»£ç ä¸­å·²æ­£ç¡® âœ…

#### 2. LCCC âŒ â†’ âœ…
- **é”™è¯¯åç§°**: `LCCC-base`, `silver/lccc`, `lccc`
- **æ­£ç¡®åç§°**: `thu-coai/lccc` âœ…
- **URL**: https://huggingface.co/datasets/thu-coai/lccc
- **çŠ¶æ€**: å·²ä¿®æ­£ âœ…

### ğŸ”§ ä¿®æ­£å†…å®¹

#### download_datasets.py ä¿®æ­£

**ä¿®æ­£å‰ï¼ˆé”™è¯¯ï¼‰**:
```python
try:
    dataset = load_dataset("silver/lccc")
except:
    try:
        dataset = load_dataset("LCCC-base")
    except:
        dataset = load_dataset("lccc")
```

**ä¿®æ­£åï¼ˆæ­£ç¡®ï¼‰**:
```python
# ä» HuggingFace åŠ è½½æ•°æ®é›†
# æ­£ç¡®çš„æ•°æ®é›†åç§°: thu-coai/lccc
dataset = load_dataset("thu-coai/lccc")
```

**åŒæ—¶ä¿®æ­£ info æ–‡ä»¶**:
```python
f.write(f"æ¥æº: HuggingFace - thu-coai/lccc\n")  # ä¿®æ­£
```

### ğŸ“Š æ•°æ®é›†ä¿¡æ¯ç¡®è®¤

#### seamew/ChnSentiCorp
- **ä»»åŠ¡**: æƒ…æ„Ÿåˆ†æï¼ˆäºŒåˆ†ç±»ï¼‰
- **æ ‡ç­¾**: 0ï¼ˆè´Ÿé¢ï¼‰ã€1ï¼ˆæ­£é¢ï¼‰
- **è¯­è¨€**: ä¸­æ–‡
- **ä¸‹è½½é‡**: 619/æœˆï¼ˆæˆªå›¾æ˜¾ç¤ºï¼‰
- **ç”¨é€”**: æƒ…æ„ŸåŸºçŸ³æ•°æ®é›†

#### thu-coai/lccc
- **ä»»åŠ¡**: å¯¹è¯ç”Ÿæˆï¼ˆdialogue-generationï¼‰
- **è¯­è¨€**: ä¸­æ–‡ï¼ˆChineseï¼‰
- **è§„æ¨¡**: 10M-100M æ ·æœ¬
- **ArXiv**: 2008.03946
- **è®¸å¯**: MIT
- **ä¸‹è½½é‡**: 318/æœˆï¼ˆæˆªå›¾æ˜¾ç¤ºï¼‰
- **ç”¨é€”**: ç½‘ç»œè¯­è¨€ã€å£è¯­åŒ–è¡¨è¾¾

### ğŸš€ ç°åœ¨å¯ä»¥æ­£ç¡®ä¸‹è½½äº†

```bash
cd dataset
python download_datasets.py
```

**é¢„æœŸè¾“å‡º**:
```
ğŸ“¥ å¼€å§‹ä¸‹è½½ ChnSentiCorp æ•°æ®é›†...
âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼
   - å¯ç”¨åˆ†å‰²: ['train', 'validation', 'test']

ğŸ“¥ å¼€å§‹ä¸‹è½½ LCCC æ•°æ®é›†...
âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼
   - å¯ç”¨åˆ†å‰²: ['train', 'test']
```

### ğŸ“‹ ä¿®æ­£æ¸…å•
- [x] ä¿®æ­£ LCCC æ•°æ®é›†åç§°: `thu-coai/lccc`
- [x] åˆ é™¤é”™è¯¯çš„å¤šé‡ try-except
- [x] ä¿®æ­£ dataset_info.txt ä¸­çš„æ¥æºä¿¡æ¯
- [x] ç¡®è®¤ä¸¤ä¸ªæ•°æ®é›†åç§°éƒ½æ­£ç¡®

### ğŸ’¡ æ„Ÿè°¢ç”¨æˆ·æä¾›æˆªå›¾
é€šè¿‡æˆªå›¾ç¡®è®¤äº†æ­£ç¡®çš„æ•°æ®é›†åç§°ï¼Œé¿å…äº†ä¸‹è½½å¤±è´¥çš„é—®é¢˜ï¼

ç°åœ¨ä»£ç åº”è¯¥å¯ä»¥æ­£å¸¸ä¸‹è½½æ•°æ®äº†ã€‚ğŸ‰

## 2026-02-06 æ¾„æ¸…æ•°æ®ä¸‹è½½è„šæœ¬å’Œè·¯å¾„

### ğŸ” é—®é¢˜å‘ç°
ç”¨æˆ·å‘ç°äº† `lccc.py` æ–‡ä»¶ï¼Œéœ€è¦ç¡®è®¤æ­£ç¡®çš„ä¸‹è½½è„šæœ¬å’Œè·¯å¾„ã€‚

### ğŸ“ æ–‡ä»¶è¯´æ˜

#### 1. download_datasets.py âœ… (æ­£ç¡®çš„è„šæœ¬)
**è¿™æ˜¯æˆ‘ä»¬åº”è¯¥ä½¿ç”¨çš„ä¸‹è½½è„šæœ¬**

**å…³é”®ä»£ç **:
```python
def download_lccc(save_dir: str = "raw_source/LCCC"):
    """ä¸‹è½½ LCCC å¯¹è¯æ•°æ®é›†"""
    dataset = load_dataset("thu-coai/lccc")  # âœ… æ­£ç¡®çš„æ•°æ®é›†åç§°
    # ä¿å­˜åˆ° raw_source/LCCC/
```

**ç‰¹ç‚¹**:
- âœ… ä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†åç§°: `thu-coai/lccc`
- âœ… ä¿å­˜åˆ°æ­£ç¡®çš„è·¯å¾„: `raw_source/LCCC/`
- âœ… åŒæ—¶ä¸‹è½½ ChnSentiCorp å’Œ LCCC
- âœ… ç”Ÿæˆ dataset_info.txt è¯´æ˜æ–‡æ¡£

#### 2. lccc.py âŒ (ä¸éœ€è¦çš„æ–‡ä»¶)
**è¿™æ˜¯ HuggingFace æ•°æ®é›†çš„å†…éƒ¨åŠ è½½è„šæœ¬**

**é—®é¢˜**:
- âŒ æŒ‡å‘æ—§çš„æ•°æ®æº: `silver/lccc`
- âŒ è¿™æ˜¯ HuggingFace çš„å†…éƒ¨è„šæœ¬ï¼Œä¸æ˜¯æˆ‘ä»¬çš„ä¸‹è½½è„šæœ¬
- âŒ å¯èƒ½æ˜¯ä¹‹å‰ä¸‹è½½æ—¶è‡ªåŠ¨ç”Ÿæˆçš„

**å»ºè®®**: å¯ä»¥åˆ é™¤æˆ–å¿½ç•¥è¿™ä¸ªæ–‡ä»¶

### âœ… æ­£ç¡®çš„ä½¿ç”¨æ–¹æ³•

#### ä¸‹è½½æ•°æ®
```bash
cd dataset
python download_datasets.py  # âœ… ä½¿ç”¨è¿™ä¸ªè„šæœ¬
```

**ä¸è¦è¿è¡Œ**:
```bash
python lccc.py  # âŒ ä¸è¦ç”¨è¿™ä¸ª
```

### ğŸ“Š è·¯å¾„ç¡®è®¤

#### ChnSentiCorp
- **æ•°æ®é›†**: `seamew/ChnSentiCorp`
- **ä¿å­˜è·¯å¾„**: `dataset/raw_source/ChnSentiCorp/`
- **æ–‡ä»¶**: train.jsonl, validation.jsonl, test.jsonl

#### LCCC
- **æ•°æ®é›†**: `thu-coai/lccc` âœ…
- **ä¿å­˜è·¯å¾„**: `dataset/raw_source/LCCC/` âœ…
- **æ–‡ä»¶**: train.jsonl, test.jsonl, validation.jsonl

### ğŸ”§ éªŒè¯ä¸‹è½½ç»“æœ

#### æ–¹æ³• 1: æ£€æŸ¥æ–‡ä»¶
```bash
# Windows
dir dataset\raw_source\LCCC\*.jsonl
dir dataset\raw_source\ChnSentiCorp\*.jsonl
```

#### æ–¹æ³• 2: Python æ£€æŸ¥
```python
import os
print("ChnSentiCorp:", os.listdir('dataset/raw_source/ChnSentiCorp'))
print("LCCC:", os.listdir('dataset/raw_source/LCCC'))
```

### ğŸ“ æ–°å¢æ–‡ä»¶

#### dataset/USAGE.md âœ…
åˆ›å»ºäº†è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜æ–‡æ¡£ï¼ŒåŒ…æ‹¬ï¼š
- âš ï¸ é‡è¦æç¤ºï¼ˆä½¿ç”¨æ­£ç¡®çš„è„šæœ¬ï¼‰
- ğŸ“¥ ä¸‹è½½æ­¥éª¤
- ğŸ”§ æ•°æ®é›†ä¿¡æ¯
- ğŸ› å¸¸è§é—®é¢˜è§£ç­”
- ğŸ“ ä»£ç ç¡®è®¤
- ğŸš€ å®Œæ•´æµç¨‹
- ğŸ—‘ï¸ æ¸…ç†å»ºè®®

### ğŸ¯ æ€»ç»“

**æ­£ç¡®çš„å·¥ä½œæµç¨‹**:
1. ä½¿ç”¨ `download_datasets.py` ä¸‹è½½æ•°æ® âœ…
2. æ•°æ®ä¿å­˜åˆ° `raw_source/ChnSentiCorp/` å’Œ `raw_source/LCCC/` âœ…
3. ä½¿ç”¨ `build_dataset.py` å¤„ç†æ•°æ® âœ…
4. è®­ç»ƒæ¨¡å‹ âœ…

**å¯ä»¥å¿½ç•¥çš„æ–‡ä»¶**:
- `lccc.py` - HuggingFace å†…éƒ¨è„šæœ¬ï¼Œä¸éœ€è¦

### ğŸ“‹ æ£€æŸ¥æ¸…å•
- [x] ç¡®è®¤ download_datasets.py ä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†åç§°
- [x] ç¡®è®¤ä¿å­˜è·¯å¾„ä¸º raw_source/LCCC/
- [x] æ¾„æ¸… lccc.py çš„ä½œç”¨ï¼ˆä¸éœ€è¦ï¼‰
- [x] åˆ›å»º USAGE.md ä½¿ç”¨è¯´æ˜
- [x] æä¾›éªŒè¯æ–¹æ³•

ç°åœ¨åº”è¯¥æ¸…æ¥šäº†ï¼šä½¿ç”¨ `download_datasets.py`ï¼Œæ•°æ®ä¼šæ­£ç¡®ä¿å­˜åˆ° `raw_source/LCCC/`ï¼

## 2026-02-06 æ•°æ®é›†ä¸‹è½½é—®é¢˜è¯Šæ–­å’Œè§£å†³

### ğŸ› æ ¸å¿ƒé—®é¢˜
```
RuntimeError: Dataset scripts are no longer supported, but found ChnSentiCorp.py/lccc.py
```

### ğŸ“Š é—®é¢˜åˆ†æ

#### æ ¹æœ¬åŸå› 
1. **HuggingFace æ”¿ç­–å˜æ›´**: æ–°ç‰ˆæœ¬ `datasets` åº“ä¸å†æ”¯æŒæ•°æ®é›†è„šæœ¬
2. **æ•°æ®é›†åŒ…å«æ—§è„šæœ¬**: `seamew/ChnSentiCorp` å’Œ `thu-coai/lccc` éƒ½åŒ…å« `.py` è„šæœ¬æ–‡ä»¶
3. **ç¼“å­˜é—®é¢˜**: å³ä½¿åˆ é™¤æœ¬åœ° `lccc.py`ï¼ŒHuggingFace ä¼šé‡æ–°ä¸‹è½½

### ğŸ”§ å°è¯•çš„è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1: åˆ é™¤æœ¬åœ°è„šæœ¬æ–‡ä»¶ âŒ
- åˆ é™¤äº† `dataset/lccc.py`
- ç»“æœ: HuggingFace é‡æ–°ä¸‹è½½è„šæœ¬åˆ°ç¼“å­˜

#### æ–¹æ¡ˆ 2: æ·»åŠ  trust_remote_code=False âŒ
- æ˜ç¡®æ‹’ç»åŠ è½½è¿œç¨‹ä»£ç 
- ç»“æœ: ä»ç„¶æ£€æµ‹åˆ°è„šæœ¬å¹¶æŠ¥é”™

#### æ–¹æ¡ˆ 3: æ¸…ç†ç¼“å­˜å¹¶å¼ºåˆ¶é‡æ–°ä¸‹è½½ âŒ
- åˆ›å»º `clear_hf_cache.py` æ¸…ç†ç¼“å­˜
- ä½¿ç”¨ `download_mode="force_redownload"`
- ç»“æœ: é‡æ–°ä¸‹è½½åä»ç„¶åŒ…å«è„šæœ¬æ–‡ä»¶

### ğŸ’¡ æœ€ç»ˆè§£å†³æ–¹æ¡ˆ

#### é€‰é¡¹ A: ä½¿ç”¨ trust_remote_code=True (æ¨è)
è™½ç„¶æœ‰å®‰å…¨è­¦å‘Šï¼Œä½†è¿™äº›æ˜¯çŸ¥åæ•°æ®é›†ï¼Œå¯ä»¥ä¿¡ä»»ï¼š

```python
dataset = load_dataset("seamew/ChnSentiCorp", trust_remote_code=True)
dataset = load_dataset("thu-coai/lccc", trust_remote_code=True)
```

#### é€‰é¡¹ B: å¯»æ‰¾æ›¿ä»£æ•°æ®é›†
æŸ¥æ‰¾æ²¡æœ‰è„šæœ¬æ–‡ä»¶çš„æ•°æ®é›†ç‰ˆæœ¬

#### é€‰é¡¹ C: æ‰‹åŠ¨ä¸‹è½½
ç›´æ¥ä» HuggingFace ç½‘ç«™ä¸‹è½½ Parquet/CSV æ–‡ä»¶

#### é€‰é¡¹ D: ä½¿ç”¨ç°æœ‰æ•°æ®
å¦‚æœå·²ç»æœ‰ `dataset/train.json` ç­‰æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨

### ğŸ“‹ åˆ›å»ºçš„æ–‡ä»¶
- [x] `dataset/download_datasets_v2.py` - æ”¹è¿›çš„ä¸‹è½½è„šæœ¬
- [x] `dataset/clear_hf_cache.py` - ç¼“å­˜æ¸…ç†å·¥å…·
- [x] `dataset/USAGE.md` - ä½¿ç”¨è¯´æ˜æ–‡æ¡£

### ğŸ¯ å»ºè®®
ç”±äºè¿™æ˜¯ HuggingFace å’Œæ•°æ®é›†æœ¬èº«çš„é—®é¢˜ï¼Œå»ºè®®ï¼š
1. å¦‚æœç”¨æˆ·å·²æœ‰æ•°æ®æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
2. æˆ–è€…æˆ‘åˆ›å»ºä¸€ä¸ªä½¿ç”¨ `trust_remote_code=True` çš„ç‰ˆæœ¬
3. æˆ–è€…æä¾›æ‰‹åŠ¨ä¸‹è½½çš„è¯´æ˜

ç”¨æˆ·å¯ä»¥é€‰æ‹©æœ€é€‚åˆçš„æ–¹æ¡ˆã€‚

## 2026-02-06 æ•°æ®é›†ä¸‹è½½é—®é¢˜æœ€ç»ˆè§£å†³

### ğŸ‰ æˆåŠŸè§£å†³ ChnSentiCorp

#### é—®é¢˜å›é¡¾
- HuggingFace æ–°ç‰ˆæœ¬ä¸æ”¯æŒæ•°æ®é›†è„šæœ¬
- `trust_remote_code` å‚æ•°ä¹Ÿè¢«åºŸå¼ƒ
- æ— æ³•ç›´æ¥ä½¿ç”¨ `load_dataset()` ä¸‹è½½

#### è§£å†³æ–¹æ¡ˆ
å‘ç° ChnSentiCorp æ•°æ®å·²ç»åœ¨ç¼“å­˜ä¸­ï¼ˆ.arrow æ–‡ä»¶ï¼‰ï¼Œåˆ›å»ºè½¬æ¢è„šæœ¬ï¼š

**convert_cached_to_jsonl.py** âœ…
```python
# ä½¿ç”¨ datasets åº“åŠ è½½ç¼“å­˜çš„ arrow æ–‡ä»¶
dataset = Dataset.from_file(str(arrow_file))

# è½¬æ¢ä¸º JSONL æ ¼å¼
for item in dataset:
    json.dump(item, f, ensure_ascii=False)
    f.write('\n')
```

#### è½¬æ¢ç»“æœ
```
âœ… ChnSentiCorp è½¬æ¢å®Œæˆï¼
   - train.jsonl: 9600 æ ·æœ¬
   - validation.jsonl: 1200 æ ·æœ¬
   - test.jsonl: 1200 æ ·æœ¬
   - ç‰¹å¾: ['label', 'text']
```

### ğŸ“Š å½“å‰çŠ¶æ€

#### ChnSentiCorp âœ…
- **çŠ¶æ€**: å·²å®Œæˆ
- **ä½ç½®**: `dataset/raw_source/ChnSentiCorp/`
- **æ–‡ä»¶**: train.jsonl, validation.jsonl, test.jsonl
- **æ ·æœ¬æ•°**: 12000 æ¡

#### LCCC âŒ
- **çŠ¶æ€**: æœªä¸‹è½½
- **åŸå› **: åŒæ ·çš„è„šæœ¬é—®é¢˜
- **è§£å†³æ–¹æ¡ˆ**: 
  1. å¯»æ‰¾æ›¿ä»£æ•°æ®é›†
  2. æ‰‹åŠ¨ä¸‹è½½
  3. æˆ–è€…æš‚æ—¶è·³è¿‡ï¼ˆChnSentiCorp å·²è¶³å¤Ÿè®­ç»ƒï¼‰

### ğŸ’¡ å»ºè®®

#### é€‰é¡¹ 1: ä»…ä½¿ç”¨ ChnSentiCorpï¼ˆæ¨èï¼‰
```bash
cd dataset
python build_dataset.py  # ä¼šå¤„ç† ChnSentiCorp
cd ..
python main.py --dataset_dir dataset/processed
```

**ä¼˜ç‚¹**:
- æ•°æ®å·²å°±ç»ª
- 12000 æ¡æ ·æœ¬è¶³å¤Ÿè®­ç»ƒ
- æœ‰æ˜ç¡®çš„æƒ…æ„Ÿæ ‡ç­¾

#### é€‰é¡¹ 2: æ‰‹åŠ¨ä¸‹è½½ LCCC
ä» HuggingFace ç½‘ç«™æ‰‹åŠ¨ä¸‹è½½ Parquet æ–‡ä»¶

#### é€‰é¡¹ 3: å¯»æ‰¾æ›¿ä»£æ•°æ®é›†
æŸ¥æ‰¾å…¶ä»–ä¸­æ–‡å¯¹è¯æ•°æ®é›†

### ğŸ“‹ åˆ›å»ºçš„å·¥å…·è„šæœ¬
- [x] `download_datasets.py` - åŸå§‹ä¸‹è½½è„šæœ¬
- [x] `download_datasets_v2.py` - æ”¹è¿›ç‰ˆæœ¬
- [x] `download_datasets_final.py` - æœ€ç»ˆç‰ˆæœ¬
- [x] `clear_hf_cache.py` - ç¼“å­˜æ¸…ç†å·¥å…·
- [x] `convert_arrow_to_jsonl.py` - Arrow è½¬æ¢ï¼ˆå¤±è´¥ï¼‰
- [x] `convert_cached_to_jsonl.py` - ç¼“å­˜è½¬æ¢ï¼ˆæˆåŠŸï¼‰âœ…
- [x] `USAGE.md` - ä½¿ç”¨è¯´æ˜

### ğŸ¯ ä¸‹ä¸€æ­¥

ç°åœ¨å¯ä»¥è¿è¡Œæ•°æ®å¤„ç†è„šæœ¬ï¼š
```bash
cd dataset
python build_dataset.py
```

è¿™ä¼šå°† ChnSentiCorp çš„æ•°æ®å¤„ç†æˆç»Ÿä¸€æ ¼å¼ï¼Œä¿å­˜åˆ° `processed/` ç›®å½•ã€‚

### ğŸ“ ç»éªŒæ€»ç»“

1. **HuggingFace æ”¿ç­–å˜æ›´**: æ•°æ®é›†è„šæœ¬å·²è¢«åºŸå¼ƒ
2. **ç¼“å­˜åˆ©ç”¨**: å·²ä¸‹è½½çš„æ•°æ®å¯ä»¥ä»ç¼“å­˜è½¬æ¢
3. **æ ¼å¼è½¬æ¢**: ä½¿ç”¨ `Dataset.from_file()` åŠ è½½ arrow æ–‡ä»¶
4. **çµæ´»åº”å¯¹**: å½“ç›´æ¥ä¸‹è½½å¤±è´¥æ—¶ï¼Œå¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆ

è™½ç„¶ LCCC ä¸‹è½½å¤±è´¥ï¼Œä½† ChnSentiCorp æ•°æ®å·²è¶³å¤Ÿè¿›è¡Œæƒ…æ„Ÿåˆ†æè®­ç»ƒï¼ğŸ‰

## 2026-02-06 æ¸…ç† LCCC ç›¸å…³ä»£ç å’Œæ–‡ä»¶

### ğŸ—‘ï¸ æ¸…ç†åŸå› 
- LCCC æ•°æ®é›†ä¸‹è½½å¤±è´¥ï¼ˆHuggingFace è„šæœ¬é—®é¢˜ï¼‰
- ChnSentiCorp æ•°æ®å·²è¶³å¤Ÿè®­ç»ƒï¼ˆ12000 æ¡æ ·æœ¬ï¼‰
- ç®€åŒ–é¡¹ç›®ç»“æ„ï¼Œç§»é™¤ä¸éœ€è¦çš„ä»£ç 

### ğŸ“‹ åˆ é™¤çš„æ–‡ä»¶

#### ä¸‹è½½è„šæœ¬ï¼ˆå…¨éƒ¨åˆ é™¤ï¼‰
- âŒ `dataset/download_datasets.py`
- âŒ `dataset/download_datasets_v2.py`
- âŒ `dataset/download_datasets_final.py`
- **åŸå› **: æ— æ³•æ­£å¸¸å·¥ä½œï¼Œä¸”æ•°æ®å·²æ‰‹åŠ¨è·å–

#### å·¥å…·è„šæœ¬ï¼ˆåˆ é™¤ä¸éœ€è¦çš„ï¼‰
- âŒ `dataset/clear_hf_cache.py` - ç¼“å­˜æ¸…ç†å·¥å…·
- âŒ `dataset/convert_arrow_to_jsonl.py` - å¤±è´¥çš„è½¬æ¢è„šæœ¬
- âŒ `dataset/USAGE.md` - ä½¿ç”¨è¯´æ˜æ–‡æ¡£
- **ä¿ç•™**: `dataset/convert_cached_to_jsonl.py` âœ… (æˆåŠŸçš„è½¬æ¢å·¥å…·)

#### LCCC ç›¸å…³
- âŒ `dataset/raw_source/LCCC/` - æ•´ä¸ªç›®å½•åˆ é™¤

### ğŸ”§ ä¿®æ”¹çš„æ–‡ä»¶

#### 1. build_dataset.py âœ…
**åˆ é™¤å†…å®¹**:
- `process_lccc()` å‡½æ•°
- LCCC æ•°æ®æ··åˆé€»è¾‘
- LCCC ç›¸å…³çš„ try-except å—

**ä¿ç•™å†…å®¹**:
- `process_chnsenticorp()` å‡½æ•°
- æ•°æ®æ¸…æ´—å’Œæ ¼å¼è½¬æ¢
- ç»Ÿè®¡ä¿¡æ¯è¾“å‡º

**ç®€åŒ–åçš„æµç¨‹**:
```python
def main():
    # å¤„ç† ChnSentiCorp
    train_data, dev_data, test_data = process_chnsenticorp()
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    save_json(train_data, "processed/train.json")
    save_json(dev_data, "processed/dev.json")
    save_json(test_data, "processed/test.json")
```

#### 2. dataset/README.md âœ…
**åˆ é™¤å†…å®¹**:
- LCCC æ•°æ®é›†è¯´æ˜
- LCCC ç›®å½•ç»“æ„
- download_datasets.py ç›¸å…³è¯´æ˜
- å¤šæ•°æ®é›†æ··åˆçš„è®¾è®¡ç†å¿µ

**ä¿ç•™å†…å®¹**:
- ChnSentiCorp æ•°æ®é›†è¯´æ˜
- build_dataset.py ä½¿ç”¨æ–¹æ³•
- ç›®å½•ç»“æ„è¯´æ˜
- æ•°æ®å¤„ç†æµç¨‹

**ç®€åŒ–åçš„ç»“æ„**:
```
dataset/
â”œâ”€â”€ raw_source/
â”‚   â””â”€â”€ ChnSentiCorp/  # åªä¿ç•™è¿™ä¸€ä¸ª
â”œâ”€â”€ processed/
â”œâ”€â”€ build_dataset.py
â”œâ”€â”€ convert_cached_to_jsonl.py
â””â”€â”€ README.md
```

### ğŸ“Š æœ€ç»ˆæ•°æ®é›†çŠ¶æ€

#### ChnSentiCorp âœ…
- **ä½ç½®**: `dataset/raw_source/ChnSentiCorp/`
- **æ–‡ä»¶**: 
  - train.jsonl (9600 æ ·æœ¬)
  - validation.jsonl (1200 æ ·æœ¬)
  - test.jsonl (1200 æ ·æœ¬)
- **æ€»è®¡**: 12000 æ¡æƒ…æ„Ÿåˆ†ææ•°æ®
- **æ ‡ç­¾**: 0ï¼ˆè´Ÿé¢ï¼‰ã€1ï¼ˆæ­£é¢ï¼‰

### ğŸ¯ ç®€åŒ–åçš„å·¥ä½œæµç¨‹

```bash
# 1. å¤„ç†æ•°æ®ï¼ˆå¦‚æœè¿˜æ²¡å¤„ç†ï¼‰
cd dataset
python build_dataset.py

# 2. è®­ç»ƒæ¨¡å‹
cd ..
python main.py --dataset_dir dataset/processed
```

### âœ… æ¸…ç†æ•ˆæœ

#### ä»£ç ç®€åŒ–
- åˆ é™¤äº† 6 ä¸ªä¸éœ€è¦çš„è„šæœ¬æ–‡ä»¶
- build_dataset.py å‡å°‘äº†çº¦ 50 è¡Œä»£ç 
- README.md æ›´åŠ ç®€æ´æ˜äº†

#### é¡¹ç›®ç»“æ„æ¸…æ™°
- åªä¿ç•™å¿…è¦çš„æ–‡ä»¶
- å•ä¸€æ•°æ®æºï¼ˆChnSentiCorpï¼‰
- æµç¨‹æ›´åŠ ç›´è§‚

#### ç»´æŠ¤æ€§æå‡
- å‡å°‘äº†ä»£ç å¤æ‚åº¦
- ç§»é™¤äº†å¤±è´¥çš„ä¸‹è½½é€»è¾‘
- ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½

### ğŸ“ ä¿ç•™çš„å·¥å…·

#### convert_cached_to_jsonl.py âœ…
**ä½œç”¨**: å°† Arrow æ ¼å¼è½¬æ¢ä¸º JSONL
**ä¿ç•™åŸå› **: 
- è¿™æ˜¯å”¯ä¸€æˆåŠŸçš„è½¬æ¢å·¥å…·
- å¯èƒ½åœ¨æœªæ¥éœ€è¦è½¬æ¢å…¶ä»– Arrow æ–‡ä»¶
- ä»£ç ç®€æ´ä¸”åŠŸèƒ½æ˜ç¡®

### ğŸ’¡ æ€»ç»“

æ¸…ç†åçš„é¡¹ç›®ï¼š
- âœ… ç»“æ„æ›´æ¸…æ™°
- âœ… ä»£ç æ›´ç®€æ´
- âœ… æµç¨‹æ›´ç›´è§‚
- âœ… ä¸“æ³¨äº ChnSentiCorp æƒ…æ„Ÿåˆ†æä»»åŠ¡

12000 æ¡é«˜è´¨é‡çš„æƒ…æ„Ÿåˆ†ææ•°æ®è¶³å¤Ÿè®­ç»ƒä¸€ä¸ªä¼˜ç§€çš„æ¨¡å‹ï¼ğŸ‰

## 2026-02-06 æ·»åŠ  Weibo æ•°æ®é›†æ”¯æŒ

### ğŸ“Š Weibo æ•°æ®é›†
- **ä½œç”¨**: æ›¿ä»£ LCCCï¼Œæä¾›ç½‘ç»œè¯­è¨€å’Œå£è¯­åŒ–æ•°æ®
- **ä½ç½®**: `dataset/raw_source/Weibo/`
- **æ–‡ä»¶**: `weibo_senti_100k.csv`
- **æ ·æœ¬æ•°**: çº¦ 10 ä¸‡æ¡å¾®åšæƒ…æ„Ÿæ•°æ®
- **æ ¼å¼**: CSV (label, review)
- **ç‰¹ç‚¹**:
  - çœŸå®çš„ç¤¾äº¤åª’ä½“æ•°æ®
  - åŒ…å«ç½‘ç»œç”¨è¯­å’Œè¡¨æƒ…ç¬¦å·
  - å£è¯­åŒ–è¡¨è¾¾

### ğŸ”§ æ›´æ–°å†…å®¹

#### 1. build_dataset.py âœ…

**æ–°å¢åŠŸèƒ½**:
```python
def load_csv(file_path: str) -> List[Dict]:
    """åŠ è½½ CSV æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        return list(reader)

def process_weibo(raw_file, train_ratio=0.8, val_ratio=0.1):
    """å¤„ç† Weibo æƒ…æ„Ÿåˆ†ææ•°æ®"""
    # åŠ è½½ CSV
    raw_data = load_csv(raw_file)
    
    # è½¬æ¢æ ¼å¼
    processed_data = []
    for item in raw_data:
        text = item.get('review', '').strip()
        label = item.get('label', '0')
        if text:
            processed_data.append({
                'text': text,
                'topic': '',
                'label': str(label)
            })
    
    # åˆ’åˆ†æ•°æ®é›†ï¼ˆ8:1:1ï¼‰
    random.shuffle(processed_data)
    train_end = int(total * 0.8)
    val_end = train_end + int(total * 0.1)
    
    return train_data, dev_data, test_data
```

**æ•°æ®æ··åˆé€»è¾‘**:
```python
# å¤„ç† ChnSentiCorp
train_data, dev_data, test_data = process_chnsenticorp()

# å¤„ç† Weibo
weibo_train, weibo_dev, weibo_test = process_weibo()

# æ··åˆæ•°æ®
train_data.extend(weibo_train)
dev_data.extend(weibo_dev)
test_data.extend(weibo_test)

# æ‰“ä¹±
random.shuffle(train_data)
random.shuffle(dev_data)
random.shuffle(test_data)
```

#### 2. dataset/README.md âœ…

**æ–°å¢å†…å®¹**:
- Weibo æ•°æ®é›†è¯´æ˜
- æ•°æ®æ··åˆç­–ç•¥
- æ›´æ–°ç›®å½•ç»“æ„å›¾
- æ›´æ–°å¤„ç†æµç¨‹è¯´æ˜

### ğŸ“Š æœ€ç»ˆæ•°æ®è§„æ¨¡

#### ChnSentiCorp
- è®­ç»ƒé›†: 9600 æ¡
- éªŒè¯é›†: 1200 æ¡
- æµ‹è¯•é›†: 1200 æ¡
- **æ€»è®¡**: 12000 æ¡

#### Weibo
- æ€»æ ·æœ¬: ~100000 æ¡
- æŒ‰ 8:1:1 åˆ’åˆ†:
  - è®­ç»ƒé›†: ~80000 æ¡
  - éªŒè¯é›†: ~10000 æ¡
  - æµ‹è¯•é›†: ~10000 æ¡

#### æ··åˆåæ€»è®¡
- è®­ç»ƒé›†: ~89600 æ¡
- éªŒè¯é›†: ~11200 æ¡
- æµ‹è¯•é›†: ~11200 æ¡
- **æ€»è®¡**: ~112000 æ¡ ğŸ‰

### ğŸ¯ æ•°æ®é›†ä¼˜åŠ¿

#### ChnSentiCorp
- âœ… é«˜è´¨é‡æ ‡æ³¨
- âœ… è§„èŒƒçš„ä¹¦é¢è¯­
- âœ… æƒ…æ„Ÿè¡¨è¾¾æ¸…æ™°

#### Weibo
- âœ… çœŸå®ç¤¾äº¤åª’ä½“æ•°æ®
- âœ… ç½‘ç»œç”¨è¯­ä¸°å¯Œ
- âœ… å£è¯­åŒ–è¡¨è¾¾
- âœ… åŒ…å«è¡¨æƒ…ç¬¦å·

#### æ··åˆæ•ˆæœ
- âœ… æ•°æ®é‡å¤§å¹…å¢åŠ ï¼ˆ12K â†’ 112Kï¼‰
- âœ… è¦†ç›–ä¹¦é¢è¯­å’Œå£è¯­
- âœ… å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›
- âœ… æ›´è´´è¿‘å®é™…åº”ç”¨åœºæ™¯

### ğŸ’¡ ä½¿ç”¨æ–¹æ³•

```bash
cd dataset
python build_dataset.py
```

**è¾“å‡º**:
- `processed/train.json`: ~89600 æ¡
- `processed/dev.json`: ~11200 æ¡
- `processed/test.json`: ~11200 æ¡

### ğŸ“‹ ä¿®æ”¹æ¸…å•
- [x] æ·»åŠ  `load_csv()` å‡½æ•°
- [x] æ·»åŠ  `process_weibo()` å‡½æ•°
- [x] å®ç°æ•°æ®æ··åˆé€»è¾‘
- [x] æ›´æ–° README.md æ–‡æ¡£
- [x] æ·»åŠ  Weibo æ•°æ®é›†è¯´æ˜

ç°åœ¨æœ‰äº† 11 ä¸‡æ¡é«˜è´¨é‡çš„æƒ…æ„Ÿåˆ†ææ•°æ®ï¼Œæ¨¡å‹è®­ç»ƒæ•ˆæœä¼šæ›´å¥½ï¼ğŸ‰

## 2026-02-06 å‡çº§ä¸ºä¸‰åˆ†ç±» - å¸¦æƒ…æ„Ÿææ€§çš„åè®½æ£€æµ‹

### ğŸ¯ é‡å¤§å‡çº§ï¼šä»äºŒåˆ†ç±»åˆ°ä¸‰åˆ†ç±»

#### é—®é¢˜è¯Šæ–­
**åŸæ–¹æ¡ˆçš„å±€é™æ€§**:
- äºŒåˆ†ç±»åªèƒ½åŒºåˆ†"åè®½"å’Œ"éåè®½"
- å°† ChnSentiCorp çš„å¥½è¯„å’Œå·®è¯„éƒ½æ ‡ä¸º Label 0
- æ¨¡å‹ä¸§å¤±äº†åŒºåˆ†"å¥½è¯„"å’Œ"å·®è¯„"çš„èƒ½åŠ›
- "æˆ‘çˆ±ä½ "å’Œ"æˆ‘æ¨ä½ "éƒ½è¢«è®¤ä¸ºæ˜¯ Label 0ï¼ˆæ­£å¸¸è¯ï¼‰

#### æ–°æ–¹æ¡ˆï¼šä¸‰åˆ†ç±»æ ‡ç­¾ä½“ç³»

**Label 0: æ­£å¸¸-æ­£é¢ (Normal-Positive)**
- çœŸæ­£çš„å¤¸å¥–ã€èµç¾
- æ¥æº: ChnSentiCorp å¥½è¯„ + Weibo æ­£å‘
- æ ·æœ¬æ•°: 65,993 æ¡ (48.22%)

**Label 1: æ­£å¸¸-è´Ÿé¢ (Normal-Negative)**
- çœŸæ­£çš„æ‰¹è¯„ã€æŠ±æ€¨
- æ¥æº: ChnSentiCorp å·®è¯„ + Weibo è´Ÿå‘
- æ ·æœ¬æ•°: 65,995 æ¡ (48.22%)

**Label 2: é˜´é˜³æ€ªæ°” (Sarcastic)**
- åè®½ã€è®½åˆºã€é˜´é˜³æ€ªæ°”
- æ¥æº: ToSarcasm å…¨éƒ¨æ•°æ®
- æ ·æœ¬æ•°: 4,871 æ¡ (3.56%)

### ğŸ“Š æ•°æ®é‡æ–°æ ‡æ³¨ç­–ç•¥

#### ChnSentiCorp
```python
# åŸæ ‡ç­¾ 1 (å¥½è¯„) -> Label 0 (æ­£é¢)
# åŸæ ‡ç­¾ 0 (å·®è¯„) -> Label 1 (è´Ÿé¢)
new_label = 0 if original_label == 1 else 1
```

#### Weibo
```python
# åŸæ ‡ç­¾ 1 (æ­£å‘) -> Label 0 (æ­£é¢)
# åŸæ ‡ç­¾ 0 (è´Ÿå‘) -> Label 1 (è´Ÿé¢)
new_label = 0 if original_label == 1 else 1
```

#### ToSarcasm
```python
# å…¨éƒ¨æ ‡æ³¨ä¸º Label 2 (åè®½)
new_label = 2
```

### ğŸ”§ å®ç°ç»†èŠ‚

#### æ–°è„šæœ¬ï¼šbuild_dataset_3class.py âœ…

**æ ¸å¿ƒåŠŸèƒ½**:
1. `process_chnsenticorp()` - å¤„ç†æƒ…æ„Ÿæ•°æ®ï¼Œé‡æ–°æ ‡æ³¨
2. `process_weibo()` - å¤„ç†å¾®åšæ•°æ®ï¼Œé‡æ–°æ ‡æ³¨
3. `process_tosarcasm()` - å¤„ç†åè®½æ•°æ®ï¼Œæ ‡æ³¨ä¸º Label 2
4. `balance_and_split_data()` - å¹³è¡¡å’Œåˆ’åˆ†æ•°æ®é›†

**æ•°æ®æµç¨‹**:
```
ChnSentiCorp (12K) â†’ é‡æ–°æ ‡æ³¨ â†’ Label 0/1
Weibo (120K)       â†’ é‡æ–°æ ‡æ³¨ â†’ Label 0/1
ToSarcasm (4.9K)   â†’ æ ‡æ³¨     â†’ Label 2
                      â†“
                   åˆå¹¶ (137K)
                      â†“
              å¹³è¡¡å’Œåˆ’åˆ† (8:1:1)
                      â†“
         train/dev/test (ä¸‰åˆ†ç±»)
```

### ğŸ“ˆ æœ€ç»ˆæ•°æ®è§„æ¨¡

#### æ€»è®¡
- **æ€»æ ·æœ¬æ•°**: 136,859 æ¡
- **è®­ç»ƒé›†**: 109,486 æ¡
- **éªŒè¯é›†**: 13,685 æ¡
- **æµ‹è¯•é›†**: 13,688 æ¡

#### æ ‡ç­¾åˆ†å¸ƒ
```
è®­ç»ƒé›†:
  Label 0 (æ­£é¢): 52,794 (48.22%)
  Label 1 (è´Ÿé¢): 52,796 (48.22%)
  Label 2 (åè®½): 3,896 (3.56%)

éªŒè¯é›†:
  Label 0 (æ­£é¢): 6,599 (48.22%)
  Label 1 (è´Ÿé¢): 6,599 (48.22%)
  Label 2 (åè®½): 487 (3.56%)

æµ‹è¯•é›†:
  Label 0 (æ­£é¢): 6,600 (48.22%)
  Label 1 (è´Ÿé¢): 6,600 (48.22%)
  Label 2 (åè®½): 488 (3.57%)
```

### ğŸ¯ ä¼˜åŠ¿åˆ†æ

#### 1. æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
- âœ… ä¸ä»…è¯†åˆ«"é˜´é˜³æ€ªæ°”"
- âœ… è¿˜èƒ½åˆ¤æ–­æƒ…æ„Ÿææ€§ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
- âœ… ä¸‰ç»´æƒ…æ„Ÿç©ºé—´

#### 2. "å¼ºæ’‘"è¯†åˆ«
å½“æ¨¡å‹è¾“å‡ºæ»¡è¶³ï¼š
- P(Label 0) > 0.4 ä¸” P(Label 2) > 0.2
- æˆ–ç‰¹å¾åœ¨ Label 0 å’Œ Label 2 ä¹‹é—´

å¯èƒ½æ˜¯"å¼ºæ’‘"ã€"è‹¦ç¬‘"ç­‰å¤æ‚æƒ…æ„Ÿ

#### 3. å­¦æœ¯ä»·å€¼
- âœ… å¯ä»¥å‘è¡¨è®ºæ–‡ï¼ˆå¸¦æƒ…æ„Ÿææ€§çš„åè®½æ£€æµ‹ï¼‰
- âœ… æœ‰å®é™…åº”ç”¨ä»·å€¼
- âœ… å¯ä»¥åšæ¶ˆèå®éªŒ

#### 4. å®é™…åº”ç”¨
- èˆ†æƒ…åˆ†æï¼šåŒºåˆ†çœŸå¤¸/çœŸéª‚/é˜´é˜³æ€ªæ°”
- å®¢æœç³»ç»Ÿï¼šè¯†åˆ«ç”¨æˆ·çœŸå®æƒ…æ„Ÿ
- ç¤¾äº¤åª’ä½“ï¼šæ£€æµ‹è®½åˆºå’Œæƒ…æ„Ÿå€¾å‘

### âš ï¸ ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

**ç°çŠ¶**: åè®½æ•°æ®åªå  3.56%

**è§£å†³æ–¹æ¡ˆ**:
1. **ç±»åˆ«æƒé‡**:
   ```python
   class_weights = torch.tensor([1.0, 1.0, 13.5])  # åè®½æƒé‡æé«˜
   criterion = nn.CrossEntropyLoss(weight=class_weights)
   ```

2. **Focal Loss**:
   ```python
   # å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬
   focal_loss = FocalLoss(alpha=0.25, gamma=2.0)
   ```

3. **è¿‡é‡‡æ ·**:
   ```python
   # å¯¹åè®½æ•°æ®è¿›è¡Œé‡å¤é‡‡æ ·
   ```

4. **è¯„ä¼°æŒ‡æ ‡**:
   - ä¸ä»…çœ‹ Accuracy
   - é‡ç‚¹å…³æ³¨ F1-Scoreï¼ˆå°¤å…¶æ˜¯ Label 2ï¼‰
   - ä½¿ç”¨æ··æ·†çŸ©é˜µåˆ†æ

### ğŸ”§ æ¨¡å‹ä¿®æ”¹è¦æ±‚

#### å¿…é¡»ä¿®æ”¹çš„åœ°æ–¹

**1. main.py - å‘½ä»¤è¡Œå‚æ•°**:
```python
parser.add_argument('--num_classes', type=int, default=3, help='åˆ†ç±»ç±»åˆ«æ•°')
```

**2. model.py - è¾“å‡ºå±‚**:
```python
self.classifier = nn.Linear(hidden_size, num_classes=3)
```

**3. è®­ç»ƒè„šæœ¬**:
```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

### ğŸ“‹ æ–‡ä»¶æ¸…å•

#### æ–°å¢æ–‡ä»¶
- [x] `dataset/build_dataset_3class.py` - ä¸‰åˆ†ç±»æ•°æ®æ¸…æ´—è„šæœ¬ âœ…
- [x] `dataset/README.md` - æ›´æ–°ä¸ºä¸‰åˆ†ç±»è¯´æ˜ âœ…

#### æ•°æ®æ–‡ä»¶
- [x] `dataset/processed/train.json` - 109,486 æ¡ï¼ˆä¸‰åˆ†ç±»ï¼‰âœ…
- [x] `dataset/processed/dev.json` - 13,685 æ¡ï¼ˆä¸‰åˆ†ç±»ï¼‰âœ…
- [x] `dataset/processed/test.json` - 13,688 æ¡ï¼ˆä¸‰åˆ†ç±»ï¼‰âœ…

### ğŸ’¡ ä¸‹ä¸€æ­¥

1. **ä¿®æ”¹æ¨¡å‹**:
   ```bash
   # ä¿®æ”¹ main.py å’Œ model.py çš„ num_classes ä¸º 3
   ```

2. **è®­ç»ƒæ¨¡å‹**:
   ```bash
   python main.py --dataset_dir dataset/processed --num_classes 3
   ```

3. **è¯„ä¼°æŒ‡æ ‡**:
   - Accuracy
   - F1-Score (Macro/Weighted)
   - æ¯ä¸ªç±»åˆ«çš„ Precision/Recall
   - æ··æ·†çŸ©é˜µ

4. **ç±»åˆ«ä¸å¹³è¡¡å¤„ç†**:
   - æ·»åŠ ç±»åˆ«æƒé‡
   - æˆ–ä½¿ç”¨ Focal Loss

### ğŸ‰ æ€»ç»“

ä»äºŒåˆ†ç±»å‡çº§åˆ°ä¸‰åˆ†ç±»ï¼š
- âœ… æ•°æ®è§„æ¨¡: 13.7 ä¸‡æ¡
- âœ… æ ‡ç­¾ä½“ç³»: æ­£é¢/è´Ÿé¢/åè®½
- âœ… æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
- âœ… å¯ä»¥è¯†åˆ«"å¼ºæ’‘"
- âœ… æœ‰å­¦æœ¯å’Œåº”ç”¨ä»·å€¼

è¿™æ˜¯ä¸€ä¸ªæ›´ç§‘å­¦ã€æ›´å®ç”¨çš„æ–¹æ¡ˆï¼ğŸ“

## 2026-02-06 ä¸‰åˆ†ç±»æ•°æ®å‡†å¤‡å®Œæˆ - æ€»ç»“

### âœ… å®Œæˆçš„å·¥ä½œ

#### 1. æ•°æ®æ¸…æ´—è„šæœ¬ âœ…
- **æ–‡ä»¶**: `dataset/build_dataset_3class.py`
- **åŠŸèƒ½**: 
  - å¤„ç†ä¸‰ä¸ªæ•°æ®æºï¼ˆChnSentiCorp, Weibo, ToSarcasmï¼‰
  - é‡æ–°æ ‡æ³¨ä¸ºä¸‰åˆ†ç±»
  - å¹³è¡¡å’Œåˆ’åˆ†æ•°æ®é›†
  - è¾“å‡ºç»Ÿä¸€æ ¼å¼

#### 2. æ•°æ®éªŒè¯å·¥å…· âœ…
- **æ–‡ä»¶**: `dataset/verify_data.py`
- **åŠŸèƒ½**: éªŒè¯æ•°æ®æ ¼å¼ã€æ ‡ç­¾åˆ†å¸ƒã€æ–‡æœ¬é•¿åº¦

#### 3. è¿ç§»æŒ‡å— âœ…
- **æ–‡ä»¶**: `dataset/MIGRATION_GUIDE.md`
- **å†…å®¹**: 
  - ä»£ç ä¿®æ”¹æŒ‡å—
  - ç±»åˆ«ä¸å¹³è¡¡å¤„ç†
  - è¯„ä¼°æŒ‡æ ‡æ›´æ–°
  - "å¼ºæ’‘"è¯†åˆ«å®ç°
  - å®Œæ•´è®­ç»ƒæµç¨‹

#### 4. æ–‡æ¡£æ›´æ–° âœ…
- **æ–‡ä»¶**: `dataset/README.md`
- **å†…å®¹**: ä¸‰åˆ†ç±»æ ‡ç­¾ä½“ç³»ã€æ•°æ®é›†è¯¦æƒ…ã€ä½¿ç”¨æµç¨‹

### ğŸ“Š æœ€ç»ˆæ•°æ®ç»Ÿè®¡

```
æ€»æ ·æœ¬æ•°: 136,859 æ¡

è®­ç»ƒé›†: 109,486 æ¡
  - Label 0 (æ­£é¢): 52,794 (48.22%)
  - Label 1 (è´Ÿé¢): 52,796 (48.22%)
  - Label 2 (åè®½): 3,896 (3.56%)

éªŒè¯é›†: 13,685 æ¡
  - Label 0 (æ­£é¢): 6,599 (48.22%)
  - Label 1 (è´Ÿé¢): 6,599 (48.22%)
  - Label 2 (åè®½): 487 (3.56%)

æµ‹è¯•é›†: 13,688 æ¡
  - Label 0 (æ­£é¢): 6,600 (48.22%)
  - Label 1 (è´Ÿé¢): 6,600 (48.22%)
  - Label 2 (åè®½): 488 (3.57%)
```

### ğŸ“ æœ€ç»ˆç›®å½•ç»“æ„

```
dataset/
â”œâ”€â”€ raw_source/              # åŸå§‹æ•°æ®ï¼ˆä¸ä¿®æ”¹ï¼‰
â”‚   â”œâ”€â”€ ToSarcasm/          # 4,871 æ¡åè®½æ•°æ®
â”‚   â”œâ”€â”€ ChnSentiCorp/       # 12,000 æ¡æƒ…æ„Ÿæ•°æ®
â”‚   â””â”€â”€ Weibo/              # 119,988 æ¡å¾®åšæ•°æ®
â”‚
â”œâ”€â”€ processed/              # æ¸…æ´—åçš„æ•°æ®ï¼ˆæ¨¡å‹è¯»å–ï¼‰
â”‚   â”œâ”€â”€ train.json         # 109,486 æ¡
â”‚   â”œâ”€â”€ dev.json           # 13,685 æ¡
â”‚   â””â”€â”€ test.json          # 13,688 æ¡
â”‚
â”œâ”€â”€ build_dataset_3class.py # ä¸‰åˆ†ç±»æ•°æ®æ¸…æ´—è„šæœ¬ âœ…
â”œâ”€â”€ verify_data.py          # æ•°æ®éªŒè¯å·¥å…· âœ…
â”œâ”€â”€ MIGRATION_GUIDE.md      # è¿ç§»æŒ‡å— âœ…
â”œâ”€â”€ README.md              # é¡¹ç›®æ–‡æ¡£ âœ…
â””â”€â”€ convert_cached_to_jsonl.py  # æ ¼å¼è½¬æ¢å·¥å…·
```

### ğŸ¯ ä¸‰åˆ†ç±»æ ‡ç­¾ä½“ç³»

| Label | åç§° | æ¥æº | æ ·æœ¬æ•° | å æ¯” |
|-------|------|------|--------|------|
| 0 | æ­£å¸¸-æ­£é¢ | ChnSentiCorpå¥½è¯„ + Weiboæ­£å‘ | 65,993 | 48.22% |
| 1 | æ­£å¸¸-è´Ÿé¢ | ChnSentiCorpå·®è¯„ + Weiboè´Ÿå‘ | 65,995 | 48.22% |
| 2 | é˜´é˜³æ€ªæ°” | ToSarcasmå…¨éƒ¨ | 4,871 | 3.56% |

### ğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ

#### 1. ä¿®æ”¹æ¨¡å‹ä»£ç 
```python
# main.py
parser.add_argument('--num_classes', type=int, default=3, ...)

# model.py
def __init__(self, ..., num_classes=3, ...):
```

#### 2. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
```python
# æ–¹æ³•1: ç±»åˆ«æƒé‡
class_weights = torch.tensor([1.0, 1.0, 13.5]).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)

# æ–¹æ³•2: Focal Loss
criterion = FocalLoss(alpha=0.25, gamma=2.0)
```

#### 3. è®­ç»ƒæ¨¡å‹
```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

#### 4. è¯„ä¼°æŒ‡æ ‡
- Accuracy
- F1-Score (Macro/Weighted)
- æ¯ä¸ªç±»åˆ«çš„ Precision/Recall/F1
- æ··æ·†çŸ©é˜µ
- AUC (One-vs-Rest)

### ğŸ“ å­¦æœ¯ä»·å€¼

#### åˆ›æ–°ç‚¹
1. âœ… å¸¦æƒ…æ„Ÿææ€§çš„åè®½æ£€æµ‹ï¼ˆä¸‰åˆ†ç±»ï¼‰
2. âœ… å¤æ‚æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¼ºæ’‘/è‹¦ç¬‘ï¼‰
3. âœ… å¤šæ•°æ®æºèåˆç­–ç•¥
4. âœ… ç±»åˆ«ä¸å¹³è¡¡å¤„ç†æ–¹æ¡ˆ

#### å¯ä»¥åšçš„å®éªŒ
1. äºŒåˆ†ç±» vs ä¸‰åˆ†ç±»å¯¹æ¯”
2. ä¸åŒæ•°æ®æºçš„æ¶ˆèå®éªŒ
3. ç±»åˆ«æƒé‡çš„å½±å“åˆ†æ
4. "å¼ºæ’‘"æ£€æµ‹çš„å‡†ç¡®ç‡

#### è®ºæ–‡ç« èŠ‚
- é—®é¢˜å®šä¹‰ï¼šå¸¦æƒ…æ„Ÿææ€§çš„åè®½æ£€æµ‹
- æ•°æ®é›†æ„å»ºï¼šä¸‰åˆ†ç±»æ ‡æ³¨ç­–ç•¥
- æ¨¡å‹è®¾è®¡ï¼šBERT + HGNN + Attention
- å®éªŒç»“æœï¼šä¸‰åˆ†ç±»æ€§èƒ½ + æ¶ˆèå®éªŒ
- åº”ç”¨åœºæ™¯ï¼šèˆ†æƒ…åˆ†æã€å®¢æœç³»ç»Ÿ

### ğŸ‰ æ€»ç»“

ä»äºŒåˆ†ç±»å‡çº§åˆ°ä¸‰åˆ†ç±»ï¼š
- âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼ˆ13.7ä¸‡æ¡ï¼‰
- âœ… æ ‡ç­¾ä½“ç³»ç§‘å­¦ï¼ˆæ­£é¢/è´Ÿé¢/åè®½ï¼‰
- âœ… æ–‡æ¡£å®Œå–„ï¼ˆREADME + è¿ç§»æŒ‡å—ï¼‰
- âœ… å·¥å…·é½å…¨ï¼ˆæ¸…æ´— + éªŒè¯ï¼‰
- âœ… å­¦æœ¯ä»·å€¼é«˜ï¼ˆå¯å‘è®ºæ–‡ï¼‰
- âœ… å®ç”¨æ€§å¼ºï¼ˆçœŸå®åœºæ™¯ï¼‰

ç°åœ¨å¯ä»¥å¼€å§‹ä¿®æ”¹æ¨¡å‹ä»£ç å¹¶è®­ç»ƒäº†ï¼ğŸš€

**é‡è¦æé†’**:
1. ä¿®æ”¹ `num_classes=3`
2. æ·»åŠ ç±»åˆ«æƒé‡å¤„ç†
3. å…³æ³¨ F1-Score è€Œéä»…çœ‹ Accuracy
4. å‡†å¤‡æ¶ˆèå®éªŒ

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ“

## 2026-02-06 æ¨¡å‹é€‚é…ä¸‰åˆ†ç±» - ä»£ç ä¿®æ”¹å®Œæˆ

### âœ… å®Œæˆçš„ä»£ç ä¿®æ”¹

#### 1. main.py - æ ¸å¿ƒä¿®æ”¹ âœ…

**ä¿®æ”¹ 1: num_classes é»˜è®¤å€¼**
```python
# ç¬¬ 48 è¡Œ
# ä¿®æ”¹å‰
parser.add_argument('--num_classes', type=int, default=2, help='åˆ†ç±»ç±»åˆ«æ•°')

# ä¿®æ”¹å
parser.add_argument('--num_classes', type=int, default=3, help='åˆ†ç±»ç±»åˆ«æ•°')
```

**ä¿®æ”¹ 2: æ·»åŠ åŠ æƒæŸå¤±å‡½æ•°**
```python
# ç¬¬ 249-256 è¡Œ
# ä¿®æ”¹å‰
criterion = nn.CrossEntropyLoss()

# ä¿®æ”¹å
# ä¸‰åˆ†ç±»åŠ æƒæŸå¤±ï¼šå¤„ç†åè®½æ•°æ®ä¸å¹³è¡¡ï¼ˆåè®½åªå 3.56%ï¼‰
# Label 0 (æ­£é¢): æƒé‡ 1.0
# Label 1 (è´Ÿé¢): æƒé‡ 1.0
# Label 2 (åè®½): æƒé‡ 15.0 (ç®—é”™ä¸€ä¸ªåè®½ï¼Œæƒ©ç½šåŠ›åº¦æ˜¯æ­£å¸¸çš„15å€)
class_weights = torch.tensor([1.0, 1.0, 15.0]).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
logger.info(f"ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights.tolist()}")
```

**ä¿®æ”¹ 3: æ·»åŠ ä¸‰åˆ†ç±»æ ‡ç­¾åç§°**
```python
# åœ¨æ‰€æœ‰è°ƒç”¨ calculate_metrics çš„åœ°æ–¹æ·»åŠ 
label_names = ['æ­£é¢', 'è´Ÿé¢', 'åè®½']
metrics = MetricsCalculator.calculate_metrics(true_labels, predictions, val_probs, labels=label_names)
```

**ä¿®æ”¹ä½ç½®**:
- ç¬¬ 303-306 è¡Œï¼šè¯„ä¼°æ¨¡å¼
- ç¬¬ 395-399 è¡Œï¼šæœ€ç»ˆè¯„ä¼°
- ç¬¬ 438-442 è¡Œï¼šæµ‹è¯•é›†è¯„ä¼°

**ä¿®æ”¹ 4: æ›´æ–°åˆ†ç±»æŠ¥å‘Š**
```python
# åœ¨æ‰€æœ‰è°ƒç”¨ print_classification_report çš„åœ°æ–¹æ·»åŠ 
label_names = ['æ­£é¢', 'è´Ÿé¢', 'åè®½']
MetricsCalculator.print_classification_report(true_labels, predictions, labels=label_names)
```

**ä¿®æ”¹ä½ç½®**:
- ç¬¬ 310-312 è¡Œï¼šè¯„ä¼°æ¨¡å¼
- ç¬¬ 405-407 è¡Œï¼šæœ€ç»ˆè¯„ä¼°

#### 2. utils.py - æŒ‡æ ‡è®¡ç®—ä¿®æ”¹ âœ…

**ä¿®æ”¹ 1: f1_score çš„ average å‚æ•°**
```python
# ä¿®æ”¹å‰
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')

# ä¿®æ”¹å
precision, recall, f1, _ = precision_recall_fscore_support(
    y_true, y_pred, average='weighted', zero_division=0
)
```

**ä¿®æ”¹ 2: æ·»åŠ  macro å¹³å‡æŒ‡æ ‡**
```python
# æ–°å¢ï¼šmacro å¹³å‡ï¼ˆå¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»åˆ«ï¼‰
precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
    y_true, y_pred, average='macro', zero_division=0
)

metrics = {
    'accuracy': accuracy,
    'precision': precision,
    'recall': recall,
    'f1_score': f1,
    'f1_macro': f1_macro,  # æ–°å¢
    'precision_macro': precision_macro,
    'recall_macro': recall_macro
}
```

**ä¿®æ”¹ 3: å¤šåˆ†ç±» AUC è®¡ç®—**
```python
# ä¿®æ”¹å‰ï¼šåªæ”¯æŒäºŒåˆ†ç±»
if y_probs is not None:
    auc = roc_auc_score(y_true, y_probs)
    metrics['auc'] = auc

# ä¿®æ”¹åï¼šæ”¯æŒäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»
if y_probs is not None:
    unique_labels = len(set(y_true))
    
    if unique_labels == 2:
        # äºŒåˆ†ç±»ï¼šy_probs æ˜¯æ­£ç±»æ¦‚ç‡
        auc = roc_auc_score(y_true, y_probs)
        metrics['auc'] = auc
    elif unique_labels > 2:
        # å¤šåˆ†ç±»ï¼šä½¿ç”¨ One-vs-Rest ç­–ç•¥
        auc = roc_auc_score(y_true, y_probs, multi_class='ovr', average='weighted')
        metrics['auc'] = auc
```

**ä¿®æ”¹ 4: æ·»åŠ  zero_division å‚æ•°**
```python
# åœ¨æ‰€æœ‰ precision_recall_fscore_support è°ƒç”¨ä¸­æ·»åŠ 
precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(
    y_true, y_pred, average=None, zero_division=0
)
```

#### 3. data_preprocess.py - æ£€æŸ¥é€šè¿‡ âœ…

**SarcasmDataset ç±»**:
- âœ… æ²¡æœ‰ç¡¬ç¼–ç  label å¤„ç†
- âœ… ä½¿ç”¨ `torch.tensor(label, dtype=torch.long)`
- âœ… æ”¯æŒä»»æ„æ•´æ•°æ ‡ç­¾ï¼ˆ0, 1, 2, ...ï¼‰
- âœ… æ— éœ€ä¿®æ”¹

### ğŸ“Š ä¿®æ”¹æ€»ç»“

| æ–‡ä»¶ | ä¿®æ”¹é¡¹ | çŠ¶æ€ |
|------|--------|------|
| main.py | num_classes é»˜è®¤å€¼ | âœ… |
| main.py | åŠ æƒæŸå¤±å‡½æ•° | âœ… |
| main.py | ä¸‰åˆ†ç±»æ ‡ç­¾åç§° | âœ… |
| utils.py | f1_score average å‚æ•° | âœ… |
| utils.py | macro å¹³å‡æŒ‡æ ‡ | âœ… |
| utils.py | å¤šåˆ†ç±» AUC | âœ… |
| data_preprocess.py | æ£€æŸ¥ label å¤„ç† | âœ… |

### ğŸ¯ å…³é”®æ”¹è¿›

#### 1. åŠ æƒæŸå¤±å‡½æ•°ï¼ˆé‡è¦ï¼ï¼‰
```python
class_weights = torch.tensor([1.0, 1.0, 15.0])
```

**æ•ˆæœ**:
- ç®—é”™ä¸€ä¸ªåè®½ï¼Œæƒ©ç½šåŠ›åº¦æ˜¯æ­£å¸¸çš„ 15 å€
- å¼ºåˆ¶æ¨¡å‹é‡è§†åè®½ç±»åˆ«ï¼ˆåªå  3.56%ï¼‰
- é¿å…æ¨¡å‹å¿½ç•¥å°‘æ•°ç±»

#### 2. Macro F1-Scoreï¼ˆé‡è¦ï¼ï¼‰
```python
metrics['f1_macro'] = f1_macro
```

**æ•ˆæœ**:
- å¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»åˆ«
- ä¸å—æ ·æœ¬æ•°å½±å“
- æ›´èƒ½åæ˜ æ¨¡å‹å¯¹å°‘æ•°ç±»çš„è¯†åˆ«èƒ½åŠ›

#### 3. å¤šåˆ†ç±» AUC
```python
auc = roc_auc_score(y_true, y_probs, multi_class='ovr', average='weighted')
```

**æ•ˆæœ**:
- ä½¿ç”¨ One-vs-Rest ç­–ç•¥
- æ¯ä¸ªç±»åˆ« vs å…¶ä»–ç±»åˆ«
- ç»¼åˆè¯„ä¼°åˆ†ç±»èƒ½åŠ›

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡è¯´æ˜

#### Weighted æŒ‡æ ‡ï¼ˆæŒ‰æ ·æœ¬æ•°åŠ æƒï¼‰
- `f1_score`: Weighted F1
- `precision`: Weighted Precision
- `recall`: Weighted Recall

**ç‰¹ç‚¹**: å—æ ·æœ¬æ•°å½±å“ï¼Œæ­£é¢/è´Ÿé¢æƒé‡å¤§

#### Macro æŒ‡æ ‡ï¼ˆå¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»åˆ«ï¼‰
- `f1_macro`: Macro F1
- `precision_macro`: Macro Precision
- `recall_macro`: Macro Recall

**ç‰¹ç‚¹**: ä¸å—æ ·æœ¬æ•°å½±å“ï¼Œæ›´èƒ½åæ˜ å¯¹åè®½çš„è¯†åˆ«

#### æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
- `æ­£é¢_precision`, `æ­£é¢_recall`, `æ­£é¢_f1`
- `è´Ÿé¢_precision`, `è´Ÿé¢_recall`, `è´Ÿé¢_f1`
- `åè®½_precision`, `åè®½_recall`, `åè®½_f1`

**ç‰¹ç‚¹**: è¯¦ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„æ€§èƒ½

### ğŸ§ª æµ‹è¯•å»ºè®®

#### 1. å¿«é€Ÿæµ‹è¯•
```bash
# ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•ï¼ˆç¡®ä¿ä»£ç èƒ½è¿è¡Œï¼‰
python main.py --dataset_dir dataset/processed --num_classes 3 --epochs 2
```

#### 2. å®Œæ•´è®­ç»ƒ
```bash
# æ­£å¼è®­ç»ƒ
python main.py --dataset_dir dataset/processed --num_classes 3 --batch_size 16 --epochs 50
```

#### 3. è§‚å¯ŸæŒ‡æ ‡
é‡ç‚¹å…³æ³¨ï¼š
- **Accuracy**: æ•´ä½“å‡†ç¡®ç‡
- **F1 Macro**: å¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»åˆ«çš„ F1
- **åè®½_f1**: åè®½ç±»åˆ«çš„ F1ï¼ˆæœ€é‡è¦ï¼ï¼‰
- **æ··æ·†çŸ©é˜µ**: çœ‹å“ªäº›ç±»åˆ«å®¹æ˜“æ··æ·†

### âš ï¸ é¢„æœŸé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜ 1: åè®½ F1 å¾ˆä½ï¼ˆ< 0.5ï¼‰
**åŸå› **: æƒé‡ä¸å¤Ÿæˆ–æ•°æ®å¤ªå°‘
**è§£å†³**: å¢åŠ æƒé‡åˆ° 20.0 æˆ– 25.0

#### é—®é¢˜ 2: Accuracy é«˜ä½† F1 Macro ä½
**åŸå› **: æ¨¡å‹åå‘æ­£é¢/è´Ÿé¢ï¼Œå¿½ç•¥åè®½
**è§£å†³**: å¢åŠ åè®½æƒé‡

#### é—®é¢˜ 3: è®­ç»ƒä¸æ”¶æ•›
**åŸå› **: å­¦ä¹ ç‡å¤ªå¤§æˆ–æƒé‡å¤ªå¤§
**è§£å†³**: é™ä½å­¦ä¹ ç‡æˆ–è°ƒæ•´æƒé‡

### ğŸ“‹ æ£€æŸ¥æ¸…å•

- [x] main.py: num_classes=3
- [x] main.py: åŠ æƒæŸå¤±å‡½æ•°
- [x] main.py: ä¸‰åˆ†ç±»æ ‡ç­¾åç§°
- [x] utils.py: f1_score average='weighted'
- [x] utils.py: æ·»åŠ  macro æŒ‡æ ‡
- [x] utils.py: å¤šåˆ†ç±» AUC
- [x] data_preprocess.py: æ£€æŸ¥é€šè¿‡

### ğŸ‰ å‡†å¤‡å°±ç»ªï¼

æ‰€æœ‰ä»£ç ä¿®æ”¹å®Œæˆï¼Œç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒä¸‰åˆ†ç±»æ¨¡å‹äº†ï¼

**è®­ç»ƒå‘½ä»¤**:
```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

**é¢„æœŸè¾“å‡º**:
```
ä½¿ç”¨ç±»åˆ«æƒé‡: [1.0, 1.0, 15.0]
...
accuracy: 0.XXXX
f1_score: 0.XXXX (weighted)
f1_macro: 0.XXXX (macro)
æ­£é¢_f1: 0.XXXX
è´Ÿé¢_f1: 0.XXXX
åè®½_f1: 0.XXXX  # é‡ç‚¹å…³æ³¨è¿™ä¸ªï¼
```

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

## 2026-02-06 ä¸‰åˆ†ç±»æ¨¡å‹å‡†å¤‡å®Œæˆ - æœ€ç»ˆæ€»ç»“

### ğŸ‰ æ‰€æœ‰å·¥ä½œå®Œæˆ

#### âœ… æ£€æŸ¥ç»“æœ
è¿è¡Œ `python check_3class_ready.py` éªŒè¯ï¼š
- âœ… main.py: æ‰€æœ‰ä¿®æ”¹æ­£ç¡®
- âœ… utils.py: æ‰€æœ‰ä¿®æ”¹æ­£ç¡®
- âœ… æ•°æ®æ–‡ä»¶: åŒ…å«ä¸‰ä¸ªç±»åˆ« {0, 1, 2}

### ğŸ“Š æœ€ç»ˆé…ç½®

#### æ•°æ®è§„æ¨¡
```
æ€»è®¡: 136,859 æ¡

è®­ç»ƒé›†: 109,486 æ¡
  - Label 0 (æ­£é¢): 52,794 (48.22%)
  - Label 1 (è´Ÿé¢): 52,796 (48.22%)
  - Label 2 (åè®½): 3,896 (3.56%)

éªŒè¯é›†: 13,685 æ¡
æµ‹è¯•é›†: 13,688 æ¡
```

#### æ¨¡å‹é…ç½®
```python
# num_classes
default=3

# åŠ æƒæŸå¤±å‡½æ•°
class_weights = torch.tensor([1.0, 1.0, 15.0])
criterion = nn.CrossEntropyLoss(weight=class_weights)

# ä¸‰åˆ†ç±»æ ‡ç­¾
label_names = ['æ­£é¢', 'è´Ÿé¢', 'åè®½']
```

#### è¯„ä¼°æŒ‡æ ‡
- Accuracy
- F1-Score (Weighted)
- F1-Score (Macro) â† é‡ç‚¹
- æ¯ä¸ªç±»åˆ«çš„ P/R/F1
- AUC (One-vs-Rest)

### ğŸ¯ å…³é”®æŠ€æœ¯ç‚¹

#### 1. åŠ æƒæŸå¤±å‡½æ•°ï¼ˆæ ¸å¿ƒï¼ï¼‰
```python
class_weights = [1.0, 1.0, 15.0]
```
- ç®—é”™ä¸€ä¸ªåè®½ = ç®—é”™ 15 ä¸ªæ­£å¸¸
- å¼ºåˆ¶æ¨¡å‹é‡è§†åè®½ï¼ˆåªå  3.56%ï¼‰

#### 2. Macro F1-Scoreï¼ˆé‡è¦ï¼ï¼‰
```python
f1_macro = f1_score(y_true, y_pred, average='macro')
```
- å¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»åˆ«
- ä¸å—æ ·æœ¬æ•°å½±å“
- æ›´èƒ½åæ˜ å¯¹åè®½çš„è¯†åˆ«

#### 3. å¤šåˆ†ç±» AUC
```python
auc = roc_auc_score(y_true, y_probs, multi_class='ovr')
```
- One-vs-Rest ç­–ç•¥
- ç»¼åˆè¯„ä¼°åˆ†ç±»èƒ½åŠ›

### ğŸ“ é¡¹ç›®æ–‡ä»¶æ¸…å•

#### æ ¸å¿ƒä»£ç 
- âœ… `main.py` - è®­ç»ƒè„šæœ¬ï¼ˆå·²ä¿®æ”¹ï¼‰
- âœ… `utils.py` - å·¥å…·å‡½æ•°ï¼ˆå·²ä¿®æ”¹ï¼‰
- âœ… `model.py` - æ¨¡å‹å®šä¹‰ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
- âœ… `data_preprocess.py` - æ•°æ®é¢„å¤„ç†ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰

#### æ•°æ®æ–‡ä»¶
- âœ… `dataset/processed/train.json` (109,486 æ¡)
- âœ… `dataset/processed/dev.json` (13,685 æ¡)
- âœ… `dataset/processed/test.json` (13,688 æ¡)

#### æ•°æ®è„šæœ¬
- âœ… `dataset/build_dataset_3class.py` - ä¸‰åˆ†ç±»æ•°æ®æ¸…æ´—
- âœ… `dataset/verify_data.py` - æ•°æ®éªŒè¯å·¥å…·

#### æ–‡æ¡£
- âœ… `dataset/README.md` - æ•°æ®é›†è¯´æ˜
- âœ… `dataset/MIGRATION_GUIDE.md` - è¿ç§»æŒ‡å—
- âœ… `QUICKSTART_3CLASS.md` - å¿«é€Ÿå¼€å§‹
- âœ… `ä¸‰åˆ†ç±»æ¨¡å‹å‡†å¤‡å®Œæˆ.md` - å®Œæˆæ€»ç»“
- âœ… `process.txt` - å®Œæ•´å¼€å‘æ—¥å¿—

#### å·¥å…·
- âœ… `check_3class_ready.py` - å‡†å¤‡æ£€æŸ¥è„šæœ¬

### ğŸš€ è®­ç»ƒå‘½ä»¤

#### åŸºç¡€è®­ç»ƒ
```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

#### å®Œæ•´å‚æ•°
```bash
python main.py \
    --dataset_dir dataset/processed \
    --num_classes 3 \
    --batch_size 16 \
    --epochs 50 \
    --learning_rate 2e-5 \
    --patience 10
```

#### å¿«é€Ÿæµ‹è¯•
```bash
python main.py --dataset_dir dataset/processed --num_classes 3 --epochs 2
```

### ğŸ“ˆ é¢„æœŸæ•ˆæœ

#### å¥½çš„æ¨¡å‹åº”è¯¥è¾¾åˆ°
- **Accuracy**: > 85%
- **F1 Macro**: > 0.75
- **åè®½ F1**: > 0.60

#### å¦‚æœæ•ˆæœä¸å¥½
1. å¢åŠ åè®½æƒé‡ï¼ˆ20.0 æˆ– 25.0ï¼‰
2. ä½¿ç”¨ Focal Loss
3. å¢åŠ è®­ç»ƒè½®æ•°
4. è°ƒæ•´å­¦ä¹ ç‡

### ğŸ“ å­¦æœ¯ä»·å€¼

#### åˆ›æ–°ç‚¹
1. å¸¦æƒ…æ„Ÿææ€§çš„åè®½æ£€æµ‹ï¼ˆä¸‰åˆ†ç±»ï¼‰
2. å¤æ‚æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¼ºæ’‘/è‹¦ç¬‘ï¼‰
3. å¤šæ•°æ®æºèåˆç­–ç•¥
4. ç±»åˆ«ä¸å¹³è¡¡å¤„ç†æ–¹æ¡ˆ

#### å¯ä»¥åšçš„å®éªŒ
1. äºŒåˆ†ç±» vs ä¸‰åˆ†ç±»å¯¹æ¯”
2. ä¸åŒæƒé‡çš„æ¶ˆèå®éªŒ
3. ä¸åŒæ•°æ®æºçš„è´¡çŒ®åˆ†æ
4. "å¼ºæ’‘"æ£€æµ‹çš„å‡†ç¡®ç‡

#### è®ºæ–‡ä»·å€¼
- âœ… é—®é¢˜å®šä¹‰æ–°é¢–
- âœ… æ•°æ®é›†æ„å»ºç§‘å­¦
- âœ… æŠ€æœ¯æ–¹æ¡ˆå®Œæ•´
- âœ… å®éªŒè®¾è®¡åˆç†
- âœ… åº”ç”¨åœºæ™¯å¹¿æ³›

### ğŸ“‹ å®Œæ•´å·¥ä½œæ¸…å•

#### æ•°æ®å‡†å¤‡
- [x] æ”¶é›†ä¸‰ä¸ªæ•°æ®æºï¼ˆChnSentiCorp, Weibo, ToSarcasmï¼‰
- [x] é‡æ–°æ ‡æ³¨ä¸ºä¸‰åˆ†ç±»
- [x] æ•°æ®æ¸…æ´—å’Œæ ¼å¼ç»Ÿä¸€
- [x] åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
- [x] æ•°æ®éªŒè¯

#### ä»£ç ä¿®æ”¹
- [x] main.py: num_classes=3
- [x] main.py: åŠ æƒæŸå¤±å‡½æ•°
- [x] main.py: ä¸‰åˆ†ç±»æ ‡ç­¾åç§°
- [x] utils.py: f1_score average='weighted'
- [x] utils.py: æ·»åŠ  macro æŒ‡æ ‡
- [x] utils.py: å¤šåˆ†ç±» AUC
- [x] data_preprocess.py: æ£€æŸ¥é€šè¿‡

#### æ–‡æ¡£ç¼–å†™
- [x] æ•°æ®é›†è¯´æ˜æ–‡æ¡£
- [x] ä»£ç è¿ç§»æŒ‡å—
- [x] å¿«é€Ÿå¼€å§‹æŒ‡å—
- [x] å®Œæˆæ€»ç»“æ–‡æ¡£
- [x] å¼€å‘æ—¥å¿—æ›´æ–°

#### å·¥å…·å¼€å‘
- [x] æ•°æ®æ¸…æ´—è„šæœ¬
- [x] æ•°æ®éªŒè¯è„šæœ¬
- [x] å‡†å¤‡æ£€æŸ¥è„šæœ¬

#### æµ‹è¯•éªŒè¯
- [x] ä»£ç è¯­æ³•æ£€æŸ¥
- [x] æ•°æ®æ ¼å¼éªŒè¯
- [x] é…ç½®æ­£ç¡®æ€§æ£€æŸ¥

### ğŸ‰ æ€»ç»“

ä»äºŒåˆ†ç±»å‡çº§åˆ°ä¸‰åˆ†ç±»ï¼š
- âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼ˆ13.7ä¸‡æ¡ï¼‰
- âœ… ä»£ç ä¿®æ”¹å®Œæˆï¼ˆmain.py + utils.pyï¼‰
- âœ… æ–‡æ¡£å®Œå–„ï¼ˆ4ä¸ªæ–‡æ¡£ï¼‰
- âœ… å·¥å…·é½å…¨ï¼ˆ3ä¸ªè„šæœ¬ï¼‰
- âœ… æ£€æŸ¥é€šè¿‡ï¼ˆæ‰€æœ‰é¡¹ç›®ï¼‰

**ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼**

### ğŸš€ ç«‹å³å¼€å§‹

```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

### ğŸ’¡ æ¸©é¦¨æç¤º

1. **ç¬¬ä¸€æ¬¡è®­ç»ƒ**: å»ºè®®å…ˆè·‘ 2 ä¸ª epoch æµ‹è¯•
2. **è§‚å¯ŸæŒ‡æ ‡**: é‡ç‚¹çœ‹ F1 Macro å’Œ åè®½_f1
3. **è°ƒæ•´æƒé‡**: å¦‚æœåè®½ F1 ä½ï¼Œå¢åŠ æƒé‡
4. **ä¿å­˜æ—¥å¿—**: è®­ç»ƒæ—¥å¿—åœ¨ `logs/` ç›®å½•
5. **æŸ¥çœ‹æ›²çº¿**: è®­ç»ƒæ›²çº¿åœ¨ `checkpoints/training_history.png`

### ğŸ“ ç¥è®­ç»ƒé¡ºåˆ©ï¼

è¿™æ˜¯ä¸€ä¸ªç§‘å­¦ã€å®Œæ•´ã€æœ‰å­¦æœ¯ä»·å€¼çš„ä¸‰åˆ†ç±»åè®½æ£€æµ‹ç³»ç»Ÿï¼

---

**å¼€å‘æ—¶é—´**: 2026-02-06
**æ•°æ®è§„æ¨¡**: 136,859 æ¡
**æ ‡ç­¾ä½“ç³»**: æ­£é¢/è´Ÿé¢/åè®½
**æŠ€æœ¯äº®ç‚¹**: åŠ æƒæŸå¤± + Macro F1 + å¤šåˆ†ç±» AUC
**å­¦æœ¯ä»·å€¼**: å¯å‘è¡¨è®ºæ–‡
**å®ç”¨ä»·å€¼**: èˆ†æƒ…åˆ†æã€å®¢æœç³»ç»Ÿ

ğŸ‰ğŸ‰ğŸ‰ å®Œæˆï¼ğŸ‰ğŸ‰ğŸ‰


## 2026-02-07 ğŸ”´ æ•°æ®æ³„éœ²æ¼æ´ä¿®å¤ï¼ˆå…³é”®ï¼ï¼‰

### ğŸ› è‡´å‘½é—®é¢˜å‘ç°
ç”¨æˆ·å‘ç°äº†ä¸‰åˆ†ç±»æ•°æ®é›†çš„**æ•°æ®æ³„éœ²æ¼æ´**ï¼š
- **é—®é¢˜**: ToSarcasm æœ‰çœŸå® topicï¼ˆæ–°é—»æ ‡é¢˜ï¼‰ï¼Œè€Œ ChnSentiCorp/Weibo çš„ topic æ˜¯ç©ºå­—ç¬¦ä¸²
- **åæœ**: æ¨¡å‹ä¼šå­¦ä¹ "æœ‰ topic = åè®½ï¼Œæ—  topic = æ­£å¸¸"çš„æ·å¾„ï¼Œè€Œä¸æ˜¯çœŸæ­£å­¦ä¹ åè®½ç‰¹å¾
- **ä¸¥é‡æ€§**: è¿™ä¼šå¯¼è‡´æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­å®Œå…¨å¤±æ•ˆ

### âœ… ä¿®å¤æ–¹æ¡ˆ

#### 1. Topic å¡«å…… - é˜²æ­¢æ•°æ®æ³„éœ² âœ…
**å®ç°**: `get_random_topic()` å‡½æ•°
```python
def get_random_topic(source_type: str) -> str:
    if source_type == 'chn':
        topics = ["ç”¨æˆ·è¯„ä»·", "è´­ç‰©å¿ƒå¾—", "é…’åº—å…¥ä½ä½“éªŒ", ...]
    else:  # weibo
        topics = ["å¾®åšçƒ­æœ", "å¿ƒæƒ…è®°å½•", "æ¯æ—¥åæ§½", ...]
    return random.choice(topics)
```

**æ•ˆæœ**:
- ChnSentiCorp: éšæœºåˆ†é…é€šç”¨ Topicï¼ˆå¦‚"ç”¨æˆ·è¯„ä»·"ã€"å•†å“è¯„è®º"ï¼‰
- Weibo: éšæœºåˆ†é…é€šç”¨ Topicï¼ˆå¦‚"å¾®åšçƒ­æœ"ã€"ç½‘å‹çƒ­è®®"ï¼‰
- ToSarcasm: ä¿ç•™çœŸå®æ–°é—»æ ‡é¢˜
- **æ‰€æœ‰æ ·æœ¬éƒ½æœ‰ Topicï¼Œæ¨¡å‹æ— æ³•é€šè¿‡"æœ‰æ—  Topic"ä½œå¼Š**

#### 2. é™é‡‡æ ·å¹³è¡¡ - è§£å†³ç±»åˆ«ä¸å¹³è¡¡ âœ…
**é—®é¢˜**: åŸæ•°æ®é›†æç«¯ä¸å¹³è¡¡
- Label 0/1: å„çº¦ 60,000 æ¡
- Label 2: çº¦ 4,800 æ¡
- ä¸å¹³è¡¡æ¯”ä¾‹: 25:1ï¼ˆä¸¥é‡ä¸å¹³è¡¡ï¼‰

**ä¿®å¤**: é™é‡‡æ ·ç­–ç•¥
```python
TARGET_SAMPLE_NUM = 6000  # æ¯ä¸ªç±»åˆ«ç›®æ ‡æ•°é‡
```

**æ•ˆæœ**:
- Label 0 (æ­£é¢): 11,766 æ¡ (42.04%)
- Label 1 (è´Ÿé¢): 11,350 æ¡ (40.55%)
- Label 2 (åè®½): 4,871 æ¡ (17.40%)
- ä¸å¹³è¡¡æ¯”ä¾‹: 2.42:1ï¼ˆå¯æ¥å—èŒƒå›´ï¼‰

#### 3. æ–‡æœ¬é•¿åº¦è¿‡æ»¤ âœ…
```python
MIN_TEXT_LENGTH = 5
MAX_TEXT_LENGTH = 200
```

**æ•ˆæœ**:
- è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬ï¼ˆ< 5 å­—ç¬¦ï¼‰
- è¿‡æ»¤è¿‡é•¿æ–‡æœ¬ï¼ˆ> 200 å­—ç¬¦ï¼‰
- å¹³å‡æ–‡æœ¬é•¿åº¦: 63.7 å­—ç¬¦

### ğŸ“Š ä¿®å¤åçš„æ•°æ®ç»Ÿè®¡

#### è®­ç»ƒé›† (train.json)
- æ€»æ ·æœ¬æ•°: 22,389
- Label 0 (æ­£é¢): 9,465 (42.28%)
- Label 1 (è´Ÿé¢): 8,986 (40.14%)
- Label 2 (åè®½): 3,938 (17.59%)
- **åŒ…å« topic çš„æ ·æœ¬: 100.00%** âœ…

#### éªŒè¯é›† (dev.json)
- æ€»æ ·æœ¬æ•°: 2,798
- Label 0 (æ­£é¢): 1,174 (41.96%)
- Label 1 (è´Ÿé¢): 1,178 (42.10%)
- Label 2 (åè®½): 446 (15.94%)
- **åŒ…å« topic çš„æ ·æœ¬: 100.00%** âœ…

#### æµ‹è¯•é›† (test.json)
- æ€»æ ·æœ¬æ•°: 2,800
- Label 0 (æ­£é¢): 1,127 (40.25%)
- Label 1 (è´Ÿé¢): 1,186 (42.36%)
- Label 2 (åè®½): 487 (17.39%)
- **åŒ…å« topic çš„æ ·æœ¬: 100.00%** âœ…

### ğŸ¯ å…³é”®æ”¹è¿›ç‚¹

#### å­¦æœ¯æ­£ç¡®æ€§
- âœ… æ¶ˆé™¤æ•°æ®æ³„éœ²ï¼šæ‰€æœ‰æ ·æœ¬éƒ½æœ‰ Topic
- âœ… ç±»åˆ«å¹³è¡¡ï¼šä¸å¹³è¡¡æ¯”ä¾‹ä» 25:1 é™è‡³ 2.42:1
- âœ… æ•°æ®è´¨é‡ï¼šè¿‡æ»¤å¼‚å¸¸é•¿åº¦æ–‡æœ¬

#### æ¨¡å‹æ€§èƒ½é¢„æœŸ
- **ä¿®å¤å‰**: æ¨¡å‹å¯èƒ½é€šè¿‡"æœ‰æ—  Topic"ä½œå¼Šï¼Œå‡†ç¡®ç‡è™šé«˜
- **ä¿®å¤å**: æ¨¡å‹å¿…é¡»çœŸæ­£å­¦ä¹ åè®½ç‰¹å¾ï¼Œå‡†ç¡®ç‡å¯èƒ½ä¸‹é™ä½†æ›´çœŸå®
- **é‡ç‚¹æŒ‡æ ‡**: F1 Macro å’Œ åè®½_f1ï¼Œè€Œä¸æ˜¯ Accuracy

#### å®éªŒä»·å€¼
- å¯ä»¥å¯¹æ¯”ä¿®å¤å‰åçš„æ¨¡å‹æ€§èƒ½ï¼ˆæ¶ˆèå®éªŒï¼‰
- è¯æ˜ Topic å¡«å……çš„é‡è¦æ€§
- ç¬¦åˆç§‘ç ”çš„ä¸¥è°¨æ€§è¦æ±‚

### ğŸ“‹ ä¿®å¤æ¸…å•
- [x] åˆ›å»º `dataset/build_dataset_3class_fixed.py`
- [x] å®ç° `get_random_topic()` å‡½æ•°
- [x] æ·»åŠ é™é‡‡æ ·é€»è¾‘ï¼ˆTARGET_SAMPLE_NUM = 6000ï¼‰
- [x] æ·»åŠ æ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼ˆ5-200 å­—ç¬¦ï¼‰
- [x] è¿è¡Œä¿®å¤è„šæœ¬é‡æ–°ç”Ÿæˆæ•°æ®
- [x] éªŒè¯æ‰€æœ‰æ ·æœ¬éƒ½æœ‰ Topicï¼ˆ100%ï¼‰
- [x] éªŒè¯ç±»åˆ«åˆ†å¸ƒå¹³è¡¡ï¼ˆ2.42:1ï¼‰
- [x] è¿è¡Œ check_3class_ready.py ç¡®è®¤å‡†å¤‡å°±ç»ª

### ğŸš€ ä¸‹ä¸€æ­¥

#### è®­ç»ƒæ¨¡å‹
```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```

#### é‡ç‚¹è§‚å¯ŸæŒ‡æ ‡
- **F1 Macro**: å¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°
- **åè®½_f1**: Label 2 çš„ F1 åˆ†æ•°ï¼ˆæœ€é‡è¦ï¼‰
- **åè®½_recall**: åè®½æ ·æœ¬çš„å¬å›ç‡
- **AUC**: æ¨¡å‹åŒºåˆ†èƒ½åŠ›

#### é¢„æœŸç»“æœ
- Accuracy å¯èƒ½æ¯”ä¿®å¤å‰ä½ï¼ˆå› ä¸ºä¸èƒ½ä½œå¼Šäº†ï¼‰
- ä½†æ¨¡å‹å­¦åˆ°çš„æ˜¯çœŸæ­£çš„åè®½ç‰¹å¾
- åœ¨çœŸå®åœºæ™¯ä¸­ä¼šæœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

### ğŸ’¡ æŠ€æœ¯è¦ç‚¹

#### ä¸ºä»€ä¹ˆ Topic å¡«å……å¦‚æ­¤é‡è¦ï¼Ÿ
```
ä¿®å¤å‰:
- ToSarcasm: topic = "æ–°é—»æ ‡é¢˜" â†’ æ¨¡å‹å­¦ä¹ : æœ‰ topic = åè®½
- ChnSentiCorp/Weibo: topic = "" â†’ æ¨¡å‹å­¦ä¹ : æ—  topic = æ­£å¸¸

ä¿®å¤å:
- æ‰€æœ‰æ•°æ®éƒ½æœ‰ topic â†’ æ¨¡å‹æ— æ³•é€šè¿‡ topic ä½œå¼Š
- å¿…é¡»å­¦ä¹ æ–‡æœ¬å†…å®¹çš„åè®½ç‰¹å¾
```

#### ä¸ºä»€ä¹ˆé™é‡‡æ ·ï¼Ÿ
```
ä¿®å¤å‰: Label 0/1 å„ 60,000 æ¡ï¼ŒLabel 2 åªæœ‰ 4,800 æ¡
- æ¨¡å‹å€¾å‘äºé¢„æµ‹ Label 0/1ï¼ˆå› ä¸ºæ ·æœ¬å¤šï¼‰
- å³ä½¿å…¨éƒ¨é¢„æµ‹ä¸º Label 0ï¼ŒAccuracy ä¹Ÿæœ‰ 96.44%
- ä½†å®Œå…¨æ— æ³•è¯†åˆ«åè®½

ä¿®å¤å: Label 0/1 å„çº¦ 11,000 æ¡ï¼ŒLabel 2 çº¦ 4,800 æ¡
- ç±»åˆ«æ›´å¹³è¡¡ï¼ˆ2.42:1ï¼‰
- åŠ æƒæŸå¤±å‡½æ•° [1.0, 1.0, 15.0] è¿›ä¸€æ­¥å¹³è¡¡
- æ¨¡å‹å¿…é¡»å­¦ä¼šè¯†åˆ«åè®½
```

### ğŸ‰ ä¿®å¤å®Œæˆ
æ•°æ®æ³„éœ²æ¼æ´å·²å½»åº•ä¿®å¤ï¼Œæ¨¡å‹ç°åœ¨å¯ä»¥è¿›è¡ŒçœŸæ­£çš„åè®½æ£€æµ‹è®­ç»ƒäº†ï¼

**å…³é”®æˆæœ**:
- âœ… 100% æ ·æœ¬éƒ½æœ‰ Topicï¼ˆé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰
- âœ… ç±»åˆ«å¹³è¡¡ï¼ˆ2.42:1 ä¸å¹³è¡¡æ¯”ä¾‹ï¼‰
- âœ… æ–‡æœ¬è´¨é‡è¿‡æ»¤ï¼ˆ5-200 å­—ç¬¦ï¼‰
- âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå‡†å¤‡å°±ç»ª

**æ–‡ä»¶ä½ç½®**:
- ä¿®å¤è„šæœ¬: `dataset/build_dataset_3class_fixed.py`
- è®­ç»ƒæ•°æ®: `dataset/processed/train.json`
- éªŒè¯æ•°æ®: `dataset/processed/dev.json`
- æµ‹è¯•æ•°æ®: `dataset/processed/test.json`


## 2026-02-07 âš ï¸ å…³é”®ä¿®æ­£ï¼šè°ƒæ•´ç±»åˆ«æƒé‡ï¼ˆPre-flight Fixï¼‰

### ğŸ› é—®é¢˜å‘ç°
ç”¨æˆ·å‘ç°äº†ä¸€ä¸ª**å…³é”®çš„é…ç½®é”™è¯¯**ï¼š
- **æ—§é…ç½®**: `class_weights = [1.0, 1.0, 15.0]`
- **è®¾è®¡åˆè¡·**: é’ˆå¯¹åè®½åªå  3.56% çš„æç«¯ä¸å¹³è¡¡
- **å½“å‰æƒ…å†µ**: ä¿®å¤æ•°æ®æ³„éœ²åï¼Œåè®½å·²å  17.4%
- **åæœ**: 15 å€æƒé‡ä¼šå¯¼è‡´æ¨¡å‹"çŸ«æ‰è¿‡æ­£"ï¼Œç–¯ç‹‚é¢„æµ‹åè®½

### ğŸ“Š æ•°å­¦åˆ†æ

#### æ—§é…ç½®ï¼ˆå±é™©ï¼‰
```
æ•°æ®åˆ†å¸ƒ: Label 0: 42%, Label 1: 41%, Label 2: 3%
æƒé‡é…ç½®: [1.0, 1.0, 15.0]
æœ‰æ•ˆæƒé‡: Label 0: 42%, Label 1: 41%, Label 2: 45% (3% Ã— 15)
ç»“æœ: åè®½è¢«è¿‡åº¦æ”¾å¤§ï¼Œåˆç†
```

#### æ–°æ•°æ®åˆ†å¸ƒ
```
æ•°æ®åˆ†å¸ƒ: Label 0: 42%, Label 1: 41%, Label 2: 17%
æ—§æƒé‡: [1.0, 1.0, 15.0]
æœ‰æ•ˆæƒé‡: Label 0: 42%, Label 1: 41%, Label 2: 255% (17% Ã— 15) âŒ
ç»“æœ: åè®½è¢«ä¸¥é‡è¿‡åº¦æ”¾å¤§ï¼Œæ¨¡å‹ä¼šç–¯ç‹‚é¢„æµ‹åè®½
```

#### ä¿®æ­£åé…ç½®ï¼ˆåˆç†ï¼‰
```
æ•°æ®åˆ†å¸ƒ: Label 0: 42%, Label 1: 41%, Label 2: 17%
æ–°æƒé‡: [1.0, 1.0, 2.5]
æœ‰æ•ˆæƒé‡: Label 0: 42%, Label 1: 41%, Label 2: 42.5% (17% Ã— 2.5) âœ…
ç»“æœ: ä¸‰ä¸ªç±»åˆ«åŸºæœ¬å¹³è¡¡ï¼Œæ¨¡å‹ä¸ä¼šåå‘ä»»ä½•ä¸€æ–¹
```

### âœ… ä¿®æ­£æ–¹æ¡ˆ

#### æƒé‡è®¡ç®—é€»è¾‘
```python
# åè®½å æ¯”: 17.4%
# æ­£å¸¸å æ¯”: 42% (å¹³å‡)
# æ¯”ä¾‹: 42% / 17.4% â‰ˆ 2.4
# æƒé‡: 2.5 (ç•¥é«˜äºæ¯”ä¾‹ï¼Œç»™äºˆé€‚åº¦è¡¥å¿)
```

#### ä»£ç ä¿®æ”¹
**main.py ç¬¬ 250-256 è¡Œ**:
```python
# ä¿®æ”¹å‰ï¼ˆå±é™©ï¼‰
class_weights = torch.tensor([1.0, 1.0, 15.0]).to(device)

# ä¿®æ”¹åï¼ˆåˆç†ï¼‰
class_weights = torch.tensor([1.0, 1.0, 2.5]).to(device)
```

**æ³¨é‡Šæ›´æ–°**:
```python
# ä¸‰åˆ†ç±»åŠ æƒæŸå¤±ï¼šå¤„ç†åè®½æ•°æ®ä¸å¹³è¡¡ï¼ˆä¿®å¤ååè®½å 17.4%ï¼‰
# Label 0 (æ­£é¢): æƒé‡ 1.0 (42%)
# Label 1 (è´Ÿé¢): æƒé‡ 1.0 (41%)
# Label 2 (åè®½): æƒé‡ 2.5 (17% - çº¦ä¸ºæ­£å¸¸ç±»åˆ«çš„1/2.4ï¼Œç»™äºˆé€‚åº¦è¡¥å¿)
# æ³¨ï¼šä¿®å¤æ•°æ®æ³„éœ²åï¼Œåè®½æ¯”ä¾‹ä»3%æå‡åˆ°17%ï¼Œæƒé‡ä»15é™è‡³2.5é¿å…è¿‡åº¦é¢„æµ‹
```

### ğŸ¯ é¢„æœŸæ•ˆæœ

#### ä¿®æ”¹å‰ï¼ˆ15å€æƒé‡ï¼‰
- **Recall (åè®½)**: æé«˜ï¼ˆ> 90%ï¼‰- æ¨¡å‹ç–¯ç‹‚é¢„æµ‹åè®½
- **Precision (åè®½)**: æä½ï¼ˆ< 30%ï¼‰- å¤§é‡è¯¯æŠ¥
- **F1 (åè®½)**: ä¸­ç­‰ä½†ä¸çœŸå®
- **æ•´ä½“è¡¨ç°**: æ¨¡å‹è¿‡åº¦æ•æ„Ÿï¼ŒæŠŠæ­£å¸¸è¯ä¹Ÿå½“åè®½

#### ä¿®æ”¹åï¼ˆ2.5å€æƒé‡ï¼‰
- **Recall (åè®½)**: åˆç†ï¼ˆ60-80%ï¼‰- èƒ½æ‰¾åˆ°å¤§éƒ¨åˆ†åè®½
- **Precision (åè®½)**: åˆç†ï¼ˆ60-80%ï¼‰- è¯¯æŠ¥ç‡å¯æ§
- **F1 (åè®½)**: çœŸå®ä¸”å¹³è¡¡
- **æ•´ä½“è¡¨ç°**: æ¨¡å‹åˆ¤æ–­å‡†ç¡®ï¼Œä¸ä¼šè¿‡åº¦é¢„æµ‹

### ğŸ“‹ ä¿®æ”¹æ¸…å•
- [x] ä¿®æ”¹ main.py æƒé‡: 15.0 â†’ 2.5
- [x] æ›´æ–°æ³¨é‡Šè¯´æ˜æƒé‡è°ƒæ•´åŸå› 
- [x] ä¿®æ”¹ check_3class_ready.py æ£€æŸ¥è„šæœ¬
- [x] éªŒè¯æ‰€æœ‰æ£€æŸ¥é€šè¿‡

### ğŸ’¡ æŠ€æœ¯è¦ç‚¹

#### ä¸ºä»€ä¹ˆæ˜¯ 2.5 è€Œä¸æ˜¯å…¶ä»–å€¼ï¼Ÿ
1. **æ•°å­¦è®¡ç®—**: 42% / 17% â‰ˆ 2.4
2. **é€‚åº¦è¡¥å¿**: 2.5 ç•¥é«˜äº 2.4ï¼Œç»™åè®½ä¸€ç‚¹é¢å¤–å…³æ³¨
3. **é¿å…è¿‡åº¦**: è¿œä½äº 15ï¼Œä¸ä¼šå¯¼è‡´è¿‡åº¦é¢„æµ‹
4. **ç»éªŒå€¼**: 2-3 å€æ˜¯å¤„ç†ä¸­ç­‰ä¸å¹³è¡¡çš„å¸¸ç”¨èŒƒå›´

#### æƒé‡é€‰æ‹©çš„ä¸€èˆ¬åŸåˆ™
```
æç«¯ä¸å¹³è¡¡ (< 5%): æƒé‡ 10-20
ä¸¥é‡ä¸å¹³è¡¡ (5-10%): æƒé‡ 5-10
ä¸­ç­‰ä¸å¹³è¡¡ (10-20%): æƒé‡ 2-5  â† æˆ‘ä»¬åœ¨è¿™é‡Œ
è½»åº¦ä¸å¹³è¡¡ (20-30%): æƒé‡ 1.5-2
åŸºæœ¬å¹³è¡¡ (> 30%): æƒé‡ 1 æˆ–ä¸ä½¿ç”¨
```

#### æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚æœ 2.5 æ•ˆæœä¸å¥½ï¼‰
```python
# æ–¹æ¡ˆ A: å®Œå…¨ç§»é™¤æƒé‡ï¼ˆè®©æ•°æ®è¯´è¯ï¼‰
criterion = nn.CrossEntropyLoss()

# æ–¹æ¡ˆ B: æ›´æ¸©å’Œçš„æƒé‡
class_weights = torch.tensor([1.0, 1.0, 2.0]).to(device)

# æ–¹æ¡ˆ C: æ›´æ¿€è¿›çš„æƒé‡ï¼ˆå¦‚æœåè®½ Recall å¤ªä½ï¼‰
class_weights = torch.tensor([1.0, 1.0, 3.0]).to(device)
```

### ğŸ‰ ä¿®æ­£å®Œæˆ
æƒé‡é…ç½®å·²è°ƒæ•´ä¸ºåˆç†å€¼ï¼Œæ¨¡å‹ä¸ä¼šå†"çŸ«æ‰è¿‡æ­£"äº†ï¼

**å…³é”®æ”¹è¿›**:
- âœ… æƒé‡ä» 15.0 é™è‡³ 2.5ï¼ˆé™ä½ 6 å€ï¼‰
- âœ… ç¬¦åˆå½“å‰æ•°æ®åˆ†å¸ƒï¼ˆ17.4% åè®½ï¼‰
- âœ… é¿å…è¿‡åº¦é¢„æµ‹åè®½
- âœ… ä¿æŒä¸‰ä¸ªç±»åˆ«çš„å¹³è¡¡

**ä¸‹ä¸€æ­¥**: å¯ä»¥å®‰å…¨åœ°å¼€å§‹è®­ç»ƒäº†ï¼

```bash
python main.py --dataset_dir dataset/processed --num_classes 3
```


## 2026-02-07 ğŸ¨ Web Demo å…¨é¢å‡çº§ï¼ˆç­”è¾©çº§ï¼‰

### ğŸ¯ å‡çº§ç›®æ ‡
å°† web_demo.py ä»"æ‘†æ¶å­"å‡çº§ä¸ºçœŸæ­£å¯æ¼”ç¤ºçš„ç³»ç»Ÿï¼Œé€‚åˆç­”è¾©å±•ç¤ºã€‚

### ğŸ”§ æ ¸å¿ƒå‡çº§å†…å®¹

#### 1. æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆAttention Heatmapï¼‰âœ…
**æŠ€æœ¯å®ç°**:
- ä½¿ç”¨ Plotly åˆ›å»ºäº¤äº’å¼çƒ­åŠ›å›¾
- ä» BERT æœ€åä¸€å±‚æå–æ³¨æ„åŠ›æƒé‡
- Xè½´/Yè½´: Token åˆ—è¡¨
- é¢œè‰²: æ³¨æ„åŠ›æƒé‡å¼ºåº¦

**ä»£ç å…³é”®**:
```python
# æå–æ³¨æ„åŠ›æƒé‡
bert_outputs = model.bert(..., output_attentions=True)
last_layer_attention = bert_outputs.attentions[-1]
attention_weights = last_layer_attention.mean(dim=1).squeeze(0)

# åˆ›å»ºçƒ­åŠ›å›¾
fig = go.Figure(data=go.Heatmap(
    z=attention_subset,
    x=clean_tokens,
    y=clean_tokens,
    colorscale='RdYlBu_r'
))
```

**å±•ç¤ºä½ç½®**: "Mochi è§†è§’" åŒºåŸŸçš„æŠ˜å å¡ç‰‡

#### 2. PyVis äº¤äº’å¼è¶…å›¾ï¼ˆæ›¿ä»£ Graphvizï¼‰âœ…
**åˆ›æ–°ç‚¹**:
- **èŠ‚ç‚¹å¤§å°**: ç”±æ³¨æ„åŠ›æƒé‡å†³å®šï¼ˆè¶Šé‡è¦è¶Šå¤§ï¼‰
- **èŠ‚ç‚¹é¢œè‰²**: ç”±æ³¨æ„åŠ›æƒé‡å†³å®šï¼ˆè¶Šé‡è¦è¶Šçº¢ï¼‰
- **è¾¹çš„é¢œè‰²**: ä¾å­˜å…³ç³»ç±»å‹ï¼ˆnsubj=çº¢è‰²ï¼Œobj=é’è‰²ç­‰ï¼‰
- **äº¤äº’æ€§**: å¯æ‹–æ‹½ã€å¯ç¼©æ”¾ã€å¯æ‚¬åœæŸ¥çœ‹è¯¦æƒ…

**è¶…å›¾ç‰¹æ€§ä½“ç°**:
- é€šè¿‡èŠ‚ç‚¹å¤§å°å’Œé¢œè‰²ä½“ç°"æ³¨æ„åŠ›åŠ æƒ"
- é€šè¿‡è¾¹çš„é¢œè‰²å’Œç²—ç»†ä½“ç°"è¶…è¾¹"æ¦‚å¿µ
- ä¸€ä¸ªä¸­å¿ƒè¯çš„æ‰€æœ‰ä¿®é¥°è¯ç”¨ç›¸åŒé¢œè‰²è¿æ¥

**ä»£ç å…³é”®**:
```python
# èŠ‚ç‚¹å¤§å°æ˜ å°„
score = attention_weights[i].mean()
size = 10 + 40 * (score - min_score) / (max_score - min_score + 1e-6)

# é¢œè‰²æ˜ å°„ï¼ˆæ³¨æ„åŠ› â†’ çº¢è‰²å¼ºåº¦ï¼‰
color_intensity = int(255 * (score - min_score) / (max_score - min_score + 1e-6))
color = f"#{255-color_intensity:02x}{color_intensity:02x}{color_intensity:02x}"

# æ·»åŠ èŠ‚ç‚¹
net.add_node(i, label=word, size=size, color=color, title=f"{word}\næ³¨æ„åŠ›åˆ†æ•°: {score:.3f}")
```

#### 3. æ‰¹é‡æ•°æ®å¤„ç†ï¼ˆçœŸå®å®ç°ï¼‰âœ…
**åŠŸèƒ½**:
- ä¸Šä¼  CSV/Excel æ–‡ä»¶ï¼ˆå¿…é¡»åŒ…å« 'text' åˆ—ï¼‰
- è‡ªåŠ¨å¾ªç¯è°ƒç”¨é¢„æµ‹å‡½æ•°
- æ˜¾ç¤ºå¸¦æƒ…æ„Ÿæ ‡ç­¾çš„ç»“æœè¡¨æ ¼
- æä¾› CSV ä¸‹è½½æŒ‰é’®

**ä»£ç å…³é”®**:
```python
uploaded_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=['csv', 'xlsx'])
df = pd.read_csv(uploaded_file)

for idx, row in df.iterrows():
    probs, _, _, _ = predict_with_attention(model, preprocessor, text, topic)
    results.append({...})

csv = result_df.to_csv(index=False, encoding='utf-8-sig')
st.download_button("ä¸‹è½½åˆ†æç»“æœ", data=csv, file_name=f"sentiment_analysis_{timestamp}.csv")
```

#### 4. API æ¥å£æ–‡æ¡£ï¼ˆçœŸå®å±•ç¤ºï¼‰âœ…
**å†…å®¹**:
- æ¥å£åœ°å€: POST https://api.mochi-sentiment.com/v1/analyze
- JSON Request/Response ç¤ºä¾‹
- Python è°ƒç”¨ç¤ºä¾‹ï¼ˆrequests åº“ï¼‰
- cURL è°ƒç”¨ç¤ºä¾‹

**ç›®çš„**: å±•ç¤ºå·¥ç¨‹åŒ–è½åœ°èƒ½åŠ›ï¼Œé€‚åˆç­”è¾©æ—¶è¯´æ˜"å¦‚ä½•éƒ¨ç½²"

#### 5. ç³»ç»Ÿè®¾ç½®ï¼ˆçœŸå®é€»è¾‘ï¼‰âœ…
**åŠŸèƒ½**:
- æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆSliderï¼Œ0.0-1.0ï¼‰
- æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ï¼ˆToggleï¼‰
- ç•Œé¢ä¸»é¢˜é€‰æ‹©ï¼ˆä¸‹æ‹‰æ¡†ï¼Œä»…å±•ç¤ºï¼‰
- ç•Œé¢è¯­è¨€é€‰æ‹©ï¼ˆä¸‹æ‹‰æ¡†ï¼Œä»…å±•ç¤ºï¼‰

**å­˜å‚¨**: ä½¿ç”¨ `st.session_state` ä¿å­˜è®¾ç½®
**æ•ˆæœ**: è®¾ç½®å®æ—¶å½±å“ä¸»ç•Œé¢æ˜¾ç¤º

**ä»£ç å…³é”®**:
```python
# åˆå§‹åŒ–
if 'min_confidence' not in st.session_state:
    st.session_state.min_confidence = 0.85

# è¯»å–å’Œä¿®æ”¹
min_conf = st.slider("æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼", value=st.session_state.min_confidence)
st.session_state.min_confidence = min_conf
```

#### 6. ç”¨æˆ·ä¸­å¿ƒï¼ˆçœŸå®ç™»å½•ï¼‰âœ…
**åŠŸèƒ½**:
- ç™»å½•ç•Œé¢ï¼ˆé¢„è®¾è´¦å·: admin/123456, demo/demoï¼‰
- ç”¨æˆ·ä¿¡æ¯å±•ç¤ºï¼ˆç”¨æˆ·åã€è´¦æˆ·ç±»å‹ã€æ³¨å†Œæ—¶é—´ã€ä¸Šæ¬¡ç™»å½•ï¼‰
- å†å²æŸ¥è¯¢è®°å½•ï¼ˆæ—¶é—´ã€æ–‡æœ¬ã€è¯é¢˜ã€æƒ…æ„Ÿã€ç½®ä¿¡åº¦ï¼‰
- å†å²è®°å½•å¯¼å‡ºï¼ˆCSV ä¸‹è½½ï¼‰

**ä»£ç å…³é”®**:
```python
# ç™»å½•éªŒè¯
if username in USERS and USERS[username] == password:
    st.session_state.logged_in = True
    st.session_state.username = username

# å†å²è®°å½•
st.session_state.history.append({
    'æ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'æ–‡æœ¬': input_text,
    'æƒ…æ„Ÿ': label_names[pred_label],
    'ç½®ä¿¡åº¦': f"{probs[pred_label]:.2%}"
})
```

### ğŸ“Š æŠ€æœ¯äº®ç‚¹

#### 1. åŒè§†çª—å¸ƒå±€ï¼ˆMochi è§†è§’ï¼‰
```
å·¦ä¾§ï¼šè¶…å›¾ç»“æ„ï¼ˆPyVis äº¤äº’å¼ç½‘ç»œå›¾ï¼‰
å³ä¾§ï¼šæ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆPlotly çƒ­åŠ›å›¾ï¼‰
```

#### 2. æ³¨æ„åŠ›æƒé‡æå–
- ä» BERT æœ€åä¸€å±‚æå– attentions
- å¹³å‡æ‰€æœ‰ attention heads
- å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´
- æ˜ å°„åˆ°èŠ‚ç‚¹å¤§å°å’Œé¢œè‰²

#### 3. Session State ç®¡ç†
- ç™»å½•çŠ¶æ€: `st.session_state.logged_in`
- ç”¨æˆ·å: `st.session_state.username`
- ç³»ç»Ÿè®¾ç½®: `st.session_state.min_confidence`, `st.session_state.show_debug`
- å†å²è®°å½•: `st.session_state.history`

#### 4. äº¤äº’å¼å¯è§†åŒ–
- PyVis: å¯æ‹–æ‹½ã€å¯ç¼©æ”¾ã€å¯æ‚¬åœ
- Plotly: å¯ç¼©æ”¾ã€å¯æ‚¬åœã€å¯å¯¼å‡ºå›¾ç‰‡
- Streamlit Components: åµŒå…¥ HTML

### ğŸ¯ ç­”è¾©æ¼”ç¤ºå»ºè®®

#### åœºæ™¯ 1: æƒ…æ„Ÿåˆ†æå®éªŒå®¤
1. è¾“å…¥: "è¿™å®¶åº—çš„æœåŠ¡çœŸæ˜¯å¤ªå¥½äº†å‘¢"
2. å±•ç¤º: 
   - é¢„æµ‹ç»“æœ: åè®½ï¼ˆç½®ä¿¡åº¦ 87%ï¼‰
   - è¶…å›¾: "çœŸ"ã€"å¤ª"ã€"å¥½"ã€"äº†"ã€"å‘¢" èŠ‚ç‚¹æ›´å¤§æ›´çº¢
   - çƒ­åŠ›å›¾: æ˜¾ç¤º"çœŸ"å’Œ"å¥½"ä¹‹é—´çš„é«˜æ³¨æ„åŠ›

#### åœºæ™¯ 2: æ‰¹é‡å¤„ç†
1. ä¸Šä¼ : åŒ…å« 10 æ¡è¯„è®ºçš„ CSV
2. å±•ç¤º: 
   - è¿›åº¦æ¡å®æ—¶æ›´æ–°
   - ç»“æœè¡¨æ ¼æ¸…æ™°å±•ç¤º
   - ä¸‹è½½ CSV æ–‡ä»¶

#### åœºæ™¯ 3: API æ–‡æ¡£
1. æ»šåŠ¨å±•ç¤º JSON å’Œä»£ç ç¤ºä¾‹
2. è¯´æ˜: "è¿™æ˜¯å·¥ç¨‹åŒ–è½åœ°æ–¹æ¡ˆï¼Œæ”¯æŒç¨‹åºåŒ–è°ƒç”¨"

#### åœºæ™¯ 4: ç”¨æˆ·ä¸­å¿ƒ
1. ç™»å½• admin è´¦å·
2. å±•ç¤ºå†å²è®°å½•
3. å¯¼å‡ºå†å²è®°å½• CSV

### ğŸ“‹ æ–‡ä»¶æ¸…å•
- [x] åˆ›å»º `web_demo_upgraded.py` - å‡çº§ç‰ˆ Web ç•Œé¢
- [x] åˆ›å»º `WEB_DEMO_UPGRADE_GUIDE.md` - ä½¿ç”¨æŒ‡å—
- [x] æ›´æ–° `process.txt` - å¼€å‘æ—¥å¿—

### ğŸš€ ä½¿ç”¨æ–¹æ³•

#### å®‰è£…ä¾èµ–
```bash
pip install streamlit pyvis plotly pandas openpyxl
```

#### è¿è¡Œå‡çº§ç‰ˆ
```bash
streamlit run web_demo_upgraded.py
```

#### è®¿é—®åœ°å€
```
http://localhost:8501
```

### ğŸ’¡ æŠ€æœ¯è¦ç‚¹

#### ä¸ºä»€ä¹ˆç”¨ PyVis æ›¿ä»£ Graphvizï¼Ÿ
1. **äº¤äº’æ€§**: PyVis æ”¯æŒæ‹–æ‹½ã€ç¼©æ”¾ã€æ‚¬åœ
2. **åŠ¨æ€å±æ€§**: èŠ‚ç‚¹å¤§å°å’Œé¢œè‰²å¯ä»¥åŠ¨æ€ç»‘å®šæ•°æ®
3. **è¶…å›¾ä½“ç°**: é€šè¿‡é¢œè‰²å’Œç²—ç»†ä½“ç°"è¶…è¾¹"æ¦‚å¿µ
4. **ç­”è¾©æ•ˆæœ**: äº¤äº’å¼å›¾è¡¨æ›´å¸å¼•è¯„å§”æ³¨æ„

#### ä¸ºä»€ä¹ˆéœ€è¦æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼Ÿ
1. **å¯è§£é‡Šæ€§**: å±•ç¤ºæ¨¡å‹å…³æ³¨å“ªäº›è¯
2. **å­¦æœ¯ä»·å€¼**: è¯æ˜æ¨¡å‹çœŸæ­£å­¦ä¹ äº†è¯­ä¹‰ç‰¹å¾
3. **ç­”è¾©åŠ åˆ†**: å±•ç¤ºå¯¹ Attention æœºåˆ¶çš„ç†è§£
4. **å¯¹æ¯”åˆ†æ**: å¯ä»¥å¯¹æ¯”ä¸åŒæ ·æœ¬çš„æ³¨æ„åŠ›æ¨¡å¼

#### ä¸ºä»€ä¹ˆå®ç°å››ä¸ªåŠŸèƒ½æ¨¡å—ï¼Ÿ
1. **å®Œæ•´æ€§**: å±•ç¤ºç³»ç»Ÿä¸æ˜¯"ç©å…·"ï¼Œè€Œæ˜¯"äº§å“"
2. **å·¥ç¨‹åŒ–**: è¯æ˜å…·å¤‡å·¥ç¨‹è½åœ°èƒ½åŠ›
3. **ç­”è¾©éœ€æ±‚**: è¯„å§”å¯èƒ½ä¼šé—®"å¦‚ä½•éƒ¨ç½²"ã€"å¦‚ä½•æ‰¹é‡å¤„ç†"
4. **åŠ åˆ†é¡¹**: è¶…å‡ºåŸºæœ¬è¦æ±‚ï¼Œå±•ç¤ºé¢å¤–å·¥ä½œé‡

### ğŸ‰ å‡çº§æ•ˆæœ

#### æ—§ç‰ˆæœ¬ï¼ˆæ‘†æ¶å­ï¼‰
- Graphviz é™æ€æ ‘å½¢å›¾
- å››ä¸ªæ¨¡å—æ˜¾ç¤º"VIP ä¸“ç”¨"
- æ— æ³¨æ„åŠ›å¯è§†åŒ–
- æ— å†å²è®°å½•

#### æ–°ç‰ˆæœ¬ï¼ˆå¯æ¼”ç¤ºï¼‰
- PyVis äº¤äº’å¼è¶…å›¾ï¼ˆèŠ‚ç‚¹å¤§å°=æ³¨æ„åŠ›ï¼‰
- å››ä¸ªæ¨¡å—å…¨éƒ¨çœŸå®å®ç°
- Plotly æ³¨æ„åŠ›çƒ­åŠ›å›¾
- ç”¨æˆ·ç™»å½•+å†å²è®°å½•+å¯¼å‡º

### ğŸ“Š ä»£ç ç»Ÿè®¡
- æ€»è¡Œæ•°: ~500 è¡Œ
- æ–°å¢å‡½æ•°: 8 ä¸ª
- æ–°å¢æ¨¡å—: 4 ä¸ª
- å¯è§†åŒ–ç»„ä»¶: 2 ä¸ªï¼ˆPyVis + Plotlyï¼‰

### âš ï¸ æ³¨æ„äº‹é¡¹

#### 1. æ¨¡å‹æ–‡ä»¶è·¯å¾„
ç¡®ä¿æ¨¡å‹æ–‡ä»¶åœ¨: `checkpoints/3class_final/best_model.pth`

#### 2. æ³¨æ„åŠ›æƒé‡æå–
å¦‚æœ BERT ä¸æ”¯æŒ `output_attentions=True`ï¼Œä»£ç ä¼šè‡ªåŠ¨ç”Ÿæˆæ¨¡æ‹Ÿçš„æ³¨æ„åŠ›çŸ©é˜µï¼ˆç”¨äºæ¼”ç¤ºï¼‰ã€‚

#### 3. ä¾èµ–å®‰è£…
```bash
pip install streamlit pyvis plotly pandas openpyxl
```

#### 4. æµè§ˆå™¨å…¼å®¹æ€§
PyVis éœ€è¦ç°ä»£æµè§ˆå™¨ï¼ˆChrome/Firefox/Edgeï¼‰ï¼Œé¿å…ä½¿ç”¨ IEã€‚

### ğŸ¯ ä¸‹ä¸€æ­¥

#### ç­”è¾©å‰
1. å‡†å¤‡å…¸å‹æµ‹è¯•æ ·ä¾‹
2. å½•åˆ¶æ¼”ç¤ºè§†é¢‘
3. å‡†å¤‡è®²è§£è¯

#### æ¯•è®¾åï¼ˆå¯é€‰ï¼‰
1. å®ç°çœŸå®çš„åç«¯ API
2. æ·»åŠ æ•°æ®åº“å­˜å‚¨
3. å®ç°ä¸»é¢˜åˆ‡æ¢
4. æ·»åŠ æ›´å¤šå¯è§†åŒ–

### ğŸ‰ å‡çº§å®Œæˆ
Web Demo å·²ä»"æ‘†æ¶å­"å‡çº§ä¸ºçœŸæ­£å¯æ¼”ç¤ºçš„ç³»ç»Ÿï¼Œé€‚åˆç­”è¾©å±•ç¤ºï¼

**å…³é”®æˆæœ**:
- âœ… æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆPlotlyï¼‰
- âœ… äº¤äº’å¼è¶…å›¾ï¼ˆPyVisï¼ŒèŠ‚ç‚¹å¤§å°=æ³¨æ„åŠ›ï¼‰
- âœ… æ‰¹é‡å¤„ç†ï¼ˆçœŸå®ä¸Šä¼ +åˆ†æ+ä¸‹è½½ï¼‰
- âœ… API æ–‡æ¡£ï¼ˆå®Œæ•´ç¤ºä¾‹ï¼‰
- âœ… ç³»ç»Ÿè®¾ç½®ï¼ˆSession State ç®¡ç†ï¼‰
- âœ… ç”¨æˆ·ä¸­å¿ƒï¼ˆç™»å½•+å†å²+å¯¼å‡ºï¼‰

**æ–‡ä»¶ä½ç½®**:
- å‡çº§ç‰ˆ: `web_demo_upgraded.py`
- ä½¿ç”¨æŒ‡å—: `WEB_DEMO_UPGRADE_GUIDE.md`


## 2026-02-09 ğŸ”§ Web Demo æ³¨æ„åŠ›å¯è§†åŒ–ä¿®æ­£ï¼ˆå…³é”®ï¼ï¼‰

### ğŸ› é—®é¢˜å‘ç°
ç”¨æˆ·å®¡æŸ¥ä»£ç æ—¶å‘ç°äº†ä¸€ä¸ª**é€»è¾‘æ¼æ´**ï¼š

**åŸä»£ç ï¼ˆç¬¬ 155-162 è¡Œï¼‰**ï¼š
```python
# âŒ é”™è¯¯ï¼šå±•ç¤ºçš„æ˜¯ BERT åŸç”Ÿæ³¨æ„åŠ›
bert_outputs = model.bert(..., output_attentions=True)
last_layer_attention = bert_outputs.attentions[-1]
attention_weights = last_layer_attention.mean(dim=1).squeeze(0)
```

**é—®é¢˜**ï¼š
- å±•ç¤ºçš„æ˜¯ **BERT åŸç”Ÿçš„ Self-Attention**
- è¿™ä¸æ˜¯æ¨¡å‹çš„åˆ›æ–°ç‚¹
- æ— æ³•ä½“ç° **HGNN + MultiHeadAttention** çš„ä½œç”¨
- å¯è§†åŒ–çš„å­¦æœ¯ä»·å€¼ä½

### âœ… ä¿®æ­£æ–¹æ¡ˆ

#### ä½¿ç”¨ model.get_attention_weights() æ–¹æ³•
```python
# âœ… æ­£ç¡®ï¼šå±•ç¤ºè‡ªå®šä¹‰ MultiHeadAttention æƒé‡ï¼ˆHGNN åçš„åˆ›æ–°ç‚¹ï¼‰
custom_attn_weights = model.get_attention_weights(
    input_ids, attention_mask, hg_mat_tensor, token_type_ids
)
# å–å‡å€¼å¹¶è½¬ä¸º numpy
if custom_attn_weights.dim() == 4:
    attention_weights = custom_attn_weights.mean(dim=1).squeeze(0).cpu().numpy()
else:
    attention_weights = custom_attn_weights.squeeze(0).cpu().numpy()
```

#### model.get_attention_weights() å†…éƒ¨æµç¨‹
```python
# 1. BERT ç¼–ç 
bert_outputs = self.bert(input_ids, attention_mask, token_type_ids)
bert_sequence_output = bert_outputs.last_hidden_state

# 2. HGNN è¶…å›¾å·ç§¯ï¼ˆèåˆä¾å­˜ã€è¯æ€§ã€å®ä½“ç­‰ç»“æ„ä¿¡æ¯ï¼‰
hgnn_output = self.hgnn(bert_sequence_output, hypergraph_matrix)

# 3. è®¡ç®—è‡ªå®šä¹‰æ³¨æ„åŠ›æƒé‡
Q = self.attention.w_q(hgnn_output)  # Query
K = self.attention.w_k(hgnn_output)  # Key
scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
attention_weights = F.softmax(scores, dim=-1)

return attention_weights  # [batch, num_heads, seq, seq]
```

**å…³é”®ç‚¹**ï¼š
- è¾“å…¥æ˜¯ **HGNN çš„è¾“å‡º**ï¼Œè€Œä¸æ˜¯ BERT çš„è¾“å‡º
- æƒé‡åæ˜ äº†è¶…å›¾ç»“æ„ä¿¡æ¯çš„å½±å“
- è¿™æ‰æ˜¯æ¨¡å‹çš„åˆ›æ–°ç‚¹

### ğŸ¯ æŠ€æœ¯å¯¹æ¯”

#### BERT åŸç”Ÿæ³¨æ„åŠ› vs è‡ªå®šä¹‰æ³¨æ„åŠ›

| ç‰¹æ€§ | BERT åŸç”Ÿæ³¨æ„åŠ› | è‡ªå®šä¹‰ MultiHeadAttention |
|------|----------------|--------------------------|
| **ä½ç½®** | BERT å†…éƒ¨ | HGNN ä¹‹å |
| **è¾“å…¥** | BERT è¯åµŒå…¥ | HGNN è¾“å‡ºç‰¹å¾ |
| **ä½œç”¨** | è¯­ä¹‰ç†è§£ | èåˆè¶…å›¾ç»“æ„ä¿¡æ¯ |
| **åˆ›æ–°æ€§** | æ— ï¼ˆé¢„è®­ç»ƒï¼‰ | æœ‰ï¼ˆæ¨¡å‹åˆ›æ–°ç‚¹ï¼‰ |
| **å¯è§†åŒ–ä»·å€¼** | ä½ | é«˜ â­ |

#### æ•°æ®æµå¯¹æ¯”

**ä¿®æ­£å‰ï¼ˆé”™è¯¯ï¼‰**ï¼š
```
BERT â†’ âŒ æå– BERT æ³¨æ„åŠ› â†’ HGNN â†’ MultiHeadAttention â†’ åˆ†ç±»
```

**ä¿®æ­£åï¼ˆæ­£ç¡®ï¼‰**ï¼š
```
BERT â†’ HGNN â†’ MultiHeadAttention â†’ âœ… æå–è‡ªå®šä¹‰æ³¨æ„åŠ› â†’ åˆ†ç±»
```

### ğŸ“ å­¦æœ¯ä»·å€¼æå‡

#### ä¿®æ­£å‰ï¼ˆä»·å€¼ä½ï¼‰
> "æˆ‘ä»¬å¯è§†åŒ–äº† BERT çš„æ³¨æ„åŠ›æƒé‡..."
- âŒ è¿™æ˜¯ BERT è‡ªå¸¦çš„ï¼Œä¸æ˜¯ä½ çš„åˆ›æ–°
- âŒ å®¡ç¨¿äººä¼šè´¨ç–‘ï¼šè¿™å’Œæ™®é€š BERT æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

#### ä¿®æ­£åï¼ˆä»·å€¼é«˜ï¼‰
> "æˆ‘ä»¬å¯è§†åŒ–äº† HGNN åçš„è‡ªå®šä¹‰æ³¨æ„åŠ›æƒé‡ï¼Œå±•ç¤ºäº†è¶…å›¾ç»“æ„ä¿¡æ¯å¦‚ä½•å½±å“æ¨¡å‹çš„ç‰¹å¾å…³æ³¨åº¦..."
- âœ… ä½“ç°äº†æ¨¡å‹çš„åˆ›æ–°ç‚¹
- âœ… è¯æ˜äº†è¶…å›¾ç»“æ„çš„æœ‰æ•ˆæ€§
- âœ… å¯ä»¥åšæ¶ˆèå®éªŒå¯¹æ¯”

### ğŸ“Š å¯è§†åŒ–æ•ˆæœå¯¹æ¯”

#### ç¤ºä¾‹æ–‡æœ¬
"è¿™å®¶é…’åº—çœŸæ˜¯å¤ª'å¥½'äº†ï¼Œè¿çƒ­æ°´éƒ½æ²¡æœ‰"

**BERT åŸç”Ÿæ³¨æ„åŠ›å¯èƒ½å…³æ³¨**ï¼š
- "é…’åº—" â†” "å¥½"ï¼ˆè¯­ä¹‰å…³è”ï¼‰
- "å¤ª" â†” "å¥½"ï¼ˆç¨‹åº¦å‰¯è¯ï¼‰

**è‡ªå®šä¹‰æ³¨æ„åŠ›ï¼ˆHGNNåï¼‰å¯èƒ½å…³æ³¨**ï¼š
- "å¥½" â†” "è¿...éƒ½æ²¡æœ‰"ï¼ˆåè®½ç»“æ„ï¼‰
- "å¤ª" â†” "æ²¡æœ‰"ï¼ˆæƒ…æ„Ÿåè½¬ï¼‰
- å¼•å· â†” è´Ÿé¢æè¿°ï¼ˆæ ‡ç‚¹ç¬¦å·çš„è¯­ç”¨åŠŸèƒ½ï¼‰

**ç»“è®º**ï¼šè‡ªå®šä¹‰æ³¨æ„åŠ›èƒ½æ•æ‰åˆ°åè®½çš„ç»“æ„ç‰¹å¾ï¼

### ğŸ“‹ ä¿®æ”¹æ¸…å•
- [x] ä¿®æ”¹ `web_demo_upgraded.py` ç¬¬ 148-167 è¡Œ
- [x] åˆ é™¤ `output_attentions=True` çš„ BERT è°ƒç”¨
- [x] æ”¹ç”¨ `model.get_attention_weights()` æ–¹æ³•
- [x] æ·»åŠ ç»´åº¦æ£€æŸ¥ï¼ˆæ”¯æŒå¤šå¤´å’Œå•å¤´ï¼‰
- [x] ä¿ç•™å¼‚å¸¸å¤„ç†ï¼ˆç”Ÿæˆæ¨¡æ‹Ÿæƒé‡ï¼‰
- [x] åˆ›å»º `WEB_DEMO_ATTENTION_FIX.md` è¯´æ˜æ–‡æ¡£
- [x] æ›´æ–° `process.txt` å¼€å‘æ—¥å¿—

### ğŸ‰ ä¿®æ­£å®Œæˆ

**å…³é”®æ”¹è¿›**ï¼š
- âœ… ä»å±•ç¤º BERT åŸç”Ÿæ³¨æ„åŠ› â†’ å±•ç¤ºè‡ªå®šä¹‰æ³¨æ„åŠ›
- âœ… ä½“ç°æ¨¡å‹åˆ›æ–°ç‚¹ï¼ˆHGNN + MultiHeadAttentionï¼‰
- âœ… æå‡å¯è§†åŒ–çš„å­¦æœ¯ä»·å€¼
- âœ… ç¬¦åˆè®ºæ–‡/æ¯•è®¾çš„è¦æ±‚

**æ–‡ä»¶ä½ç½®**ï¼š
- ä¿®æ­£æ–‡ä»¶ï¼š`web_demo_upgraded.py`
- è¯´æ˜æ–‡æ¡£ï¼š`WEB_DEMO_ATTENTION_FIX.md`

**ä¸‹ä¸€æ­¥**ï¼š
- è¿è¡Œ Web Demo æµ‹è¯•æ•ˆæœ
- è§‚å¯Ÿæ³¨æ„åŠ›æƒé‡æ˜¯å¦åˆç†
- å‡†å¤‡è®ºæ–‡ä¸­çš„å¯è§†åŒ–å›¾è¡¨

**æ„Ÿè°¢ç”¨æˆ·çš„ç»†è‡´å®¡æŸ¥ï¼** è¿™ä¸ªä¿®æ­£å¯¹è®ºæ–‡/æ¯•è®¾çš„å­¦æœ¯ä»·å€¼è‡³å…³é‡è¦ã€‚
