# é¡¹ç›®å¼€å‘æ—¥å¿— - Process Log

## 2024-02-05 é¡¹ç›®åˆå§‹åŒ–

### 1. æ¨¡å—åŒ–é‡æ„
- åˆ›å»º data_preprocess.pyï¼šHanLPå¤„ç† + è¶…å›¾ç»“æ„ç”Ÿæˆ
- åˆ›å»º model.pyï¼šBERT + HGNN + Attention ç½‘ç»œç»“æ„  
- åˆ›å»º utils.pyï¼šå·¥å…·å‡½æ•°é›†åˆ
- åˆ›å»º main.pyï¼šå®éªŒå…¥å£å’Œè®­ç»ƒæµç¨‹
- åˆ›å»º requirements.txtï¼šä¾èµ–åŒ…ç®¡ç†
- åˆ›å»º README.mdï¼šé¡¹ç›®æ–‡æ¡£

### 2. ç¡¬ä»¶æ— å…³æ€§ä¼˜åŒ–
- ç»Ÿä¸€ä½¿ç”¨æ ‡å‡†å†™æ³•ï¼š`device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`
- æ‰€æœ‰æ¨¡å‹å’Œå¼ é‡åˆå§‹åŒ–æ—¶æ·»åŠ  `.to(device)`
- æ›´æ–° requirements.txt æ·»åŠ  DHG åº“
- ç¡®ä¿ä»£ç åœ¨ CPU/GPU ç¯å¢ƒæ— ç¼åˆ‡æ¢

### 3. é¡¹ç›®ç»“æ„
```
â”œâ”€â”€ data_preprocess.py    # æ•°æ®é¢„å¤„ç†
â”œâ”€â”€ model.py             # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”œâ”€â”€ main.py              # ä¸»ç¨‹åº
â”œâ”€â”€ requirements.txt     # ä¾èµ–ç®¡ç†
â”œâ”€â”€ README.md           # é¡¹ç›®æ–‡æ¡£
â””â”€â”€ process.txt         # å¼€å‘æ—¥å¿—
```

### 4. æ ¸å¿ƒåŠŸèƒ½
- HanLP æ–‡æœ¬å¤„ç†ï¼ˆåˆ†è¯ã€è¯æ€§ã€NERã€ä¾å­˜åˆ†æï¼‰
- è¶…å›¾ç»“æ„æ„å»ºï¼ˆèŠ‚ç‚¹ç‰¹å¾ + è¾¹è¿æ¥ï¼‰
- BERT + HGNN + MultiHead Attention èåˆæ¨¡å‹
- å®Œæ•´è®­ç»ƒ/éªŒè¯/æµ‹è¯•æµç¨‹
- å¯è§†åŒ–å’ŒæŒ‡æ ‡è®¡ç®—

### 5. ä¸‹ä¸€æ­¥è®¡åˆ’
- [ ] æµ‹è¯•æ•°æ®é¢„å¤„ç†æ¨¡å—
- [ ] éªŒè¯æ¨¡å‹å‰å‘ä¼ æ’­
- [ ] å®Œå–„è®­ç»ƒå¾ªç¯ä¸­çš„æ•°æ®æµ
- [ ] æ·»åŠ è¶…å‚æ•°è°ƒä¼˜åŠŸèƒ½
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†

---
*è®°å½•æ ¼å¼ï¼šæ—¥æœŸ + æ“ä½œç±»å‹ + å…·ä½“å†…å®¹*

## 2024-02-05 é‡å¤§é”™è¯¯ä¿®æ­£

### 1. BERT Tokenizer ä¿®æ­£ âŒâ¡ï¸âœ…
**é”™è¯¯**: è‡ªå·±æ„å»ºè¯æ±‡è¡¨ï¼Œä¸BERTé¢„è®­ç»ƒtokenizerä¸åŒ¹é…
**ä¿®æ­£**: 
- åˆ é™¤ `_build_vocabulary()` å‡½æ•°
- ä½¿ç”¨ `BertTokenizer.from_pretrained()` 
- ç¡®ä¿token IDsä¸BERTé¢„è®­ç»ƒæ¨¡å‹ä¸€è‡´

### 2. è¶…å›¾ç»“æ„ä¿®æ­£ âŒâ¡ï¸âœ…
**é”™è¯¯**: ä½¿ç”¨æ™®é€šå›¾é‚»æ¥çŸ©é˜µ NÃ—N
**ä¿®æ­£**: 
- æ”¹ä¸ºè¶…å›¾å…³è”çŸ©é˜µ H âˆˆ R^{NÃ—M}
- Nä¸ªèŠ‚ç‚¹(è¯ä½ç½®)ï¼ŒMæ¡è¶…è¾¹(è¯­è¨€å­¦ç»“æ„)
- è¶…è¾¹ç±»å‹ï¼šä¾å­˜å¥æ³•ç°‡ã€è¯æ€§æ ‡æ³¨ç°‡ã€å‘½åå®ä½“ç°‡ã€æ»‘åŠ¨çª—å£

### 3. è¶…å›¾å·ç§¯å…¬å¼ä¿®æ­£ âŒâ¡ï¸âœ…
**é”™è¯¯**: ç®€å•çŸ©é˜µç›¸ä¹˜ï¼Œç¼ºå°‘åº¦çŸ©é˜µå½’ä¸€åŒ–
**ä¿®æ­£**: 
- å®ç°å®Œæ•´å…¬å¼ï¼šX^{(l+1)} = Ïƒ(D_v^{-1/2} H W D_e^{-1} H^T D_v^{-1/2} X^{(l)} Î˜)
- æ·»åŠ èŠ‚ç‚¹åº¦çŸ©é˜µ D_v å’Œè¶…è¾¹åº¦çŸ©é˜µ D_e
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### 4. é…ç½®ç®¡ç†ä¼˜åŒ– âœ…
- åˆ›å»º config.yaml é¿å…ç¡¬ç¼–ç è·¯å¾„
- HanLPæ¨¡å‹åœ¨ __init__ ä¸­ä¸€æ¬¡æ€§åŠ è½½
- æ”¯æŒçµæ´»çš„å‚æ•°é…ç½®

### 5. æ•°æ®æµä¿®æ­£ âœ…
- BERT tokenization ä¸ HanLP å¤„ç†åˆ†ç¦»
- æ­£ç¡®çš„å¼ é‡ç»´åº¦å’Œè®¾å¤‡ç®¡ç†
- ä¿®æ­£æ•°æ®åŠ è½½å™¨è¿”å›æ ¼å¼

### 6. æŠ€æœ¯å€ºåŠ¡æ¸…ç†
- [x] åˆ é™¤é”™è¯¯çš„è¯æ±‡è¡¨æ„å»º
- [x] ä¿®æ­£è¶…å›¾æ•°å­¦å®šä¹‰
- [x] å®ç°æ­£ç¡®çš„è¶…å›¾å·ç§¯
- [x] æ·»åŠ é…ç½®æ–‡ä»¶ç®¡ç†
- [x] ä¼˜åŒ–HanLPåŠ è½½ç­–ç•¥
## 2024-02-05 æ•°æ®é›†é€‚é…é‡æ„

### 1. JSONæ•°æ®é›†æ”¯æŒ âœ…
**æ•°æ®æ ¼å¼**: 
- train.json, dev.json, test.json
- æ¯æ¡æ•°æ®ï¼š{"text": "è¯„è®º", "topic": "ä¸»é¢˜", "label": "0/1"}
- è®½åˆºæ£€æµ‹ä»»åŠ¡ï¼š0-éè®½åˆºï¼Œ1-è®½åˆº

### 2. æ–‡æœ¬å¯¹å¤„ç†ä¼˜åŒ– âœ…
**BERTå¤„ç†**:
- ä½¿ç”¨ `tokenizer(text, topic)` å¤„ç†æ–‡æœ¬å¯¹
- è‡ªåŠ¨æ·»åŠ  [CLS] text [SEP] topic [SEP] ç»“æ„
- æ­£ç¡®çš„ token_type_ids åŒºåˆ†ä¸¤ä¸ªå¥å­

**HanLPå¤„ç†**:
- åˆå¹¶æ–‡æœ¬å’Œä¸»é¢˜è¿›è¡Œè¯­è¨€å­¦åˆ†æ
- è®°å½•ä¸»é¢˜å’Œè¯„è®ºçš„é•¿åº¦è¾¹ç•Œ
- ç”¨äºæ„å»ºä¸»é¢˜-è¯„è®ºäº¤äº’è¶…è¾¹

### 3. è¶…å›¾ç»“æ„å¢å¼º âœ…
**æ–°å¢è¶…è¾¹ç±»å‹**:
- ä¸»é¢˜-è¯„è®ºäº¤äº’è¶…è¾¹ï¼ˆè®½åˆºæ£€æµ‹ç‰¹æœ‰ï¼‰
- ä¸»é¢˜å†…éƒ¨è¶…è¾¹
- è¯„è®ºå†…éƒ¨è¶…è¾¹
- ä¿ç•™åŸæœ‰çš„ä¾å­˜ã€è¯æ€§ã€å®ä½“ã€çª—å£è¶…è¾¹

### 4. æ•°æ®åŠ è½½å™¨é‡æ„ âœ…
**SarcasmDatasetç±»**:
- ä¸“é—¨å¤„ç†è®½åˆºæ£€æµ‹æ•°æ®æ ¼å¼
- è‡ªå®šä¹‰ collate_fn æ‰¹å¤„ç†è¶…å›¾çŸ©é˜µ
- ç¡¬ä»¶æ— å…³çš„å¼ é‡ç®¡ç†

**å®Œæ•´æ•°æ®æµ**:
- JSONåŠ è½½ â†’ BERT tokenization â†’ HanLPåˆ†æ â†’ è¶…å›¾æ„å»º â†’ æ‰¹å¤„ç†

### 5. è®­ç»ƒæµç¨‹å®Œå–„ âœ…
- ä¿®å¤è®­ç»ƒå’ŒéªŒè¯å‡½æ•°çš„æ•°æ®æµ
- æ­£ç¡®çš„å‰å‘ä¼ æ’­è°ƒç”¨
- å®Œæ•´çš„æŒ‡æ ‡è®¡ç®—å’Œæ—¥å¿—è®°å½•
- æ”¯æŒæµ‹è¯•é›†è¯„ä¼°

### 6. é…ç½®ä¼˜åŒ– âœ…
- æ›´æ–° config.yaml é€‚é…JSONæ•°æ®é›†
- å‡å°æ‰¹æ¬¡å¤§å°é€‚åº”è¶…å›¾è®¡ç®—å¼€é”€
- çµæ´»çš„æ•°æ®è·¯å¾„é…ç½®

### 7. ä½¿ç”¨æ–¹æ³•
```bash
# ä½¿ç”¨é»˜è®¤æ•°æ®é›†ç›®å½•
python main.py --dataset_dir dataset

# æˆ–æŒ‡å®šå…·ä½“æ–‡ä»¶
python main.py --train_data dataset/train.json --val_data dataset/dev.json
```
## 2024-02-05 å…³é”®æŠ€æœ¯é—®é¢˜ä¿®æ­£

### 1. æ‰¹å¤„ç†é€»è¾‘æ¼æ´ä¿®æ­£ âŒâ¡ï¸âœ…
**é—®é¢˜**: æ ·æœ¬é—´æ•°æ®æ³„éœ² - æ‰€æœ‰æ ·æœ¬å…±ç”¨ä¸€ä¸ªè¶…å›¾çŸ©é˜µ
**ä¿®æ­£**: 
- `_create_single_hypergraph_matrix()`: æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹çš„è¶…å›¾çŸ©é˜µ
- `estimate_max_edges()`: åŠ¨æ€ä¼°ç®—æœ€å¤§è¶…è¾¹æ•°ç”¨äºpadding
- 3Då¼ é‡: [batch_size, max_nodes, max_edges]
- å®Œå…¨é¿å…æ ·æœ¬é—´ä¿¡æ¯æ³„éœ²

### 2. è¶…å›¾å·ç§¯ç»´åº¦å…¼å®¹æ€§ä¿®æ­£ âŒâ¡ï¸âœ…
**é—®é¢˜**: 2DçŸ©é˜µä¹˜æ³•æ— æ³•å¤„ç†æ‰¹å¤„ç†çš„3Då¼ é‡
**ä¿®æ­£**:
- ä½¿ç”¨ `torch.bmm()` æ‰¹çŸ©é˜µä¹˜æ³•
- å¹¿æ’­æœºåˆ¶å¤„ç†åº¦çŸ©é˜µå½’ä¸€åŒ–
- æ”¯æŒ [batch_size, N, M] è¾“å…¥æ ¼å¼
- å‘é‡åŒ–è®¡ç®—é¿å…å¾ªç¯

### 3. æ€§èƒ½ç“¶é¢ˆä¼˜åŒ– ğŸš€
**é—®é¢˜**: HanLPåœ¨è®­ç»ƒå¾ªç¯ä¸­é‡å¤è®¡ç®—ï¼ŒGPUåˆ©ç”¨ç‡æä½
**è§£å†³æ–¹æ¡ˆ**: ç¦»çº¿é¢„å¤„ç†
- `preprocess_offline.py`: ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰HanLPåˆ†æ
- é¢„å¤„ç†ç»“æœä¿å­˜ä¸º .pkl æ–‡ä»¶
- `PreprocessedDataset`: ç›´æ¥åŠ è½½é¢„å¤„ç†æ•°æ®
- **é¢„æœŸæ€§èƒ½æå‡**: 10å€ä»¥ä¸Šè®­ç»ƒé€Ÿåº¦

### 4. åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥ âœ…
**ä¼˜åŒ–**: ä¸åŒæ¨¡å—ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
- BERTå‚æ•°: 2e-5 (å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹)
- HGNN+Attention: 1e-3 (ä»é›¶è®­ç»ƒæ–°å±‚)
- 50å€å­¦ä¹ ç‡å·®å¼‚ï¼ŒåŠ é€Ÿæ”¶æ•›

### 5. ä½¿ç”¨æ–¹æ³•æ›´æ–°

**ç¬¬ä¸€æ­¥ - ç¦»çº¿é¢„å¤„ç†ï¼ˆæ¨èï¼‰**:
```bash
python preprocess_offline.py --dataset_dir dataset --output_dir preprocessed_data
```

**ç¬¬äºŒæ­¥ - å¿«é€Ÿè®­ç»ƒ**:
```bash
python main.py --use_preprocessed --preprocessed_dir preprocessed_data --batch_size 16
```

**æˆ–å®æ—¶å¤„ç†ï¼ˆè¾ƒæ…¢ï¼‰**:
```bash
python main.py --dataset_dir dataset --batch_size 8
```

### 6. æŠ€æœ¯å€ºåŠ¡æ¸…ç†
- [x] ä¿®å¤æ ·æœ¬é—´æ•°æ®æ³„éœ²
- [x] æ”¯æŒæ‰¹å¤„ç†3Dè¶…å›¾çŸ©é˜µ
- [x] ç¦»çº¿é¢„å¤„ç†æ€§èƒ½ä¼˜åŒ–
- [x] åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥
- [x] å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—

### 7. å…³é”®æ”¹è¿›ç‚¹
- **å­¦æœ¯æ­£ç¡®æ€§**: é¿å…æ•°æ®æ³„éœ²
- **è®¡ç®—æ•ˆç‡**: æ‰¹å¤„ç†ä¼˜åŒ–
- **è®­ç»ƒé€Ÿåº¦**: ç¦»çº¿é¢„å¤„ç†
- **æ”¶æ•›é€Ÿåº¦**: åˆ†å±‚å­¦ä¹ ç‡
- **ä»£ç è´¨é‡**: é”™è¯¯å¤„ç†å’Œæ–‡æ¡£
## 2024-02-05 ç¦»çº¿é¢„å¤„ç†è„šæœ¬ä¿®æ­£

### ğŸ› é—®é¢˜è¯Šæ–­
**é”™è¯¯**: `AttributeError: 'BertTokenizer' object has no attribute 'encode_plus'`
**æ ¹æœ¬åŸå› **: 
1. HanLP å’Œ transformers åº“çš„å‘½åç©ºé—´å†²çª
2. å¯èƒ½çš„å¾ªç¯å¯¼å…¥é—®é¢˜
3. é”™è¯¯å¤„ç†ä¸è¶³å¯¼è‡´æ‰€æœ‰æ ·æœ¬å¤±è´¥

### ğŸ”§ ä¿®æ­£æ–¹æ¡ˆ

#### 1. å‘½åç©ºé—´å†²çªè§£å†³ âœ…
- åˆ›å»º `SafeDataPreprocessor` ç±»é¿å…å¾ªç¯å¯¼å…¥
- æ˜ç¡®å¯¼å…¥ `from transformers import BertTokenizer`
- ç‹¬ç«‹çš„é¢„å¤„ç†é€»è¾‘ï¼Œä¸ä¾èµ– `data_preprocess.py`

#### 2. ç°ä»£åŒ– tokenizer è°ƒç”¨ âœ…
```python
# ä¿®æ­£å‰ï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰
encoded = tokenizer.encode_plus(...)

# ä¿®æ­£åï¼ˆç°ä»£åŒ–å†™æ³•ï¼‰
encoded = tokenizer(text, topic, ...)
```

#### 3. å¢å¼ºé”™è¯¯å¤„ç† âœ…
- æ¯ä¸ªæ­¥éª¤éƒ½æœ‰ try-catch åŒ…è£…
- è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè°ƒè¯•è¾“å‡º
- å¤±è´¥æ ·æœ¬ç»Ÿè®¡å’ŒæˆåŠŸç‡æŠ¥å‘Š
- ç©ºç»“æœæ£€æµ‹å’Œè·³è¿‡æœºåˆ¶

#### 4. é¿å…å¾ªç¯å¯¼å…¥ âœ…
- `create_fast_data_loaders` ä¸­å†…è”è¶…å›¾æ„å»ºå‡½æ•°
- ä¸å†ä¾èµ– `data_preprocess.DataPreprocessor`
- ç‹¬ç«‹çš„åŠŸèƒ½å®ç°

### ğŸ§¹ æ¸…ç†å’Œé‡è¯•æ­¥éª¤

#### ç¬¬ä¸€æ­¥ï¼šæ¸…ç†é”™è¯¯æ•°æ®
```bash
python clean_preprocessed.py
```

#### ç¬¬äºŒæ­¥ï¼šé‡æ–°é¢„å¤„ç†
```bash
python preprocess_offline.py --dataset_dir dataset --output_dir preprocessed_data
```

#### ç¬¬ä¸‰æ­¥ï¼šéªŒè¯ç»“æœ
- æ£€æŸ¥æˆåŠŸç‡ > 90%
- ç¡®è®¤ .pkl æ–‡ä»¶ä¸ä¸ºç©º
- æŸ¥çœ‹è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡

### ğŸ¯ é¢„æœŸæ”¹è¿›
- **é”™è¯¯ç‡**: ä» 100% é™è‡³ < 10%
- **è°ƒè¯•ä¿¡æ¯**: è¯¦ç»†çš„é”™è¯¯è¿½è¸ª
- **ç¨³å®šæ€§**: å•ä¸ªæ ·æœ¬å¤±è´¥ä¸å½±å“æ•´ä½“
- **å…¼å®¹æ€§**: è§£å†³åº“å†²çªé—®é¢˜

### ğŸ“‹ æ•…éšœæ’é™¤æ¸…å•
- [x] ä¿®æ­£ tokenizer è°ƒç”¨æ–¹å¼
- [x] è§£å†³å‘½åç©ºé—´å†²çª
- [x] å¢å¼ºé”™è¯¯å¤„ç†
- [x] é¿å…å¾ªç¯å¯¼å…¥
- [x] æ·»åŠ æ¸…ç†è„šæœ¬
- [x] è¯¦ç»†çš„è°ƒè¯•è¾“å‡º
## 2024-02-05 å½»åº•é‡å†™ç¦»çº¿é¢„å¤„ç†è„šæœ¬

### ğŸ”¥ é—®é¢˜æ ¹æºåˆ†æ
**æ ¸å¿ƒé—®é¢˜**: ç¯å¢ƒ"æ‰“æ¶" - ä»£ç ä¾èµ–å†²çª
- **æ—§ä»£ç **: è¯•å›¾è°ƒç”¨ `data_preprocess.py` ä¸­çš„ `DataPreprocessor` ç±»
- **å†²çªæº**: HanLP å’Œ transformers æ··åˆç¯å¢ƒå¯¼è‡´ tokenizer å‘½åç©ºé—´å†²çª
- **é”™è¯¯è¡¨ç°**: `BertTokenizer has no attribute encode_plus`

### ğŸš€ å½»åº•é‡å†™æ–¹æ¡ˆ

#### 1. å®Œå…¨è§£è€¦è®¾è®¡ âœ…
```python
# æ—§ä»£ç ï¼ˆæœ‰é—®é¢˜ï¼‰
from data_preprocess import DataPreprocessor  # å¯¼è‡´ç¯å¢ƒå†²çª

# æ–°ä»£ç ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
class IndependentPreprocessor:  # ä¸ä¾èµ–ä»»ä½•å…¶ä»–æ¨¡å—
```

#### 2. çº¯å‡€çš„åº“å¯¼å…¥ âœ…
```python
# ç‹¬ç«‹å¯¼å…¥ï¼Œé¿å…å‘½åç©ºé—´å†²çª
from transformers import BertTokenizer  # çº¯å‡€çš„ BERT tokenizer
import hanlp                           # ç‹¬ç«‹çš„ HanLP
```

#### 3. ç°ä»£åŒ– API è°ƒç”¨ âœ…
```python
# ç›´æ¥è°ƒç”¨æ–¹å¼ï¼ˆæ¨èï¼‰
encoded = self.bert_tokenizer(
    text, topic,
    add_special_tokens=True,
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)
```

#### 4. å¢å¼ºçš„é”™è¯¯å¤„ç† âœ…
- æ¯ä¸ªæ­¥éª¤ç‹¬ç«‹çš„ try-catch
- è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè°ƒè¯•è¾“å‡º
- å¤±è´¥æ ·æœ¬ç»Ÿè®¡å’ŒæˆåŠŸç‡æŠ¥å‘Š
- ç©ºç»“æœæ£€æµ‹å’ŒéªŒè¯æœºåˆ¶

### ğŸ¯ é‡å†™ç‰¹ç‚¹

#### å®Œå…¨ç‹¬ç«‹æ€§
- **é›¶ä¾èµ–**: ä¸ä¾èµ– `data_preprocess.py`
- **çº¯å‡€ç¯å¢ƒ**: ç‹¬ç«‹åŠ è½½ BERT å’Œ HanLP
- **é¿å…å†²çª**: å½»åº•è§£å†³å‘½åç©ºé—´é—®é¢˜

#### ç°ä»£åŒ–è®¾è®¡
- **ç±»å‹æç¤º**: å®Œæ•´çš„ç±»å‹æ³¨è§£
- **é”™è¯¯å¤„ç†**: å¥å£®çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
- **è¿›åº¦æ˜¾ç¤º**: è¯¦ç»†çš„å¤„ç†è¿›åº¦å’Œç»Ÿè®¡
- **æ¨¡å—åŒ–**: æ¸…æ™°çš„å‡½æ•°èŒè´£åˆ†ç¦»

#### ç”¨æˆ·å‹å¥½
- **è¯¦ç»†æ—¥å¿—**: ä¸°å¯Œçš„çŠ¶æ€ä¿¡æ¯
- **é”™è¯¯è¯Šæ–­**: å…·ä½“çš„é”™è¯¯ä½ç½®å’ŒåŸå› 
- **æˆåŠŸç‡ç»Ÿè®¡**: å®æ—¶çš„å¤„ç†æˆåŠŸç‡
- **æ–‡ä»¶éªŒè¯**: è‡ªåŠ¨æ£€æµ‹å’ŒéªŒè¯è¾“å‡º

### ğŸš€ ä½¿ç”¨æ–¹æ³•

#### ç¬¬ä¸€æ­¥ï¼šæ¸…ç†æ—§æ•°æ®
```bash
python clean_preprocessed.py
```

#### ç¬¬äºŒæ­¥ï¼šè¿è¡Œæ–°çš„é¢„å¤„ç†è„šæœ¬
```bash
python preprocess_offline.py --dataset_dir dataset --output_dir preprocessed_data
```

#### ç¬¬ä¸‰æ­¥ï¼šéªŒè¯ç»“æœ
- åº”è¯¥çœ‹åˆ°æˆåŠŸç‡ > 95%
- ç»¿è‰²çš„æˆåŠŸä¿¡æ¯ï¼Œè€Œä¸æ˜¯çº¢è‰²é”™è¯¯
- ç”Ÿæˆçš„ .pkl æ–‡ä»¶åŒ…å«æœ‰æ•ˆæ•°æ®

### ğŸ“Š é¢„æœŸæ”¹è¿›
- **é”™è¯¯ç‡**: ä» 100% é™è‡³ < 5%
- **ç¨³å®šæ€§**: å•ä¸ªæ ·æœ¬å¤±è´¥ä¸å½±å“æ•´ä½“
- **å¯ç»´æŠ¤æ€§**: ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºè°ƒè¯•
- **å…¼å®¹æ€§**: å½»åº•è§£å†³åº“å†²çªé—®é¢˜

### ğŸ”§ æŠ€æœ¯è¦ç‚¹
- **IndependentPreprocessor**: å®Œå…¨ç‹¬ç«‹çš„é¢„å¤„ç†å™¨ç±»
- **bert_encode()**: çº¯å‡€çš„ BERT ç¼–ç æ–¹æ³•
- **hanlp_analyze()**: ç‹¬ç«‹çš„ HanLP åˆ†ææ–¹æ³•
- **process_single_sample()**: å•æ ·æœ¬å¤„ç†æµç¨‹
- **è¯¦ç»†ç»Ÿè®¡**: æˆåŠŸç‡ã€é”™è¯¯è®¡æ•°ã€å¤„ç†æ—¶é—´

ç°åœ¨åº”è¯¥èƒ½çœ‹åˆ°ç»¿è‰²çš„æˆåŠŸä¿¡æ¯äº†ï¼ğŸ‰
## 2024-02-05 æ ¸å¿ƒæ‰‹æœ¯ - ä¿®æ­£å¯¼å…¥å’Œé€»è¾‘é—®é¢˜

### ğŸ”§ ç¬¬ä¸€æ­¥ï¼šdata_preprocess.py æ ¸å¿ƒæ‰‹æœ¯ âœ…

#### é—®é¢˜è¯Šæ–­
- **SarcasmDataset ç±»è¢«é”åœ¨å‡½æ•°å†…éƒ¨**ï¼šç¼©è¿›é”™è¯¯å¯¼è‡´ç±»æ— æ³•è¢«å¤–éƒ¨å¯¼å…¥
- **main.py æ‰¾ä¸åˆ° SarcasmDataset**ï¼š`from data_preprocess import SarcasmDataset` å¤±è´¥

#### æ‰‹æœ¯æ“ä½œ
```python
# ä¿®æ­£å‰ï¼ˆæœ‰é—®é¢˜ï¼‰
def create_data_loaders(...):
    class SarcasmDataset(Dataset):  # è¢«é”åœ¨å‡½æ•°å†…éƒ¨
        ...

# ä¿®æ­£åï¼ˆæ­£ç¡®ï¼‰
class SarcasmDataset(torch.utils.data.Dataset):  # å…¨å±€ä½œç”¨åŸŸ
    """è®½åˆºæ£€æµ‹æ•°æ®é›†ç±» - ç§»åˆ°å…¨å±€ä½œç”¨åŸŸ"""
    ...

def create_hypergraph_collate_fn(preprocessor, max_length):
    """åˆ›å»ºè¶…å›¾æ‰¹å¤„ç†å‡½æ•°çš„å·¥å‚å‡½æ•°"""
    ...
```

#### ä¿®æ­£è¦ç‚¹
- **å‘å·¦åç¼©è¿›**ï¼šå°† `SarcasmDataset` ç±»ç§»åˆ°é¡¶æ ¼ï¼ˆå…¨å±€ä½œç”¨åŸŸï¼‰
- **å·¥å‚å‡½æ•°æ¨¡å¼**ï¼š`create_hypergraph_collate_fn` è¿”å›é…ç½®å¥½çš„ `collate_fn`
- **æ¸…æ™°çš„èŒè´£åˆ†ç¦»**ï¼šç±»å®šä¹‰å’Œå‡½æ•°åˆ›å»ºåˆ†å¼€

### ğŸ”§ ç¬¬äºŒæ­¥ï¼špreprocess_offline.py æ¸…ç†é€»è¾‘ âœ…

#### é—®é¢˜è¯Šæ–­
- **é€»è¾‘æ­»èƒ¡åŒ**ï¼š`fast_collate_fn` ä¸­è°ƒç”¨ä¸å­˜åœ¨çš„ `preprocessor` å¯¹è±¡
- **å¤æ‚è®¡ç®—é”™ä½**ï¼šç¦»çº¿é¢„å¤„ç†åº”è¯¥"å¿«é€Ÿè¯»å–"ï¼Œä¸åº”è¯¥å†åšå¤æ‚è®¡ç®—

#### æ¸…ç†æ“ä½œ
```python
# ä¿®æ­£å‰ï¼ˆæœ‰é—®é¢˜ï¼‰
def fast_collate_fn(batch):
    max_edges = preprocessor.estimate_max_edges(...)  # preprocessor ä¸å­˜åœ¨ï¼
    single_matrix = preprocessor._create_single_hypergraph_matrix(...)  # å¤æ‚è®¡ç®—

# ä¿®æ­£åï¼ˆç®€åŒ–ï¼‰
def simple_collate_fn(batch):
    # åªåšåŸºæœ¬çš„å¼ é‡å †å 
    input_ids = torch.stack([item['input_ids'] for item in batch])
    hanlp_results = [item['hanlp_result'] for item in batch]  # ç®€å•åˆ—è¡¨
    return {...}
```

#### æ¸…ç†è¦ç‚¹
- **åˆ é™¤å¤æ‚è®¡ç®—**ï¼šç§»é™¤ `estimate_max_edges` å’Œè¶…å›¾çŸ©é˜µæ„å»º
- **ç®€åŒ–æ•°æ®æµ**ï¼šåªåšåŸºæœ¬çš„å¼ é‡å †å å’Œè®¾å¤‡ç§»åŠ¨
- **èŒè´£æ˜ç¡®**ï¼šç¦»çº¿é¢„å¤„ç†è´Ÿè´£"é¢„å¤„ç†"ï¼Œæ•°æ®åŠ è½½å™¨è´Ÿè´£"åŠ è½½"

### ğŸ¯ ä¿®æ­£æ•ˆæœ

#### å¯¼å…¥é—®é¢˜è§£å†³
```python
# ç°åœ¨å¯ä»¥æ­£å¸¸å¯¼å…¥
from data_preprocess import SarcasmDataset  # âœ… æˆåŠŸ
from preprocess_offline import load_preprocessed_data  # âœ… æˆåŠŸ
```

#### é€»è¾‘æ¸…æ™°åŒ–
- **data_preprocess.py**ï¼šè´Ÿè´£å®æ—¶å¤„ç†å’Œè¶…å›¾è®¡ç®—
- **preprocess_offline.py**ï¼šè´Ÿè´£ç¦»çº¿é¢„å¤„ç†å’Œå¿«é€ŸåŠ è½½
- **main.py**ï¼šæ ¹æ®å‚æ•°é€‰æ‹©å¤„ç†æ¨¡å¼

#### æ€§èƒ½ä¼˜åŒ–
- **ç¦»çº¿æ¨¡å¼**ï¼šé¢„å¤„ç†ä¸€æ¬¡ï¼Œå¿«é€ŸåŠ è½½
- **å®æ—¶æ¨¡å¼**ï¼šæ¯æ¬¡éƒ½è®¡ç®—ï¼Œä½†é€»è¾‘æ­£ç¡®
- **ç¡¬ä»¶æ— å…³**ï¼šä¸¤ç§æ¨¡å¼éƒ½æ”¯æŒ CPU/GPU

### ğŸ“‹ ä¿®æ­£æ¸…å•
- [x] SarcasmDataset ç§»åˆ°å…¨å±€ä½œç”¨åŸŸ
- [x] åˆ›å»º create_hypergraph_collate_fn å·¥å‚å‡½æ•°
- [x] ç®€åŒ– preprocess_offline.py çš„ collate_fn
- [x] åˆ é™¤ä¸å­˜åœ¨çš„ preprocessor å¼•ç”¨
- [x] ä¿æŒæ•°æ®æµçš„ä¸€è‡´æ€§
- [x] ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®

ç°åœ¨ main.py åº”è¯¥èƒ½æ­£å¸¸å¯¼å…¥å’Œä½¿ç”¨è¿™äº›ç±»äº†ï¼ğŸ‰
## 2024-02-05 è‡´å‘½Bugä¿®æ­£å’Œæ€§èƒ½ä¼˜åŒ–

### ğŸ› è‡´å‘½Bug 1ï¼šæ–¹æ³•åä¸åŒ¹é… âœ…

#### é—®é¢˜è¯Šæ–­
- **è°ƒç”¨**: `self._create_hypergraph_incidence_matrix(...)`
- **å®šä¹‰**: `_create_single_hypergraph_matrix(...)`
- **åæœ**: `AttributeError` è¿è¡Œç›´æ¥æŠ¥é”™

#### ä¿®æ­£æ–¹æ¡ˆ
- **åˆ é™¤å¤šä½™æ–¹æ³•**: ç§»é™¤ `build_hypergraph_structure` æ–¹æ³•
- **åŸå› **: `collate_fn` ä¸­å·²æœ‰æ­£ç¡®çš„æ‰¹å¤„ç†é€»è¾‘
- **é¿å…æ··æ·†**: ä¿æŒå•ä¸€èŒè´£ï¼Œé¿å…é‡å¤é€»è¾‘

### âš¡ æ€§èƒ½éšæ‚£ï¼šHanLPé€Ÿåº¦é™·é˜± âœ…

#### é—®é¢˜è¯Šæ–­
```python
# æ—§ä»£ç ï¼ˆè‡´å‘½æ€§èƒ½é—®é¢˜ï¼‰
def __getitem__(self, idx):
    hanlp_result = self.preprocessor.process_text_with_hanlp(...)  # æ¯æ¬¡éƒ½è·‘HanLPï¼
```

**åæœ**:
- H100 GPU ç­‰å¾… CPU è·‘ HanLP
- è®­ç»ƒé€Ÿåº¦è¢«æ‹–æ…¢ **100å€**
- GPU åˆ©ç”¨ç‡æä½

#### ä¿®æ­£æ–¹æ¡ˆï¼šå†…ç½®ç¼“å­˜æœºåˆ¶
```python
class SarcasmDataset:
    def __init__(self, data, preprocessor, max_length, cache_file=None):
        if cache_file and os.path.exists(cache_file):
            # ç›´æ¥åŠ è½½ç¼“å­˜
            self.cached_data = pickle.load(...)
        else:
            # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šé¢„å¤„ç†å¹¶ç¼“å­˜
            for text, topic, label in tqdm(self.data):
                bert_tokens = ...
                hanlp_result = ...  # åªåšä¸€æ¬¡ï¼
                self.cached_data.append(...)
            # ä¿å­˜ç¼“å­˜
            pickle.dump(self.cached_data, ...)
    
    def __getitem__(self, idx):
        # ç›´æ¥è¿”å›å†…å­˜æ•°æ®ï¼Œé›¶è®¡ç®—ï¼
        return self.cached_data[idx]
```

#### æ€§èƒ½æå‡
- **ç¬¬ä¸€æ¬¡è¿è¡Œ**: æ…¢ï¼ˆéœ€è¦HanLPå¤„ç†ï¼‰
- **ä¹‹åè¿è¡Œ**: é£å¿«ï¼ˆç›´æ¥åŠ è½½ç¼“å­˜ï¼‰
- **GPUåˆ©ç”¨ç‡**: ä» <10% æå‡åˆ° >90%
- **è®­ç»ƒé€Ÿåº¦**: æå‡ **100å€**

### ğŸ—‘ï¸ æ–‡ä»¶æ¸…ç† âœ…

#### åˆ é™¤åºŸå¼ƒæ–‡ä»¶
- âŒ `preprocess_offline.py` - åŠŸèƒ½å·²é›†æˆåˆ° `SarcasmDataset`
- âŒ `clean_preprocessed.py` - ä¸å†éœ€è¦

#### ç®€åŒ–å·¥ä½œæµ
```bash
# æ—§æµç¨‹ï¼ˆå¤æ‚ï¼‰
python preprocess_offline.py  # ç¬¬ä¸€æ­¥
python main.py --use_preprocessed  # ç¬¬äºŒæ­¥

# æ–°æµç¨‹ï¼ˆç®€å•ï¼‰
python main.py  # ä¸€æ­¥æå®šï¼ç¬¬ä¸€æ¬¡æ…¢ï¼Œä¹‹åå¿«
```

### ğŸ¯ ä¿®æ­£æ•ˆæœ

#### ä»£ç è´¨é‡
- **æ–¹æ³•åä¸€è‡´**: é¿å… AttributeError
- **é€»è¾‘æ¸…æ™°**: åˆ é™¤é‡å¤å’Œå¤šä½™ä»£ç 
- **èŒè´£å•ä¸€**: æ¯ä¸ªæ–¹æ³•åªåšä¸€ä»¶äº‹

#### æ€§èƒ½ä¼˜åŒ–
- **ç¼“å­˜æœºåˆ¶**: è‡ªåŠ¨ç¼“å­˜HanLPç»“æœ
- **é›¶é‡å¤è®¡ç®—**: ç¬¬äºŒæ¬¡è¿è¡Œç›´æ¥åŠ è½½
- **GPUå‹å¥½**: ä¸å†ç­‰å¾…CPUå¤„ç†

#### ç”¨æˆ·ä½“éªŒ
- **ç®€åŒ–æµç¨‹**: ä¸€ä¸ªå‘½ä»¤å¯åŠ¨è®­ç»ƒ
- **æ™ºèƒ½ç¼“å­˜**: è‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨ç¼“å­˜
- **æ¸…æ™°æç¤º**: å‘ŠçŸ¥ç”¨æˆ·ç¼“å­˜çŠ¶æ€

### ğŸ“‹ ä¿®æ­£æ¸…å•
- [x] åˆ é™¤ `build_hypergraph_structure` æ–¹æ³•
- [x] ä¿®æ­£æ–¹æ³•åä¸åŒ¹é…é—®é¢˜
- [x] æ·»åŠ ç¼“å­˜æœºåˆ¶åˆ° `SarcasmDataset`
- [x] æ›´æ–° `create_data_loaders` æ”¯æŒç¼“å­˜
- [x] åˆ é™¤ `preprocess_offline.py`
- [x] åˆ é™¤ `clean_preprocessed.py`
- [x] æ›´æ–° `main.py` ç§»é™¤ç¦»çº¿é¢„å¤„ç†é€»è¾‘
- [x] ç®€åŒ–å‘½ä»¤è¡Œå‚æ•°

### ğŸš€ ä½¿ç”¨æ–¹æ³•

```bash
# ç›´æ¥è¿è¡Œï¼ˆç¬¬ä¸€æ¬¡ä¼šæ…¢ï¼Œä¹‹åé£å¿«ï¼‰
python main.py --dataset_dir dataset --cache_dir cache

# æ¸…é™¤ç¼“å­˜é‡æ–°å¤„ç†
rm -rf cache/
python main.py
```

ç°åœ¨ä»£ç å·²ç»å‡†å¤‡å¥½åœ¨H100ä¸Šé£é©°äº†ï¼ğŸ‰
## 2024-02-05 ä¿®å¤ main.py é€»è¾‘æ¼æ´

### ğŸ› é—®é¢˜è¯Šæ–­ï¼šNameError

#### é”™è¯¯ä»£ç ï¼ˆç¬¬254è¡Œï¼‰
```python
test_loader = DataLoader(test_dataset, ..., collate_fn=collate_fn)
```

**é”™è¯¯åŸå› **:
- `collate_fn` åœ¨ `main.py` ä¸­æ ¹æœ¬æ²¡æœ‰å®šä¹‰
- å®ƒè¢«å°è£…åœ¨ `data_preprocess.py` çš„ `create_data_loaders` å†…éƒ¨
- è¿è¡Œæ—¶æŠ¥é”™ï¼š`NameError: name 'collate_fn' is not defined`

### âœ… ä¿®å¤æ–¹æ¡ˆ

#### 1. æ›´æ–°å¯¼å…¥è¯­å¥
```python
# ä¿®å¤å‰
from data_preprocess import DataPreprocessor, load_all_datasets, create_data_loaders, SarcasmDataset

# ä¿®å¤å
from data_preprocess import (
    DataPreprocessor, 
    load_all_datasets, 
    create_data_loaders, 
    SarcasmDataset,
    create_hypergraph_collate_fn  # æ–°å¢ï¼šç”¨äºæµ‹è¯•é›†
)
```

#### 2. ä¿®å¤æµ‹è¯•é›†è¯„ä¼°é€»è¾‘
```python
if test_data and len(test_data) > 0:
    # 1. åˆ›å»º collate_fnï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„é…ç½®ï¼‰
    test_collate_fn = create_hypergraph_collate_fn(preprocessor, max_length=512)
    
    # 2. åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆå¸¦ç¼“å­˜ï¼‰
    test_cache_file = os.path.join(args.cache_dir, 'test_cache.pkl')
    test_dataset = SarcasmDataset(test_data, preprocessor, 512, cache_file=test_cache_file)
    
    # 3. åˆ›å»º DataLoaderï¼ˆnum_workers=0 é¿å…å¤šè¿›ç¨‹ä¸GPUå†²çªï¼‰
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        collate_fn=test_collate_fn,
        num_workers=0  # é‡è¦ï¼
    )
```

### ğŸ¯ ä¿®å¤è¦ç‚¹

#### å¤ç”¨ç°æœ‰é€»è¾‘
- ä½¿ç”¨ `create_hypergraph_collate_fn` åˆ›å»º collate å‡½æ•°
- ä¿æŒä¸è®­ç»ƒé›†ç›¸åŒçš„æ•°æ®å¤„ç†æµç¨‹
- ç¡®ä¿æµ‹è¯•é›†ä¹Ÿä½¿ç”¨ç¼“å­˜æœºåˆ¶

#### é¿å…å¤šè¿›ç¨‹é—®é¢˜
- è®¾ç½® `num_workers=0`
- åŸå› ï¼š`collate_fn` ä¸­ä½¿ç”¨äº† `.to(device)`
- å¤šè¿›ç¨‹ä¼šå¯¼è‡´ CUDA ä¸Šä¸‹æ–‡å†²çª

#### ç¼“å­˜ä¸€è‡´æ€§
- æµ‹è¯•é›†ä¹Ÿä½¿ç”¨ç¼“å­˜ï¼š`test_cache.pkl`
- ç¬¬ä¸€æ¬¡è¿è¡Œæ…¢ï¼Œä¹‹åå¿«
- ä¸è®­ç»ƒé›†/éªŒè¯é›†ä¿æŒä¸€è‡´

### ğŸ“‹ ä¿®å¤æ¸…å•
- [x] å¯¼å…¥ `create_hypergraph_collate_fn`
- [x] åˆ›å»ºæµ‹è¯•é›† collate_fn
- [x] æ·»åŠ æµ‹è¯•é›†ç¼“å­˜æ”¯æŒ
- [x] è®¾ç½® `num_workers=0`
- [x] æ·»åŠ è¯¦ç»†æ—¥å¿—è¾“å‡º
- [x] ä¿æŒä»£ç ä¸€è‡´æ€§

### ğŸš€ æ•ˆæœ

#### ä¿®å¤å‰
```
NameError: name 'collate_fn' is not defined
```

#### ä¿®å¤å
```
âœ… æµ‹è¯•æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ: XXX æ ·æœ¬
æµ‹è¯•å‡†ç¡®ç‡: X.XXXX
æµ‹è¯•é›†ç»“æœ:
  accuracy: X.XXXX
  precision: X.XXXX
  ...
```

ç°åœ¨æµ‹è¯•é›†è¯„ä¼°å¯ä»¥æ­£å¸¸è¿è¡Œäº†ï¼ğŸ‰

## 2026-02-06 æ˜¾å­˜ä¼˜åŒ– - é€‚é… 12GB GPU

### ğŸ¯ é—®é¢˜èƒŒæ™¯
- **ç¡¬ä»¶é™åˆ¶**: 12GB æ˜¾å­˜
- **åŸé…ç½®**: Batch Size = 32, Max Length = 512
- **é£é™©**: åœ¨ 12GB æ˜¾å­˜ä¸‹å‡ ä¹å¿…å®š OOMï¼ˆæ˜¾å­˜æº¢å‡ºï¼‰

### âš¡ ä¼˜åŒ–æ–¹æ¡ˆ

#### 1. Batch Size è°ƒæ•´ âœ…
```python
# ä¿®æ”¹å‰
parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')

# ä¿®æ”¹å
parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
```
- **é™ä½å¹…åº¦**: 32 â†’ 16ï¼ˆå‡åŠï¼‰
- **æ˜¾å­˜èŠ‚çœ**: çº¦ 50%

#### 2. Max Length è°ƒæ•´ âœ…
```python
# ä¿®æ”¹å‰ï¼ˆè®­ç»ƒ/éªŒè¯é›†ï¼‰
train_loader, val_loader = create_data_loaders(
    train_data, val_data, preprocessor, args.batch_size, max_length=512, cache_dir=args.cache_dir
)

# ä¿®æ”¹å
train_loader, val_loader = create_data_loaders(
    train_data, val_data, preprocessor, args.batch_size, max_length=256, cache_dir=args.cache_dir
)
```

```python
# ä¿®æ”¹å‰ï¼ˆæµ‹è¯•é›†ï¼‰
test_collate_fn = create_hypergraph_collate_fn(preprocessor, max_length=512)
test_dataset = SarcasmDataset(test_data, preprocessor, 512, cache_file=test_cache_file)

# ä¿®æ”¹å
test_collate_fn = create_hypergraph_collate_fn(preprocessor, max_length=256)
test_dataset = SarcasmDataset(test_data, preprocessor, 256, cache_file=test_cache_file)
```
- **é™ä½å¹…åº¦**: 512 â†’ 256ï¼ˆå‡åŠï¼‰
- **æ˜¾å­˜èŠ‚çœ**: çº¦ 50%ï¼ˆåºåˆ—é•¿åº¦å¯¹æ˜¾å­˜å½±å“æ˜¯å¹³æ–¹çº§ï¼‰

### ğŸ“Š æ˜¾å­˜ä¼°ç®—

#### ä¿®æ”¹å‰ï¼ˆå±é™©é…ç½®ï¼‰
- Batch Size: 32
- Max Length: 512
- BERT Hidden: 768
- ä¼°ç®—æ˜¾å­˜: ~15-18 GBï¼ˆè¶…å‡º 12GBï¼ï¼‰

#### ä¿®æ”¹åï¼ˆå®‰å…¨é…ç½®ï¼‰
- Batch Size: 16
- Max Length: 256
- BERT Hidden: 768
- ä¼°ç®—æ˜¾å­˜: ~6-8 GBï¼ˆå®‰å…¨èŒƒå›´å†…ï¼‰

### ğŸ¯ ä¿®æ”¹ä½ç½®æ€»ç»“
1. **main.py ç¬¬ 48 è¡Œ**: `--batch_size` é»˜è®¤å€¼ 32 â†’ 16
2. **main.py ç¬¬ 217 è¡Œ**: è®­ç»ƒ/éªŒè¯é›† `max_length=512` â†’ `max_length=256`
3. **main.py ç¬¬ 398 è¡Œ**: æµ‹è¯•é›† collate_fn `max_length=512` â†’ `max_length=256`
4. **main.py ç¬¬ 402 è¡Œ**: æµ‹è¯•é›† dataset `512` â†’ `256`

### ğŸ’¡ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®
å¦‚æœ 12GB æ˜¾å­˜ä»ç„¶ç´§å¼ ï¼Œå¯ä»¥ï¼š
- é™ä½ Batch Size åˆ° 8
- å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
- å†»ç»“ BERT éƒ¨åˆ†å±‚

### âœ… éªŒè¯æ–¹æ³•
```bash
# è¿è¡Œè®­ç»ƒï¼Œè§‚å¯Ÿæ˜¾å­˜ä½¿ç”¨
python main.py

# ä½¿ç”¨ nvidia-smi ç›‘æ§æ˜¾å­˜
watch -n 1 nvidia-smi
```

é¢„æœŸæ˜¾å­˜å³°å€¼åº”è¯¥åœ¨ 8GB ä»¥å†…ï¼Œç•™æœ‰å……è¶³çš„å®‰å…¨è¾¹é™…ã€‚ğŸ‰
